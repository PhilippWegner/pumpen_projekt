{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63037734",
   "metadata": {},
   "source": [
    "\n",
    "<img style=\"float: right;\" src=\"img/bo_logo.png\" alt=\"Hochschule Bochum\" width=\"30%\" height=\"30%\" title=\"Hochschule Bochum\">\n",
    "<p style=\"text-align: center;\">\n",
    "<b>Hochschule Bochum<br>\n",
    "Fachbereich Informatik und Ingenieurwissenschaften<br>\n",
    "Wahlmodul: Projektbasierte Vertiefung aktueller Themen der Informatik<br>\n",
    "Betreuer: Prof. Dr. rer. nat. Henrik Blunck, Prof. Dr.-Ing. Ralph Lindken, Marc Ladwig (M.Sc.)</b><br>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "565cce04",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c1aa3c4",
   "metadata": {},
   "source": [
    "## Inhaltsverzeichnis\n",
    "1. [Installationsanleitung](#1-installationsanleitung)\n",
    "    * 1.1 [Paketinstallation in Anaconda mit der Konsole](#11-paketinstallation-in-anaconda-mit-der-konsole)\n",
    "    * 1.2 [Verwendung von JupyterLab](#12-verwendung-von-jupyterlab)\n",
    "2. [Aufgabenbeschreibung und Datenstruktur](#2-aufgabenbeschreibung-und-datenstruktur)\n",
    "    * 2.1 [Beschreibung](#21-beschreibung)\n",
    "    * 2.2 [Aufgabe](#22-aufgabe)\n",
    "    * 2.3 [Daten](#23-daten)\n",
    "        * 2.3.1 [KiDAQ-System Drucksignale](#231-kidaq-system-drucksignale)\n",
    "        * 2.3.2 [VIB-System Schwingungssignale](#232-vib-system-schwingungssignale)\n",
    "    * 2.4 [Beschreibung der Rohdatenstruktur](#24-beschreibung-der-rohdatenstruktur)\n",
    "3. [Deskriptive und Explorative Datenanalyse](#3-deskriptive-und-explorative-datenanalyse)\n",
    "    * 3.1 [Datenanalyse](#31-datenanalyse)\n",
    "    * 3.2 [Aggregative Analyse](#32-aggregative-analyse)\n",
    "    * 3.3 [Frequentative Analyse](#33-frequentative-analyse)\n",
    "4. [Preprocessing](#4-preprocessing)\n",
    "    * 4.1 [Parametrierung](#41-parametrierung)\n",
    "    * 4.2 [Start](#42-start)\n",
    "    * 4.3 [Vorbereitung der Tainings- und Testdaten](#43-vorbereitung-der-tainings--und-testdaten)\n",
    "5. [Maschinelles Lernen](#5-maschinelles-lernen1)\n",
    "    * 5.1 [Trainings- und Testdaten wählen und laden](#51-trainings--und-testdaten-wählen-und-laden)\n",
    "    * 5.2 [Entscheidungsbaum - sklearn (empfohlen)](#52-entscheidungsbaum---sklearn-(empfohlen))\n",
    "        * 5.2.1 [Konfiguration 1](#521-konfiguration-1)\n",
    "        * 5.2.2 [Trainieren 1](#522-trainieren-1)\n",
    "    * 5.3 [Tiefes neuronales Netz - tensorflow](#53-tiefes-neuronales-Netz---tensorflow)\n",
    "        * 5.3.1 [Konfiguration 2](#531-konfiguration-2)\n",
    "        * 5.3.2 [Trainieren 2](#532-trainieren-2)\n",
    "    * 5.4 [Extremes Gradienten-Boosting - XGBoost](#54-extremes-gradienten-boosting---xgboost)\n",
    "        * 5.4.1 [Konfiguration 3](#541-konfiguration3)\n",
    "        * 5.4.2 [Trainieren 3](#542trainieren-3)\n",
    "6. [Modelanalyse des Learners](#6-modelanalyse-des-Learners)\n",
    "7. [Statische Interpretation des Resultats](#7-statische-interpretation-des-resultats)\n",
    "    * 7.1 [Deskriptive und Explorative Datenanalyse](#71-deskriptive-und-explorative-datenanalyse)\n",
    "        * 7.1.1 [Aggregation](#711-aggregation)\n",
    "        * 7.1.2 [Frequenztransformation](#712-frequenztransformation)\n",
    "    * 7.2 [Interpretation des maschinellen Lernens](#72-interpretation-des-maschinellen-lernens)\n",
    "        * 7.2.1 [VIB](#721-vib)\n",
    "        * 7.2.2 [KiDAQ](#722-kidaq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5daad510",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "730e15ad",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Installationsanleitung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8e4a562",
   "metadata": {},
   "source": [
    "### 1.1 Paketinstallation in Anaconda mit der Konsole"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d77a7780",
   "metadata": {},
   "source": [
    "1. Öffnen Sie das Startmenü und suchen Sie nach \"Anaconda Prompt\".\n",
    "2. Starten Sie die Anaconda Prompt aus den Suchergebnissen. Das Anaconda Prompt-Fenster wird geöffnet.\n",
    "3. Installieren Sie ein Paket oder eine Dependency, indem Sie den folgenden Befehl eingeben:\n",
    "```bash\n",
    "conda install paketname\n",
    "```\n",
    "\n",
    "Ersetzen Sie `paketname` durch den Namen des Pakets oder der Dependency, die Sie installieren möchten. Sie können auch mehrere Paketnamen angeben, indem Sie sie durch Leerzeichen trennen.\n",
    "\n",
    "4. Warten Sie, bis die Installation abgeschlossen ist. Anaconda wird automatisch alle benötigten Abhängigkeiten herunterladen und installieren.\n",
    "5. Überprüfen Sie, ob die Installation erfolgreich war, indem Sie das Paket oder die Dependency verwenden oder den folgenden Befehl ausführen, um eine Liste der installierten Pakete anzuzeigen:\n",
    "```bash\n",
    "conda list\n",
    "```\n",
    "Dadurch wird eine Liste aller installierten Pakete in der aktuellen Umgebung angezeigt.\n",
    "\n",
    "8. Wiederholen Sie die Schritte 3-5 für alle weiteren Pakete oder Dependencies, die Sie installieren möchten.\n",
    "9. Wenn Sie die Installation beendet haben, können Sie die Anaconda Prompt schließen oder mit weiteren Befehlen und Aktionen fortfahren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "041c5068",
   "metadata": {},
   "source": [
    "### 1.2 Verwendung von JupyterLab\n",
    "\n",
    "1. Öffnen Sie das Startmenü und suchen Sie nach \"Anaconda Navigator\".\n",
    "\n",
    "2. Starten Sie den Anaconda Navigator aus den Suchergebnissen. Das Anaconda Navigator-Fenster wird geöffnet.\n",
    "\n",
    "3. Klicken Sie auf das Symbol für JupyterLab. Dadurch wird ein neues Browserfenster geöffnet, das die JupyterLab-Benutzeroberfläche anzeigt.\n",
    "\n",
    "4. In der JupyterLab-Benutzeroberfläche können Sie entweder eine neue Datei erstellen oder vorhandene Dateien öffnen.\n",
    "\n",
    "   - Um eine neue Datei zu erstellen, klicken Sie auf das \"+\" -Symbol in der Symbolleiste und wählen Sie den gewünschten Dateityp, z. B. \"Python 3\". Dadurch wird eine neue Datei geöffnet.\n",
    "\n",
    "   - Um eine vorhandene Datei zu öffnen, klicken Sie auf \"File\" in der Menüleiste und wählen Sie \"Open\" aus. Navigieren Sie zum Speicherort der Datei auf Ihrem Computer und wählen Sie sie aus. Die Datei wird in einem neuen Tab geöffnet.\n",
    "\n",
    "5. Jeder geöffnete Tab enthält eine Arbeitsfläche, in der Sie Code schreiben, Dateien anzeigen und bearbeiten, sowie andere Aktionen durchführen können.\n",
    "\n",
    "6. JupyterLab bietet verschiedene Arten von Dokumenten, die Sie in der Arbeitsfläche verwenden können:\n",
    "\n",
    "   - Jupyter Notebook: Sie können Jupyter Notebook-Dateien (.ipynb) öffnen und bearbeiten. Diese Dateien bestehen aus Zellen, die Code, Text oder Markdown enthalten können.\n",
    "\n",
    "   - Texteditor: Sie können Textdateien (.txt), Python-Skripte (.py) und andere Textdateiformate anzeigen und bearbeiten.\n",
    "\n",
    "   - Terminal: Sie können eine Befehlszeilenschnittstelle öffnen und Befehle direkt in JupyterLab ausführen.\n",
    "\n",
    "7. Jeder Tab enthält eine Symbolleiste mit verschiedenen Optionen, um Aktionen wie das Ausführen von Code, das Hinzufügen neuer Zellen, das Speichern von Dateien und vieles mehr durchzuführen.\n",
    "\n",
    "8. JupyterLab bietet auch eine Vielzahl von Tastenkombinationen, die Ihnen helfen können, schneller zu arbeiten. Sie können auf \"Help\" in der Menüleiste klicken und dann \"Keyboard Shortcuts\" auswählen, um eine Liste der verfügbaren Tastenkombinationen anzuzeigen.\n",
    "\n",
    "9. Um JupyterLab zu beenden, können Sie das Browserfenster schließen oder zur JupyterLab-Benutzeroberfläche zurückkehren und auf \"File\" in der Menüleiste klicken, gefolgt von \"Quit\".\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3aab5844",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "685818aa",
   "metadata": {},
   "source": [
    "# Analyse Pumpenprüfstandsdaten (ML-Workflow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31ebf547",
   "metadata": {},
   "source": [
    "Für die ordnungsgemäße Nutzung müssen einige Bibliotheken (Libraries) vorab importiert werden. Ein Import wird über das Schlüsselwort <b>import</b> durchgeführt, zusätzlich kann noch über das <b>as</b> der Name des importieren Pakets geändert werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d08d88a7-0fc1-4c7a-91f7-df5aa0a4f9a2",
   "metadata": {},
   "source": [
    "## 2. Aufgabenbeschreibung und Datenstruktur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b0caab5",
   "metadata": {},
   "source": [
    "### 2.1 Beschreibung\n",
    "Die Zustandsüberwachung hydraulischer Anlagen und insbesondere Pumpen spielt eine wichtige Rolle bei der Sicherung eines korrekten Betriebs. Durch die Integration von künstlicher Intelligenz in vorrausschauende Systeme gewinnt die Predictive Maintenance gestützt durch künstliche Intelligenz an Bedeutung. Kreiselpumpen sind aufgrund ihrer Verbreitung in Industrie, Gewerbe und Haushalten besonders relevant für eine effiziente Betriebsweise. Schwingungsbasierte Systeme sind derzeit der Stand der Technik zur Überwachung von Pumpen, können jedoch nicht immer eindeutige Schlüsse auf die Ursachen von Schwingungen ziehen. In diesem Beitrag erfolgt ein Vergleich von druck- und schwingungsbasierten Zustandsüberwachungssystemen an einer typischen Kreiselpumpe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ce466d4",
   "metadata": {},
   "source": [
    "### 2.2 Aufgabe\n",
    "Die Aufgabe besteht darin, die übergebenen Daten aus dem Pumpenprüfstand im Labor der Hochschule Bochum auszuwerten und in ein geeignetes KI-Modell zu überführen. Das KI-Modell soll in der Lage sein, den Zustand der Pumpe zu erkennen und zu klassifizieren. Diese Erkennung soll anhand der Prognosefähigkeit des Modells die Wartungsintervalle und die Nutzungsdauer der Pumpe optimieren.\n",
    "Die Daten enthalten dabei sowohl Druck- als auch Schwingungssignale, die an unterschiedlichen Positionen an der Pumpe angebracht sind.<br>\n",
    "\n",
    "<img src=\"img/positionierung_sensoren.png\" alt=\"Positionierung Sensoren\" width=\"40%\" height=\"40%\" title=\"Positionierung der Sensoren\"><br>\n",
    "\n",
    "Jede Messung wird dabei nach Erreichen eines stationären Förderstroms für 12 Minuten konstant gehalten. Während dieser Zeit werden die Messungen in hoher Frequenz aufgezeichnet und in den Formaten .mf4 und .csv abgespeichert. Beide Signalarten werden getrennt von einander durch zwei unterschiedliche System aufgezeichnet.\n",
    "Das KiDAQ-System ist für die Aufzeichnung der Drucksignale zuständig und das VIB-System ist für die Aufnahme der Schwingungssignale zuständig. Beide Signalarten sollen innerhalb des Projektes zur Erarbeitung eines KI-Modells beitragen. Ebenfalls soll bewertet werden, ob es vielleicht auch ausreichend ist nur eine Signalart für die Erkennung des Pumpenzustandes zu verwenden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "071bc31c",
   "metadata": {},
   "source": [
    "### 2.3 Daten\n",
    "Wie bereits in der Vorstellung der Projektaufgabe kurz erläuert wurde, werden die Daten in zwei unterschiedlichen Formaten abgespeichert. Die Daten des KiDAQ-Systems werden im Format .mf4 abgespeichert und die Daten des VIB-Systems werden im Format .csv abgespeichert. Beide Datenarten werden im folgenden Abschnitt kurz erläutert.\n",
    "\n",
    "#### 2.3.1 KiDAQ-System Drucksignale:\n",
    "Das KiDAQ-System zeichnet die Messwerte der Drucksignale mit bis ca. 80 kHz auf. Hierfür werden piezoelektrische Sensoren verwendet, die an den Positionen P1 und P2 an der Pumpe angebracht sind. Die Sensoren sind in der Lage Druckänderungen zu messen und in elektrische Signale umzuwandeln. Um die Messungen zu homogenisieren existiert für jede Messsequenz ein Formblatt, welches genau dokumentiert welcher Förderstrom (l/min) vorliegt, an welchem Datum und zu welcher Zeit die Messung durchgeführt wurde und mit welcher Drehzahl die Pumpe angetrieben wurde. Ebenfalls wurde festgehalten, ob es sich bei dem Versuch, um einen Gut-, Kavitations-, Fehlausrichtungs- oder Laufradschadenversuch gehandelt hat. Da während der Messung eine große Datenanzahl generiert wird, wird das mf4-Format zum Abspeichern verwendet. Das mf4-Format ist ein standardisiertes Format, das in der Messtechnik und in der Datenanalyse eingesetzt wird.\n",
    "\n",
    "#### 2.3.2 VIB-System Schwingungssignale:\n",
    "Das VIB-System ist für die Aufzeichnung der Schwingungssignals mit bis ca. 80kHz zuständig. VIB steht hierbei für Vibration. Es werden auch hier spezielle Sensoren an der Pumpe angebracht. Insgesamt sind XX Sensoren verbaut, die alle ausgelesen werden. Die Schwingungssignale werden nach dem selben Prinzip wie die Drucksignale aufgezeichnet. Sie wandeln elektromagnetische Schwingungen in elektrische Signale um. Die Messungeinstellungen werden ebenfalls in einem Formblatt dokumentiert. Die Formblätter sind identisch zu den Formblättern des KiDAQ-Systems. Die Messungen werden in einem .csv-Format abgespeichert. Das .csv-Format lässt sich hierbei ohne Umformungsschritt sofort auslesen und vereinfacht somit die Arbeit, allerdings zu den Kosten einer größeren Einzeldatei.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ea80c24",
   "metadata": {},
   "source": [
    "### 2.4 Beschreibung der Rohdatenstruktur\n",
    "Die Rohdaten des Pumpen Versuchs wurden ins in folgender Form übergeben:\n",
    "* Pumpensetup\n",
    "  * Versuchsart\n",
    "    * Formblatt\n",
    "    * Aufzeichnungssystem\n",
    "      * VIB-System\n",
    "      * KiDAQ-System\n",
    "      * Umrichterdaten\n",
    "  \n",
    "\n",
    "Das Pumpensetup gibt hierbei an, um welche Pumpe es sich handelt, und an welcher Stelle sich welche Sensoren befinden. Für unsere Auswertung wurden uns zwei unterschiedliche Setups übergeben. Das erste Setup nennt sich innerhalb der Ordnerstruktur __Setup-I__ und das zweite Setup nennt sich __Setup-II__.\n",
    "In jedem Setup-Ordner befinden sich die Versuchsarten.\n",
    "Das Versuchsart-Verzeichnis gibt an, um welche Art von Versuch es sich handelt. Hierbei gibt es vier unterschiedliche Versuchsarten, welche in unterschiedlicher Anzahl vorliegen:\n",
    "- Gutversuch \n",
    "- Kavitationsversuch\n",
    "- Fehlausrichtungsversuch\n",
    "- Laufradschadenversuch\n",
    "\n",
    "Alle Versuchsarten sind in den beiden Setups enthalten und werden im späteren Verlauf auch für die Erzeugung des KI-Modells und für die Validation eingesetzt.\n",
    "\n",
    "Ebenfalls interessant ist das Formblatt. Es definiert in welcher Position sich die Pumpe zur Versuchsreihe befunden hat und zeigt ebenfalls die Ausrichtung der Messensensoren. Ebenfalls wird hier dokumentiert, welche Drehzahl die Pumpe angetrieben hat und welcher Förderstrom vorlag. Das Format der Formblätter ist für jede Versuchsart identisch.\n",
    "\n",
    "Die beiden Aufzeichnungssysteme, die bereits im vorherigen Abschnitt erläutert wurden, enthalten die Messdaten: VIB für Schwingungssignale und KiDAQ für Drucksignale. Beide System verwenden unterschiedliche Dateiformate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caadf792",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b3383c9",
   "metadata": {},
   "source": [
    "## Imports und globale Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3394772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import ipywidgets as widgets\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import asammdf\n",
    "from IPython.display import display\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pathlib as pl\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ea6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the global variables\n",
    "\n",
    "PATH_RAW_DATA = \"D:/Messdaten/\" #r\"E:\\Messdaten\"\n",
    "PATH_FEATURE_DATA = \"./data/\"\n",
    "PATH_EXPLORATION_DATA = \"./exploration/\"\n",
    "PATH_MODEL = \"./models/\"\n",
    "DATA_SOURCE_KIDAQ = [\"TEST_NAME\", \"TEST_TYPE\", \"RPM\", \"FLOW_RATE\", \"P1\", \"P2\"]\n",
    "RAW_DATA_TYPE = [\"KIDAQ\", \"VIB\"]\n",
    "\n",
    "DATA_SOURCE_VID = [\n",
    "    \"TEST_NAME\",\n",
    "    \"TEST_TYPE\",\n",
    "    \"RPM\",\n",
    "    \"FLOW_RATE\",\n",
    "    \"S1\",\n",
    "    \"S2\",\n",
    "    \"S3\",\n",
    "    \"S4\",\n",
    "    \"S5\",\n",
    "    \"S6\",\n",
    "    \"S7\",\n",
    "    \"S8\",\n",
    "]\n",
    "FEATURE = [\n",
    "    \"STD\",\n",
    "    \"RANGE\",\n",
    "    \"IQR\",\n",
    "    \"MEAN_MEDIAN\",\n",
    "    \"FFT\",\n",
    "]\n",
    "OPERATING_POINT_FREQ = [725, 1450, 2175, 2900]\n",
    "OPERATING_POINT_FLOW_RATE = [0, 25, 50, 75, 100]\n",
    "\n",
    "PREPROCESSING_WINDOW_SIZE_MS_DEFAULT = 1000\n",
    "PREPROCESSING_WINDOW_SIZE_MS_MIN = 100\n",
    "PREPROCESSING_WINDOW_SIZE_MS_MAX = 10000\n",
    "\n",
    "PREPROCESSING_FREQUENCY_BAND_COUNT_DEFAULT = 10\n",
    "PREPROCESSING_FREQUENCY_BAND_COUNT_MIN = 10\n",
    "PREPROCESSING_FREQUENCY_BAND_COUNT_MAX = 100\n",
    "\n",
    "DEFAULT_RAW_DATA_TYPE = RAW_DATA_TYPE[1]\n",
    "DEFAULT_RAW_DATA = DATA_SOURCE_KIDAQ\n",
    "DEFAULT_SAMPLE_RATE = 10000\n",
    "\n",
    "DEFAULT_CLASS_LABEL = \"TEST_TYPE\"\n",
    "# 2021-04-28 - Gut 1, 2021-05-19 - Kavitation 1, 2021-05-26 - Fehlausrichtung 1, 2021-07-10 - Laufradschaden 1\n",
    "TEST_TYPES = [\"Gut\", \"Kavitation\", \"Fehlausrichtung\", \"Laufradschaden\"]\n",
    "TEST_TYPE_REGEX = r\"\\b(?:\" + \"|\".join(TEST_TYPES) + r\")\\b\"\n",
    "\n",
    "# ..\\Messdaten\\Setup-I\\2021-04-28 - Gut 1\\KiDAQ\\1450rpm\\1450rpm@100%.mf4\n",
    "TEST_NAME_REGEX = r\"\\d{4}-\\d{2}-\\d{2} - [\\w\\s]+\"\n",
    "\n",
    "KIDAQ_FILE_SEARCH_PATTERN = \"**/*.mf4\"  # 725rpm@0%.mf4\n",
    "KIDAQ_FILE_RPM_REGEX = r\"(\\d+)rpm\"\n",
    "KIDAQ_FILE_FLOW_RATE_REGEX = r\"(\\d+)%\"\n",
    "\n",
    "\n",
    "KIDAQ_DEFAULT_FEATURES = [\n",
    "    \"test_name\",\n",
    "    \"test_type\",\n",
    "    \"flow_rate\",\n",
    "    \"rpm\",\n",
    "    \"p1_std\",\n",
    "    \"p2_std\",\n",
    "    \"p1_range\",\n",
    "    \"p2_range\",\n",
    "    \"p1_iqr\",\n",
    "    \"p2_iqr\",\n",
    "    \"p1_mean_median\",\n",
    "    \"p2_mean_median\",\n",
    "]\n",
    "\n",
    "VIB_DEFAULT_FEATURES = [\n",
    "    \"test_name\",\n",
    "    \"test_type\",\n",
    "    \"flow_rate\",\n",
    "    \"rpm\",\n",
    "    \"s1_std\",\n",
    "    \"s2_std\",\n",
    "    \"s3_std\",\n",
    "    \"s4_std\",\n",
    "    \"s5_std\",\n",
    "    \"s6_std\",\n",
    "    \"s7_std\",\n",
    "    \"s8_std\",\n",
    "    \"s1_range\",\n",
    "    \"s2_range\",\n",
    "    \"s3_range\",\n",
    "    \"s4_range\",\n",
    "    \"s5_range\",\n",
    "    \"s6_range\", \n",
    "    \"s7_range\",\n",
    "    \"s8_range\",\n",
    "    \"s1_iqr\",\n",
    "    \"s2_iqr\",\n",
    "    \"s3_iqr\",\n",
    "    \"s4_iqr\",\n",
    "    \"s5_iqr\",\n",
    "    \"s6_iqr\",\n",
    "    \"s7_iqr\",\n",
    "    \"s8_iqr\",\n",
    "    \"s1_mean_median\",\n",
    "    \"s2_mean_median\",\n",
    "    \"s3_mean_median\",\n",
    "    \"s4_mean_median\",\n",
    "    \"s5_mean_median\",\n",
    "    \"s6_mean_median\",\n",
    "    \"s7_mean_median\",\n",
    "    \"s8_mean_median\"\n",
    "]\n",
    "    \n",
    "# 2900_25_1.mf4\n",
    "\n",
    "KIDAQ_FILE_FIX_REGEX = r\"(\\d+)_(\\d+)_(\\d+).mf4\"\n",
    "\n",
    "\n",
    "VIB_FILE_SEARCH_PATTERN = (\n",
    "    \"**/VIB*.csv\"  # 725rpm\\0%\\1\\VIB1 2021-05-03 10-58-02.453.519.300.csv\n",
    ")\n",
    "VIB_FILE_SENSOR_REGEX = r\"VIB(\\d+)\"\n",
    "VIB_FILE_RPM_REGEX = r\"(\\d+)rpm\"\n",
    "VIB_FILE_FLOW_RATE_REGEX = r\"(\\d+)%\"\n",
    "VIB_SENSOR_COUNT = 8\n",
    "\n",
    "DATANALYSIS_WINDOW_SIZE_DEFAULT = 100000\n",
    "DATANALYSIS_WINDOW_SIZE_MIN = 0\n",
    "DATANALYSIS_WINDOW_SIZE_MAX = 1000000\n",
    "\n",
    "DATANALYSIS_STEP_SIZE_DEFAULT = 100000\n",
    "DATANALYSIS_STEP_SIZE_MIN = 0\n",
    "DATANALYSIS_STEP_SIZE_MAX = 1000000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8ed0170-f3d5-4287-8dd8-7439080d27af",
   "metadata": {},
   "source": [
    "## 3. Deskriptive und explorative Datenanalyse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0addb1c1",
   "metadata": {},
   "source": [
    "### 3.1 Datenanalyse\n",
    "Um die Daten deskriptiv und explorativ auszuwerten, müssen die Daten visualisert werden. Um diesen Prozess zu automatisieren und dadurch zu vereinfachen können innerhalb dieses interaktiven Abschnitts verschiedene Einstellungen vorgenommen werden, um sich Plots zu generieren.\n",
    "Diese Plots werden innerhalb des Notebooks angezeigt und können Einzeln oder als Serie abgespeichert werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "139f9030",
   "metadata": {},
   "source": [
    "### 3.2 Aggregative Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSizeSlider = widgets.IntSlider(\n",
    "    value=DATANALYSIS_WINDOW_SIZE_DEFAULT,\n",
    "    min=DATANALYSIS_WINDOW_SIZE_MIN,\n",
    "    max=DATANALYSIS_WINDOW_SIZE_MAX,\n",
    "    description='Window size:'\n",
    ")\n",
    "display(windowSizeSlider)\n",
    "\n",
    "stepSizeSlider = widgets.IntSlider(\n",
    "    value=DATANALYSIS_STEP_SIZE_DEFAULT,\n",
    "    min=DATANALYSIS_STEP_SIZE_MIN,\n",
    "    max=DATANALYSIS_STEP_SIZE_MAX,\n",
    "    description='Step size:'\n",
    ")\n",
    "display(stepSizeSlider)\n",
    "\n",
    "\n",
    "setups = list(filter(lambda x: os.path.isdir(os.path.join(PATH_RAW_DATA, x)), os.listdir(PATH_RAW_DATA)))\n",
    "setupSelectionDropdown = widgets.Dropdown(\n",
    "    options=setups,\n",
    "    description='Setup:'\n",
    ")\n",
    "display(setupSelectionDropdown)\n",
    "\n",
    "\n",
    "# list all folder in the selected setup and order by name of folder\n",
    "seriesNames = sorted(list(filter(lambda x: os.path.isdir(os.path.join(PATH_RAW_DATA, setupSelectionDropdown.value, x)), os.listdir(os.path.join(PATH_RAW_DATA, setupSelectionDropdown.value)))))\n",
    "seriesDropdown = widgets.Dropdown(\n",
    "    options=seriesNames,\n",
    "    description='Series:',\n",
    "    value=seriesNames[1]\n",
    ")\n",
    "# if setupSelectionDropdown.value changes update seriesDropdown\n",
    "def updateSeriesDropdown(change):\n",
    "    seriesNames = sorted(list(filter(lambda x: os.path.isdir(os.path.join(PATH_RAW_DATA, setupSelectionDropdown.value, x)), os.listdir(os.path.join(PATH_RAW_DATA, setupSelectionDropdown.value)))))\n",
    "    seriesDropdown.options = seriesNames\n",
    "setupSelectionDropdown.observe(updateSeriesDropdown, names='value')\n",
    "display(seriesDropdown)\n",
    "\n",
    "\n",
    "dataSourceDropdown = widgets.Dropdown(\n",
    "    options=RAW_DATA_TYPE,\n",
    "    description='Data source:',\n",
    "    value=RAW_DATA_TYPE[0]\n",
    ")\n",
    "display(dataSourceDropdown)\n",
    "\n",
    "\n",
    "# create Pathlib Path to the selected series\n",
    "def createPathToSeries(setup, series):\n",
    "    return os.path.join(PATH_RAW_DATA, setup, series)\n",
    "\n",
    "# list all files in all subfolders of createPathToSeries if data source is KIDAQ file should end with .mf4 else with .csv use function createPathToSeries\n",
    "def listFilesInPath(path):\n",
    "    files = []\n",
    "    if dataSourceDropdown.value == 'KIDAQ':\n",
    "        for file in pl.Path(path).glob(KIDAQ_FILE_SEARCH_PATTERN):\n",
    "            # append full file path as string to files list\n",
    "            files.append(file) \n",
    "    else:\n",
    "        if path is None:\n",
    "            return []\n",
    "        for file in pl.Path(path).glob(VIB_FILE_SEARCH_PATTERN):\n",
    "            files.append(file)\n",
    "    return sorted(files)\n",
    "\n",
    "fileDropdown = widgets.Dropdown(\n",
    "    options=listFilesInPath(createPathToSeries(setupSelectionDropdown.value, seriesDropdown.value)),\n",
    "    description='File:'\n",
    ")\n",
    "# if one of the dropdowns changes update fileDropdown\n",
    "def updateFileDropdown(change):\n",
    "    fileDropdown.options = listFilesInPath(createPathToSeries(setupSelectionDropdown.value, seriesDropdown.value))\n",
    "setupSelectionDropdown.observe(updateFileDropdown, names='value')\n",
    "seriesDropdown.observe(updateFileDropdown, names='value')\n",
    "dataSourceDropdown.observe(updateFileDropdown, names='value')\n",
    "display(fileDropdown)\n",
    "\n",
    "\n",
    "aggregationDropdown = widgets.Dropdown(\n",
    "    options=['median', 'std', 'min', 'max'],\n",
    "    value='median',\n",
    "    description='Aggregation:',\n",
    "    disabled=False,\n",
    ")\n",
    "display(aggregationDropdown)\n",
    "\n",
    "# add button with refresh icon to refresh columns of the selected file\n",
    "refreshColumnsButton = widgets.Button(\n",
    "    description='Refresh columns',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Refresh columns',\n",
    "    icon='refresh'\n",
    ")\n",
    "display(refreshColumnsButton)\n",
    "\n",
    "# read the selected file and return a pandas dataframe\n",
    "def readFile():\n",
    "    # check if path is not None return empty dataframe\n",
    "    if fileDropdown.value is None:\n",
    "        return pd.DataFrame()\n",
    "    if dataSourceDropdown.value == 'KIDAQ':\n",
    "        return asammdf.MDF(fileDropdown.value).to_dataframe()\n",
    "    elif dataSourceDropdown.value == 'VIB':\n",
    "        # return pd dataframe with only the first 2 columns delete the 3 row and set the second row as header\n",
    "        df = pd.read_csv(fileDropdown.value, sep=';', encoding = \"ISO-8859-1\", header=1, usecols=[0,1], low_memory=False)\n",
    "        # rename the first column to 'Timestamp [ns]'\n",
    "        df.rename(columns={df.columns[0]: 'Timestamp [ns]'}, inplace=True)\n",
    "        # delete the first row\n",
    "        df = df.iloc[1:]\n",
    "        # set the first column as index\n",
    "        df.set_index(df.columns[0], inplace=True)\n",
    "        # return the dataframe\n",
    "        return df\n",
    "        \n",
    "        \n",
    "\n",
    "# create a list of all columns of the selected file\n",
    "def getColumnsToList():\n",
    "    return list(readFile().columns.to_list())\n",
    "\n",
    "multiselectColumns = widgets.SelectMultiple(\n",
    "    options=[],\n",
    "    description='Columns:',\n",
    "    disabled=False\n",
    ")\n",
    "# if refreshColumnsButton is clicked update multiselectColumns\n",
    "def refreshColumnsButtonClicked(b):\n",
    "    multiselectColumns.options = getColumnsToList()\n",
    "refreshColumnsButton.on_click(refreshColumnsButtonClicked)\n",
    "# if one of the dropdowns changes update multiselectColumns to empty list\n",
    "def updateMultiselectColumns(change):\n",
    "    multiselectColumns.options = []\n",
    "setupSelectionDropdown.observe(updateMultiselectColumns, names='value')\n",
    "seriesDropdown.observe(updateMultiselectColumns, names='value')\n",
    "dataSourceDropdown.observe(updateMultiselectColumns, names='value')\n",
    "fileDropdown.observe(updateMultiselectColumns, names='value')\n",
    "display(multiselectColumns)\n",
    "\n",
    "def createPlot(columnTuple, aggregation, windowSize, stepSize, file):\n",
    "    columnList = list(columnTuple)\n",
    "    if len(columnList) == 0:\n",
    "        return\n",
    "    # if no columns are selected return\n",
    "    if len(columnList) == 0:\n",
    "        return\n",
    "    df = readFile()\n",
    "    # drop all columns that are not selected\n",
    "    df = df[columnList]\n",
    "    df_agg = df.rolling(\n",
    "        window=windowSize,\n",
    "        step=stepSize,\n",
    "    ).agg(aggregation)\n",
    "    # set title to the name of the selected file\n",
    "    title=file.name\n",
    "    # plot data\n",
    "    df_agg.plot(title=title, grid=True)\n",
    "\n",
    "# create button to plot the selected columns\n",
    "plotButton = widgets.Button(\n",
    "    description='Plot',\n",
    "    disabled=True,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Plot',\n",
    "    icon='check'\n",
    ")\n",
    "# if plotButton is clicked plot the selected columns\n",
    "def plotButtonClicked(b):\n",
    "    createPlot(multiselectColumns.value, aggregationDropdown.value, windowSizeSlider.value, stepSizeSlider.value, fileDropdown.value)\n",
    "plotButton.on_click(plotButtonClicked)\n",
    "# if no column is selected disable plotButton\n",
    "def updatePlotButton(change):\n",
    "    if len(multiselectColumns.value) == 0:\n",
    "        plotButton.disabled = True\n",
    "    else:\n",
    "        plotButton.disabled = False\n",
    "multiselectColumns.observe(updatePlotButton, names='value')\n",
    "display(plotButton)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b3cbfeb",
   "metadata": {},
   "source": [
    " - Über die Window-Size kann die Größer der Fenster eingestellt werden. Standardmäßig werden 100.000 Punkte zu einem Fenster zusammengefügt.\n",
    " - Über die Step-Size kann die Verschiebung der einzelnen Fenster eingestellt werden. Stellen wir diesen ebenfalls auf 100.000 kommt es zu keiner Überlappung, da jedes Fenster immer um 100.000 Punkte weitergeschoben wird. Stellen wir allerdings die Step-Size kleiner als die Window-Size, so kommt es zu einer Überlappung der Fenster, da die selben Punkte nun in mehreren Fenstern liegen können.\n",
    " - Das Setup kann hierbei ausgewählt werden. Es stehen die beiden Setups Setup-I und Setup-II zur Verfügung.\n",
    " - Die Series gibt an, welche Versuchsreihe ausgewählt werden sollen. Es können hierbei innerhalb des ausgewählten Setups auf die einzelnen Versuchsreihen zugegriffen werden.\n",
    " - Data Source ist für die jeweilige Aufzeichnungsart verantwortlich. Es kann zwischen KIDAQ und VIB gewählt werden.\n",
    " - File ist für die Auswahl der Datei verantwortlich, die ausgewertet und geplottet werden soll.\n",
    " - Die letzte Einstellung beinhaltet die Aggregation der Werte. Es kann hierbei zwischen 'median', 'std', 'min' und 'max' gewählt werden. Die Aggregation bezieht sich hierbei immer auf die einzelnen Fenster. Alle Punkte innerhalb des Fenstern werden mittels der Aggregation zusammen gerechnet und es entsteht ein neuer einzelner Punkte pro Fenster. Aus diesem Grund ist es manchmal Ratsam eine Fensterüberlappung zu zulassen.\n",
    " - Columns gibt an, welche Spalten ausgewählt werden sollen. Hierbei können mehrere Spalten ausgewählt werden. Die Spalten werden dann mit unterschiedlichen Farben geplottet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78ed38f4",
   "metadata": {},
   "source": [
    "### 3.3 Frequentative Analyse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7652b1ba",
   "metadata": {},
   "source": [
    "Eine Fast-Fourier-Transformation (FFT) wird verwendet, um ein zeitabhängiges Signal in den Frequenzbereich zu transformieren. Bei der Anwendung auf Vibrationssignale können folgende Erkenntnisse gewonnen werden:\n",
    "\n",
    "1. Frequenzinhalt: Die FFT zeigt, welche Frequenzen im Vibrationssignal vorhanden sind und in welchem Maße sie vertreten sind. Dies ermöglicht die Identifizierung von dominierenden Frequenzen oder Frequenzbereichen.\n",
    "\n",
    "2. Amplitudenverteilung: Die Amplituden der Frequenzkomponenten im Vibrationssignal werden dargestellt. Dadurch kann man herausfinden, wie stark die einzelnen Frequenzen in Bezug auf ihre Amplitude sind. Dies hilft bei der Analyse der Energieverteilung im Frequenzbereich.\n",
    "\n",
    "3. Peak-Erkennung: Die FFT kann helfen, Peaks oder Spitzen im Frequenzspektrum zu identifizieren. Diese Peaks können auf resonante oder periodische Eigenschaften des Systems hinweisen und Informationen über potenzielle Fehler oder Probleme liefern.\n",
    "\n",
    "4. Harmonische Analyse: Durch die FFT können harmonische Komponenten identifiziert werden, die durch periodische Schwingungen im Vibrationssignal entstehen. Dies kann auf Unregelmäßigkeiten im System oder auf das Vorhandensein von störenden Frequenzen hinweisen.\n",
    "\n",
    "5. Filterung: Durch die Analyse des Frequenzspektrums mit Hilfe der FFT kann man unerwünschte Frequenzkomponenten identifizieren und gezielt herausfiltern. Dies ermöglicht es, spezifische Frequenzen zu isolieren oder Störungen zu reduzieren.\n",
    "\n",
    "6. Zeit-Frequenz-Analyse: Durch die Anwendung von Fensterfunktionen auf das Vibrationssignal vor der FFT kann eine Zeit-Frequenz-Analyse durchgeführt werden. Diese Technik ermöglicht die Beobachtung von Frequenzänderungen im Laufe der Zeit und kann bei der Diagnose von instationären oder sich ändernden Zuständen helfen.\n",
    "\n",
    "Die FFT ist ein leistungsfähiges Werkzeug zur Analyse von Vibrationssignalen und findet in verschiedenen Anwendungen wie der Zustandsüberwachung, der Schadenserkennung, der Maschinendiagnose und der akustischen Analyse Anwendung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c6ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleRateInput = widgets.IntText(\n",
    "    value=DEFAULT_SAMPLE_RATE,\n",
    "    description='Sample rate:',\n",
    ")\n",
    "display(sampleRateInput)\n",
    "\n",
    "maxFrequencyInput = widgets.IntText(\n",
    "    value=DEFAULT_SAMPLE_RATE / 10,\n",
    "    description='Max. frequency:',\n",
    ")\n",
    "display(maxFrequencyInput)\n",
    "\n",
    "setups = list(filter(lambda x: os.path.isdir(os.path.join(PATH_RAW_DATA, x)), os.listdir(PATH_RAW_DATA)))\n",
    "setupSelectionDropdown = widgets.Dropdown(\n",
    "    options=setups,\n",
    "    description='Setup:'\n",
    ")\n",
    "display(setupSelectionDropdown)\n",
    "\n",
    "\n",
    "# list all folder in the selected setup and order by name of folder\n",
    "seriesNames = sorted(list(filter(lambda x: os.path.isdir(os.path.join(PATH_RAW_DATA, setupSelectionDropdown.value, x)), os.listdir(os.path.join(PATH_RAW_DATA, setupSelectionDropdown.value)))))\n",
    "seriesDropdown = widgets.Dropdown(\n",
    "    options=seriesNames,\n",
    "    description='Series:',\n",
    "    value=seriesNames[1]\n",
    ")\n",
    "# if setupSelectionDropdown.value changes update seriesDropdown\n",
    "def updateSeriesDropdown(change):\n",
    "    seriesNames = sorted(list(filter(lambda x: os.path.isdir(os.path.join(PATH_RAW_DATA, setupSelectionDropdown.value, x)), os.listdir(os.path.join(PATH_RAW_DATA, setupSelectionDropdown.value)))))\n",
    "    seriesDropdown.options = seriesNames\n",
    "setupSelectionDropdown.observe(updateSeriesDropdown, names='value')\n",
    "display(seriesDropdown)\n",
    "\n",
    "\n",
    "dataSourceDropdown = widgets.Dropdown(\n",
    "    options=RAW_DATA_TYPE,\n",
    "    description='Data source:',\n",
    "    value=RAW_DATA_TYPE[0]\n",
    ")\n",
    "display(dataSourceDropdown)\n",
    "\n",
    "\n",
    "# create Pathlib Path to the selected series\n",
    "def createPathToSeries(setup, series):\n",
    "    return os.path.join(PATH_RAW_DATA, setup, series)\n",
    "\n",
    "# list all files in all subfolders of createPathToSeries if data source is KIDAQ file should end with .mf4 else with .csv use function createPathToSeries\n",
    "def listFilesInPath(path):\n",
    "    files = []\n",
    "    if dataSourceDropdown.value == 'KIDAQ':\n",
    "        for file in pl.Path(path).glob(KIDAQ_FILE_SEARCH_PATTERN):\n",
    "            # append full file path as string to files list\n",
    "            files.append(file) \n",
    "    else:\n",
    "        if path is None:\n",
    "            return []\n",
    "        for file in pl.Path(path).glob(VIB_FILE_SEARCH_PATTERN):\n",
    "            files.append(file)\n",
    "    return sorted(files)\n",
    "\n",
    "fileDropdown = widgets.Dropdown(\n",
    "    options=listFilesInPath(createPathToSeries(setupSelectionDropdown.value, seriesDropdown.value)),\n",
    "    description='File:'\n",
    ")\n",
    "# if one of the dropdowns changes update fileDropdown\n",
    "def updateFileDropdown(change):\n",
    "    fileDropdown.options = listFilesInPath(createPathToSeries(setupSelectionDropdown.value, seriesDropdown.value))\n",
    "setupSelectionDropdown.observe(updateFileDropdown, names='value')\n",
    "seriesDropdown.observe(updateFileDropdown, names='value')\n",
    "dataSourceDropdown.observe(updateFileDropdown, names='value')\n",
    "display(fileDropdown)\n",
    "\n",
    "# add button with refresh icon to refresh columns of the selected file\n",
    "refreshColumnsButton = widgets.Button(\n",
    "    description='Refresh columns',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Refresh columns',\n",
    "    icon='refresh'\n",
    ")\n",
    "display(refreshColumnsButton)\n",
    "\n",
    "# read the selected file and return a pandas dataframe\n",
    "def readFile():\n",
    "    # check if path is not None return empty dataframe\n",
    "    if fileDropdown.value is None:\n",
    "        return pd.DataFrame()\n",
    "    if dataSourceDropdown.value == 'KIDAQ':\n",
    "        return asammdf.MDF(fileDropdown.value).to_dataframe()\n",
    "    elif dataSourceDropdown.value == 'VIB':\n",
    "        # return pd dataframe with only the first 2 columns delete the 3 row and set the second row as header\n",
    "        df = pd.read_csv(fileDropdown.value, sep=';', encoding = \"ISO-8859-1\", header=1, usecols=[0,1], low_memory=False)\n",
    "        # rename the first column to 'Timestamp [ns]'\n",
    "        df.rename(columns={df.columns[0]: 'Timestamp [ns]'}, inplace=True)\n",
    "        # delete the first row\n",
    "        df = df.iloc[1:]\n",
    "        # set the first column as index\n",
    "        df.set_index(df.columns[0], inplace=True)\n",
    "        # return the dataframe\n",
    "        return df\n",
    "        \n",
    "        \n",
    "\n",
    "# create a list of all columns of the selected file\n",
    "def getColumnsToList():\n",
    "    return list(readFile().columns.to_list())\n",
    "\n",
    "multiselectColumns = widgets.SelectMultiple(\n",
    "    options=[],\n",
    "    description='Columns:',\n",
    "    disabled=False\n",
    ")\n",
    "# if refreshColumnsButton is clicked update multiselectColumns\n",
    "def refreshColumnsButtonClicked(b):\n",
    "    multiselectColumns.options = getColumnsToList()\n",
    "refreshColumnsButton.on_click(refreshColumnsButtonClicked)\n",
    "# if one of the dropdowns changes update multiselectColumns to empty list\n",
    "def updateMultiselectColumns(change):\n",
    "    multiselectColumns.options = []\n",
    "setupSelectionDropdown.observe(updateMultiselectColumns, names='value')\n",
    "seriesDropdown.observe(updateMultiselectColumns, names='value')\n",
    "dataSourceDropdown.observe(updateMultiselectColumns, names='value')\n",
    "fileDropdown.observe(updateMultiselectColumns, names='value')\n",
    "display(multiselectColumns)\n",
    "\n",
    "# use the selected columns and the selected file to create fourier transform plot without matplotlib\n",
    "def createFFTPlot(columns):\n",
    "    df = readFile()\n",
    "    # create a list of all columns of the selected file\n",
    "    columns = list(columns)\n",
    "    \n",
    "    \n",
    "\n",
    "def createFFTPlot(columns, file, sampleRate=10000, maxFrequency=1000):\n",
    "    # read the selected file\n",
    "    dataframe = readFile()\n",
    "    # columns\n",
    "    columns = list(columns)\n",
    "    dataframe = dataframe[columns]\n",
    "    \n",
    "    # Erstellung der Plots für jede Spalte\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        # Extrahieren der Vibrationsdaten aus der aktuellen Spalte\n",
    "        vibrations = dataframe[column].values\n",
    "        \n",
    "        # Durchführung des FFT\n",
    "        fft = np.fft.fft(vibrations)\n",
    "        freqFreq = np.fft.fftfreq(len(vibrations), 1/sampleRate)\n",
    "        \n",
    "        fftDf = pd.DataFrame(np.abs(fft), index=freqFreq)\n",
    "        fftDf = fftDf.loc[(fftDf.index >= 0) & (fftDf.index <= maxFrequency)]\n",
    "        \n",
    "        # Plotten des Amplitudenspektrums and tight_layout()\n",
    "        fftDf.plot(xlabel=\"Frequency\", ylabel=\"Amplitude\", grid=True, legend=False, title=f\"FFT Plot - {column}\")    \n",
    "\n",
    "# create button to plot the selected columns\n",
    "plotButton = widgets.Button(\n",
    "    description='Plot',\n",
    "    disabled=True,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Plot',\n",
    "    icon='check'\n",
    ")\n",
    "# if plotButton is clicked plot the selected columns\n",
    "def plotButtonClicked(b):\n",
    "    createFFTPlot(multiselectColumns.value, fileDropdown.value, sampleRateInput.value, maxFrequencyInput.value)\n",
    "plotButton.on_click(plotButtonClicked)\n",
    "# if no column is selected disable plotButton\n",
    "def updatePlotButton(change):\n",
    "    if len(multiselectColumns.value) == 0:\n",
    "        plotButton.disabled = True\n",
    "    else:\n",
    "        plotButton.disabled = False\n",
    "multiselectColumns.observe(updatePlotButton, names='value')\n",
    "display(plotButton)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1291fcff",
   "metadata": {},
   "source": [
    " - Die Samplerate gibt an, wie viele Punkte pro Sekunde aufgezeichnet wurden. Diese Information ist wichtig, um die Frequenzanalyse durchführen zu können.\n",
    " - Maximale Frequency gibt an, bis zu welcher Frequenz die Frequenzanalyse durchgeführt werden soll.\n",
    " - Das Setup kann hierbei ausgewählt werden. Es stehen die beiden Setups Setup-I und Setup-II zur Verfügung.\n",
    " - Die Series gibt an, welche Versuchsreihe ausgewählt werden sollen. Es können hierbei innerhalb des ausgewählten Setups auf die einzelnen Versuchsreihen zugegriffen werden.\n",
    " - Data Source ist für die jeweilige Aufzeichnungsart verantwortlich. Es kann zwischen KIDAQ und VIB gewählt werden.\n",
    " - File ist für die Auswahl der Datei verantwortlich, die ausgewertet und geplottet werden soll.\n",
    " - Columns gibt an, welche Spalten ausgewählt werden sollen. Hierbei können mehrere Spalten ausgewählt werden. Die Spalten werden dann mit unterschiedlichen Farben geplottet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6efc6220-a14b-4ff0-b0d8-66ba17569050",
   "metadata": {},
   "source": [
    "## 4. Preprocessing\n",
    "Aufgeteilt nach KIDAQ und VIB\n",
    "- Auswahl der Fenstergröße in Millisekunden\n",
    "- Auswahl des Setups\n",
    "- Frequenzanalyse (Fourier-Transformation)\n",
    "    - Abtastrate\n",
    "    - Maximalfrequenz\n",
    "    - Feature Resampling\n",
    "    - Fenstergröße der Aggregationen nach Frequenzbereichen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae00846f",
   "metadata": {},
   "source": [
    "### 4.1 Parametrierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e61290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing parameters\n",
    "\n",
    "windowSizeSlider = widgets.IntSlider(\n",
    "    value=PREPROCESSING_WINDOW_SIZE_MS_DEFAULT,\n",
    "    min=PREPROCESSING_WINDOW_SIZE_MS_MIN,\n",
    "    max=PREPROCESSING_WINDOW_SIZE_MS_MAX,\n",
    "    step=100,\n",
    "    description='Window size:'\n",
    ")\n",
    "display(windowSizeSlider)\n",
    "\n",
    "dataSourceDropdown = widgets.Dropdown(\n",
    "    options=RAW_DATA_TYPE,\n",
    "    description='Data source:'\n",
    ")\n",
    "\n",
    "display(dataSourceDropdown)\n",
    "\n",
    "setups = list(filter(lambda x: os.path.isdir(os.path.join(PATH_RAW_DATA, x)), os.listdir(PATH_RAW_DATA)))\n",
    "\n",
    "setupSelectionDropdown = widgets.Dropdown(\n",
    "    options=setups,\n",
    "    description='Setup:'\n",
    ")\n",
    "\n",
    "display(setupSelectionDropdown)\n",
    "\n",
    "\n",
    "\n",
    "sampleRateInput = widgets.IntText(\n",
    "    value=DEFAULT_SAMPLE_RATE,\n",
    "    description='Sample rate:',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "maxFrequencyInput = widgets.IntText(\n",
    "    value=DEFAULT_SAMPLE_RATE / 10,\n",
    "    description='Max. frequency:',\n",
    ")\n",
    "\n",
    "frequencyResampleInput = widgets.IntText(\n",
    "    value=DEFAULT_SAMPLE_RATE / 100,\n",
    "    description='Resample count:',\n",
    ")\n",
    "\n",
    "frequencyBandCountSlider = widgets.IntSlider(\n",
    "    value=PREPROCESSING_FREQUENCY_BAND_COUNT_DEFAULT,\n",
    "    min=PREPROCESSING_FREQUENCY_BAND_COUNT_MIN,\n",
    "    max=PREPROCESSING_FREQUENCY_BAND_COUNT_MAX,\n",
    "    step=10,\n",
    "    description='Freq. bands:'\n",
    ")\n",
    "\n",
    "displayed = False\n",
    "def showFrequencyAnalysisInput():\n",
    "    global displayed\n",
    "    if not displayed:\n",
    "        display(sampleRateInput)\n",
    "        display(maxFrequencyInput)\n",
    "        display(frequencyResampleInput)\n",
    "        display(frequencyBandCountSlider)\n",
    "        displayed = True\n",
    "\n",
    "selectFrequencyAnalysis = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Frequency analysis:'\n",
    ")\n",
    "\n",
    "selectFrequencyAnalysis.observe(lambda x: showFrequencyAnalysisInput(), names='value')\n",
    "\n",
    "display(selectFrequencyAnalysis)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT helper functions\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "def compute_top_n_amplitudes(fft_values, n):\n",
    "    sorted_indices = np.argsort(fft_values)  # Sortiere die Amplituden\n",
    "    top_indices = sorted_indices[-n:]  # Wähle die Top-N-Amplituden aus\n",
    "    top_amplitudes = fft_values[top_indices]\n",
    "    return top_amplitudes[::-1]\n",
    "\n",
    "\n",
    "def compute_top_n_frequencies(fft_values, n):\n",
    "    sorted_indices = np.argsort(fft_values)  # Sortiere die Amplituden\n",
    "    top_frequencies = sorted_indices[-n:]  # Wähle die Top-N-Amplituden aus\n",
    "    return top_frequencies[::-1]\n",
    "\n",
    "\n",
    "def compute_energy_in_bands(fft_values, bands):\n",
    "    energy = []\n",
    "    for band in bands:\n",
    "        band_values = fft_values[band[0] : band[1]]  # Frequenzbereich für das Band\n",
    "        band_energy = np.sum(np.abs(band_values) ** 2)  # Berechne die Energie im Band\n",
    "        energy.append(band_energy)\n",
    "    return energy\n",
    "\n",
    "\n",
    "def compute_spectral_features(fft_values):\n",
    "    spectral_skewness = skew(np.abs(fft_values))\n",
    "    spectral_kurt = kurtosis(np.abs(fft_values))\n",
    "    amplitudes = np.abs(fft_values)  # Berechnung der Amplituden\n",
    "    geometric_mean = np.exp(\n",
    "        np.mean(np.log(amplitudes[amplitudes != 0]))\n",
    "    )  # Berechnung des geometrischen Mittelwerts\n",
    "\n",
    "    spectral_flatness = geometric_mean / (\n",
    "        np.mean(amplitudes) + np.finfo(float).eps\n",
    "    )  # Berechnung des spektralen Flachheitsmaßes\n",
    "    return spectral_skewness, spectral_kurt, spectral_flatness\n",
    "\n",
    "\n",
    "def compute_frequency_bands_mean(fft_values, bands):\n",
    "    band_features = []\n",
    "    for band in bands:\n",
    "        band_values = fft_values[band[0] : band[1]]\n",
    "        band_feature = np.mean(\n",
    "            np.abs(band_values)\n",
    "        )  # Hier: Durchschnittliche Amplitude im Band\n",
    "        band_features.append(band_feature)\n",
    "    return band_features\n",
    "\n",
    "\n",
    "def compute_harmonic_dominance(fft_values):\n",
    "    fundamental_amplitude = np.abs(fft_values[0])  # Amplitude der Grundfrequenz\n",
    "    harmonic_amplitudes = np.abs(\n",
    "        fft_values[1:]\n",
    "    )  # Amplituden der harmonischen Frequenzen\n",
    "    total_harmonic_amplitude = np.sum(\n",
    "        harmonic_amplitudes\n",
    "    )  # Summe der harmonischen Amplituden\n",
    "    harmonic_dominance = total_harmonic_amplitude / (\n",
    "        fundamental_amplitude + total_harmonic_amplitude\n",
    "    )\n",
    "    return harmonic_dominance\n",
    "\n",
    "\n",
    "def compute_periodicity(fft_values, n):\n",
    "    fundamental_amplitude = np.abs(fft_values[0])  # Amplitude der Grundfrequenz\n",
    "    residual_amplitudes = np.abs(\n",
    "        fft_values[1:]\n",
    "    )  # Amplituden der nicht-grundfrequenten Komponenten\n",
    "    significant_residuals = residual_amplitudes[-n:]  # Filtere signifikante Komponenten\n",
    "    periodicity = np.sum(significant_residuals) / fundamental_amplitude\n",
    "    return periodicity\n",
    "\n",
    "\n",
    "def count_harmonic_peaks(fft_values, relative_peak_threshold):\n",
    "    threshold = relative_peak_threshold * np.max(\n",
    "        np.abs(fft_values)\n",
    "    )  # Schwellwert für die Peak-Erkennung\n",
    "    peaks, _ = find_peaks(\n",
    "        np.abs(fft_values), height=threshold\n",
    "    )  # Finde Peaks im Amplitudenspektrum\n",
    "    harmonic_peak_count = (\n",
    "        len(peaks) - 1\n",
    "    )  # Zähle die Anzahl der Peaks (ohne die Grundfrequenz)\n",
    "    return harmonic_peak_count\n",
    "\n",
    "\n",
    "def extractBandFeatures(\n",
    "    fft,\n",
    "    fftFreq,\n",
    "    maxFrequency,\n",
    "    resolution,\n",
    "    frequencyBandCount,\n",
    "    relativePeakThreshold=0.25,\n",
    "):\n",
    "    fftDf = pd.DataFrame(np.abs(fft), index=fftFreq)\n",
    "    fftDf = fftDf.loc[(fftDf.index >= 0) & (fftDf.index <= maxFrequency)]\n",
    "\n",
    "    flattenedFFT = fftDf.values.flatten()\n",
    "    bands = [\n",
    "        [\n",
    "            int(i * maxFrequency * resolution / frequencyBandCount),\n",
    "            (int((i + 1) * maxFrequency * resolution / frequencyBandCount) - 1),\n",
    "        ]\n",
    "        for i in range(frequencyBandCount)\n",
    "    ]\n",
    "    top_n_amplitudes = compute_top_n_amplitudes(flattenedFFT, frequencyBandCount)\n",
    "    top_n_frequencies = compute_top_n_frequencies(flattenedFFT, frequencyBandCount)\n",
    "    band_energy = compute_energy_in_bands(flattenedFFT, bands)\n",
    "    spectral_skewness, spectral_kurt, spectral_flatness = compute_spectral_features(\n",
    "        flattenedFFT\n",
    "    )\n",
    "    frequency_bands = compute_frequency_bands_mean(flattenedFFT, bands)\n",
    "    harmonic_dominance = compute_harmonic_dominance(flattenedFFT)\n",
    "    periodicity = compute_periodicity(flattenedFFT, frequencyBandCount)\n",
    "    harmonic_peak_count = count_harmonic_peaks(flattenedFFT, relativePeakThreshold)\n",
    "    data = np.concatenate(\n",
    "        (\n",
    "            top_n_amplitudes,\n",
    "            top_n_frequencies / resolution,\n",
    "            band_energy,\n",
    "            frequency_bands,\n",
    "            [\n",
    "                spectral_skewness,\n",
    "                spectral_kurt,\n",
    "                spectral_flatness,\n",
    "                harmonic_dominance,\n",
    "                periodicity,\n",
    "                harmonic_peak_count,\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    feature_names = np.concatenate(\n",
    "        (\n",
    "            [f\"amplitude_{i+1}_highest_energy\" for i in range(frequencyBandCount)],\n",
    "            [f\"amplitude_{i+1}_highest_freq\" for i in range(frequencyBandCount)],\n",
    "            [\n",
    "                f\"band_{band[0]/resolution}_{band[1]/resolution}_energy\"\n",
    "                for band in bands\n",
    "            ],\n",
    "            [\n",
    "                f\"band_{band[0]/resolution}_{band[1]/resolution}_energy_mean\"\n",
    "                for band in bands\n",
    "            ],\n",
    "            [\n",
    "                \"spectral_skewness\",\n",
    "                \"spectral_kurt\",\n",
    "                \"spectral_flatness\",\n",
    "                \"harmonic_dominance\",\n",
    "                \"periodicity\",\n",
    "                \"harmonic_peak_count\",\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.index = feature_names\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extractFlattenedFeatureNames(dataFrame):\n",
    "    featureNames = []\n",
    "    for row in dataFrame.index:\n",
    "        for col in dataFrame.columns:\n",
    "            featureNames.append(\"_\".join([row, col]))\n",
    "    return featureNames\n",
    "\n",
    "\n",
    "def extractAggregationFeatures(fft, fftFreq, maxFrequency, aggregationResampleCount):\n",
    "    fftDf = pd.DataFrame(np.abs(fft), index=fftFreq)\n",
    "    fftDf = fftDf.loc[(fftDf.index >= 0) & (fftDf.index <= maxFrequency)]\n",
    "\n",
    "    windowSize = int(maxFrequency / aggregationResampleCount)\n",
    "    fftMax = fftDf.max()\n",
    "\n",
    "    windowLabels = [\n",
    "        f\"{windowSize * i}_{windowSize * (i+1)}\"\n",
    "        for i in range(aggregationResampleCount)\n",
    "    ]\n",
    "    fftDf.index = pd.cut(\n",
    "        fftDf.index, bins=aggregationResampleCount, labels=windowLabels\n",
    "    )\n",
    "    fftWindows = fftDf.groupby(fftDf.index)\n",
    "    fftDf = pd.concat(\n",
    "        [\n",
    "            fftWindows.std().fillna(0),\n",
    "            (fftWindows.max() - fftWindows.min()).fillna(0),\n",
    "            fftWindows.aggregate(lambda sample: stats.iqr(sample.to_numpy())).fillna(0),\n",
    "            (fftWindows.mean() - fftWindows.median()).fillna(0),\n",
    "            (fftWindows.max() / fftMax).fillna(0),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    fftDf.columns = [\"std\", \"range\", \"iqr\", \"mean_median\", \"relative_max\"]\n",
    "    fftDf.index.name = \"freq_window\"\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        fftDf.to_numpy().flatten(), index=extractFlattenedFeatureNames(fftDf)\n",
    "    )\n",
    "\n",
    "\n",
    "def fftFeatureExtraction(\n",
    "    samples, sampleRate, maxFrequency, aggregationResampleCount, frequencyBandCount\n",
    "):\n",
    "    resolution = len(samples) / sampleRate\n",
    "    fft = np.fft.fft(samples)\n",
    "    fftFreq = np.fft.fftfreq(len(samples), 1 / sampleRate)\n",
    "    aggregationFeatures = extractAggregationFeatures(\n",
    "        fft, fftFreq, maxFrequency, aggregationResampleCount\n",
    "    )\n",
    "    bandFeatures = extractBandFeatures(\n",
    "        fft, fftFreq, maxFrequency, resolution, frequencyBandCount\n",
    "    )\n",
    "    return pd.concat([aggregationFeatures, bandFeatures])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9805956",
   "metadata": {},
   "source": [
    "### 4.2 Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the preprocessing\n",
    "import re\n",
    "from asammdf import MDF\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "windowSize = windowSizeSlider.value\n",
    "setup = setupSelectionDropdown.value\n",
    "rawDataType = dataSourceDropdown.value\n",
    "frequencyAnalysis = selectFrequencyAnalysis.value\n",
    "sampleRate = sampleRateInput.value\n",
    "maxFrequency = maxFrequencyInput.value\n",
    "resampleCount = frequencyResampleInput.value\n",
    "frequencyBandCount = frequencyBandCountSlider.value\n",
    "\n",
    "fileName = f\"{setup}_{rawDataType}_{windowSize}.csv\"\n",
    "\n",
    "if frequencyAnalysis:\n",
    "    fileName = f\"{setup}_{rawDataType}_{windowSize}ws_{sampleRate}sr_{maxFrequency}mf_{resampleCount}rc_{frequencyBandCount}fbc.csv\"\n",
    "\n",
    "\n",
    "outputFile = open(\n",
    "    os.path.join(PATH_FEATURE_DATA, fileName),\n",
    "    \"w\",\n",
    ")\n",
    "\n",
    "headerWritten = False\n",
    "match rawDataType:\n",
    "    case \"KIDAQ\":\n",
    "        columns = KIDAQ_DEFAULT_FEATURES\n",
    "        sensorInColumns = []\n",
    "\n",
    "        kidaqFiles = list(\n",
    "            pl.Path(os.path.join(PATH_RAW_DATA, setup)).glob(KIDAQ_FILE_SEARCH_PATTERN)\n",
    "        )\n",
    "        for file in tqdm(kidaqFiles):\n",
    "            testName = re.search(TEST_NAME_REGEX, str(file)).group(0)\n",
    "            testType = re.search(TEST_TYPE_REGEX, str(file)).group(0)\n",
    "            secondFormat = re.search(KIDAQ_FILE_FIX_REGEX, file.name)\n",
    "            if secondFormat:\n",
    "                rpm = secondFormat.group(1)\n",
    "                flowRate = secondFormat.group(2)\n",
    "            else:\n",
    "                rpm = re.search(KIDAQ_FILE_RPM_REGEX, file.name).group(1)\n",
    "                flowRate = re.search(KIDAQ_FILE_FLOW_RATE_REGEX, file.name).group(1)\n",
    "\n",
    "            mdf = MDF(file)\n",
    "            df = mdf.filter([\"p1\", \"p2\"]).to_dataframe()\n",
    "            df.index = pd.to_timedelta(df.index, unit=\"s\")\n",
    "            resampled = df.resample(str(windowSize) + \"ms\")\n",
    "\n",
    "            median = resampled.median().to_numpy()\n",
    "            mean = resampled.mean().to_numpy()\n",
    "            max_val = resampled.max().to_numpy()\n",
    "            min_val = resampled.min().to_numpy()\n",
    "            std = resampled.std().to_numpy()\n",
    "            range_val = max_val - min_val\n",
    "            iqr = resampled.aggregate(\n",
    "                lambda sample: stats.iqr(sample.to_numpy())\n",
    "            ).to_numpy()\n",
    "\n",
    "            result = np.concatenate(\n",
    "                (\n",
    "                    np.repeat(\n",
    "                        [[testName, testType, flowRate, rpm]],\n",
    "                        len(std),\n",
    "                        axis=0,\n",
    "                    ),\n",
    "                    std,\n",
    "                    range_val,\n",
    "                    iqr,\n",
    "                    median - mean,\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "            if frequencyAnalysis:\n",
    "                fftAnalysisWindows = []\n",
    "                for samples in resampled:\n",
    "                    fftAnalysis = []\n",
    "                    for sensor in samples[1]:\n",
    "                        frequencyFeatures = fftFeatureExtraction(\n",
    "                            samples[1][sensor],\n",
    "                            sampleRate,\n",
    "                            maxFrequency,\n",
    "                            resampleCount,\n",
    "                            frequencyBandCount,\n",
    "                        )\n",
    "                        if not sensor in sensorInColumns:\n",
    "                            columns = np.concatenate(\n",
    "                                (columns, sensor + \"_\" + frequencyFeatures.index)\n",
    "                            )\n",
    "                            sensorInColumns.append(sensor)\n",
    "                        fftAnalysis.append(frequencyFeatures.to_numpy().flatten())\n",
    "                    fftAnalysisWindows.append(np.concatenate(fftAnalysis, axis=0))\n",
    "                result = np.concatenate(\n",
    "                    (result, fftAnalysisWindows),\n",
    "                    axis=1,\n",
    "                )\n",
    "\n",
    "            if not headerWritten:\n",
    "                outputFile.write(\";\".join(columns) + \"\\n\")\n",
    "                headerWritten = True\n",
    "            for row in result:\n",
    "                outputFile.write(\";\".join(row) + \"\\n\")\n",
    "\n",
    "    case \"VIB\":\n",
    "        columns = VIB_DEFAULT_FEATURES\n",
    "        sensorInColumns = []\n",
    "\n",
    "        vibFiles = list(\n",
    "            pl.Path(os.path.join(PATH_RAW_DATA, setup)).glob(VIB_FILE_SEARCH_PATTERN)\n",
    "        )\n",
    "\n",
    "        currentTestFiles = []\n",
    "        currentTestSzenario = \"\"\n",
    "        for file in tqdm(vibFiles):\n",
    "            testSzenario = os.path.dirname(file)\n",
    "\n",
    "            if testSzenario != currentTestSzenario and len(currentTestFiles) > 0:\n",
    "                sensorData = []\n",
    "                for testFile in currentTestFiles:\n",
    "                    testType = re.search(TEST_TYPE_REGEX, str(testFile)).group(0)\n",
    "                    testName = re.search(TEST_NAME_REGEX, str(file)).group(0)\n",
    "\n",
    "                    rpm = re.search(VIB_FILE_RPM_REGEX, str(testFile)).group(1)\n",
    "                    flowRate = re.search(VIB_FILE_FLOW_RATE_REGEX, str(testFile)).group(\n",
    "                        1\n",
    "                    )\n",
    "                    sensor = re.search(VIB_FILE_SENSOR_REGEX, testFile.name).group(1)\n",
    "\n",
    "                    dataFrame = pd.read_csv(\n",
    "                        testFile,\n",
    "                        skiprows=2,\n",
    "                        encoding=\"ISO-8859-1\",\n",
    "                        sep=\";\",\n",
    "                        index_col=0,\n",
    "                    )\n",
    "                    dataFrame.index = pd.to_datetime(dataFrame.index, unit=\"ns\")\n",
    "\n",
    "                    while len(sensorData) < VIB_SENSOR_COUNT:\n",
    "                        placeholder = pd.DataFrame(\n",
    "                            0, index=dataFrame.index, columns=dataFrame.columns\n",
    "                        )\n",
    "                        placeholder.rename(\n",
    "                            columns={\"Value\": \"VIB\" + str(len(sensorData) + 1)},\n",
    "                            inplace=True,\n",
    "                        )\n",
    "                        sensorData.append(placeholder)\n",
    "\n",
    "                    dataFrame.rename(\n",
    "                        columns={\"Value\": \"VIB\" + str(sensor)}, inplace=True\n",
    "                    )\n",
    "\n",
    "                    sensorData[int(sensor) - 1] = dataFrame\n",
    "\n",
    "                currentTestFiles = []\n",
    "\n",
    "                mergedSensorData = sensorData[0]\n",
    "                for i in range(1, len(sensorData)):\n",
    "                    mergedSensorData = pd.merge_asof(\n",
    "                        mergedSensorData,\n",
    "                        sensorData[i],\n",
    "                        left_index=True,\n",
    "                        right_index=True,\n",
    "                        tolerance=pd.Timedelta(str(windowSize // 100) + \"ms\"),\n",
    "                    )\n",
    "\n",
    "                resampled = mergedSensorData.resample(\n",
    "                    str(windowSize) + \"ms\", origin=\"end\"\n",
    "                )\n",
    "\n",
    "                median = resampled.median().to_numpy()\n",
    "                mean = resampled.mean().to_numpy()\n",
    "                max_val = resampled.max().to_numpy()\n",
    "                min_val = resampled.min().to_numpy()\n",
    "                std = resampled.std().to_numpy()\n",
    "\n",
    "                range_val = max_val - min_val\n",
    "                iqr = resampled.aggregate(\n",
    "                    lambda sample: stats.iqr(sample.to_numpy())\n",
    "                ).to_numpy()\n",
    "\n",
    "                result = np.concatenate(\n",
    "                    (\n",
    "                        np.repeat(\n",
    "                            [[testName, testType, flowRate, rpm]],\n",
    "                            len(std),\n",
    "                            axis=0,\n",
    "                        ),\n",
    "                        std,\n",
    "                        range_val,\n",
    "                        iqr,\n",
    "                        median - mean,\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "                if frequencyAnalysis:\n",
    "                    fftAnalysisWindows = []\n",
    "                    for samples in resampled:\n",
    "                        fftAnalysis = []\n",
    "                        if samples[1].shape[0] >= (\n",
    "                            maxFrequency * (windowSize / 1000) * 2\n",
    "                        ):\n",
    "                            for sensor in samples[1]:\n",
    "                                sensorSamples = samples[1][sensor]\n",
    "                                frequencyFeatures = fftFeatureExtraction(\n",
    "                                    sensorSamples,\n",
    "                                    sampleRate,\n",
    "                                    maxFrequency,\n",
    "                                    resampleCount,\n",
    "                                    frequencyBandCount,\n",
    "                                )\n",
    "                                if not sensor in sensorInColumns:\n",
    "                                    columns = np.concatenate(\n",
    "                                        (\n",
    "                                            columns,\n",
    "                                            sensor + \"_\" + frequencyFeatures.index,\n",
    "                                        )\n",
    "                                    )\n",
    "                                    sensorInColumns.append(sensor)\n",
    "                                fftAnalysis.append(\n",
    "                                    frequencyFeatures.to_numpy().flatten()\n",
    "                                )\n",
    "                        else:\n",
    "                            fftAnalysis = [\n",
    "                                np.array([0])\n",
    "                                for i in range(\n",
    "                                    samples[1].shape[1]\n",
    "                                    * (frequencyBandCount * 4 + 6 + resampleCount * 5)\n",
    "                                )\n",
    "                            ]\n",
    "                        fftAnalysisWindows.append(np.concatenate(fftAnalysis, axis=0))\n",
    "\n",
    "                    result = np.concatenate(\n",
    "                        (result, np.array(fftAnalysisWindows)),\n",
    "                        axis=1,\n",
    "                    )\n",
    "\n",
    "                result = result[~np.any(result == \"nan\", axis=1)]\n",
    "\n",
    "                if not headerWritten:\n",
    "                    outputFile.write(\";\".join(columns) + \"\\n\")\n",
    "                    headerWritten = True\n",
    "                for row in result:\n",
    "                    outputFile.write(\";\".join(row) + \"\\n\")\n",
    "\n",
    "            currentTestSzenario = testSzenario\n",
    "            currentTestFiles.append(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e71a651",
   "metadata": {},
   "source": [
    "### 4.3 Vorbereitung der Tainings- und Testdaten\n",
    "- Vorverarbeitete Daten auswählen\n",
    "- Features wählen\n",
    "- Nach Spaltenwert filtern (z.B. Betriebspunkte oder Szenarien)\n",
    "- neuen Namen eingeben und speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT PREPROCESSED DATA\n",
    "clear_output(wait=True)\n",
    "featureDataDir = list(\n",
    "    filter(\n",
    "        lambda x: os.path.isfile(os.path.join(PATH_FEATURE_DATA, x)),\n",
    "        os.listdir(PATH_FEATURE_DATA),\n",
    "    )\n",
    ")\n",
    "\n",
    "preprocessedData = pd.DataFrame()\n",
    "preprocessedFileDropdown = widgets.Dropdown(description=\"preprocessed data\")\n",
    "preprocessedFileDropdown.options = featureDataDir\n",
    "display(preprocessedFileDropdown)\n",
    "\n",
    "# SELECT FEATURES\n",
    "featureSelection = widgets.SelectMultiple(description=\"features\")\n",
    "display(featureSelection)\n",
    "\n",
    "rowSelectorColumn = widgets.Dropdown(description=\"row selector column\")\n",
    "display(rowSelectorColumn)\n",
    "rowSelection = widgets.SelectMultiple(description=\"rows\")\n",
    "display(rowSelection)\n",
    "\n",
    "\n",
    "def updateDynamicFields():\n",
    "    global featureSelection\n",
    "    global rowSelection\n",
    "    global rowSelectorColumn\n",
    "    featureSelection.options = list(preprocessedData.columns)\n",
    "    featureSelection.value = featureSelection.options\n",
    "    rowSelectorColumn.options = list(preprocessedData.columns)\n",
    "    rowSelectorColumn.value = rowSelectorColumn.options[0]\n",
    "\n",
    "    \n",
    "    rowSelection.options = list(preprocessedData[rowSelectorColumn.value].unique())\n",
    "    rowSelection.value = rowSelection.options\n",
    "    \n",
    "\n",
    "def rowSelectorColumnChange(change):\n",
    "    global rowSelection\n",
    "    rowSelection.options = list(preprocessedData[change.new].unique())\n",
    "    rowSelection.value = rowSelection.options\n",
    "\n",
    "\n",
    "def onPreprocessedFileChange(change):\n",
    "    global preprocessedData\n",
    "    preprocessedData = pd.read_csv(os.path.join(PATH_FEATURE_DATA, change.new), sep=\";\")\n",
    "    updateDynamicFields()\n",
    "\n",
    "\n",
    "preprocessedFileDropdown.observe(onPreprocessedFileChange, names=\"value\")\n",
    "rowSelectorColumn.observe(rowSelectorColumnChange, names=\"value\")\n",
    "\n",
    "saveName = widgets.Text(description=\"new name\")\n",
    "display(saveName)\n",
    "saveButton = widgets.Button(description=\"save\")\n",
    "display(saveButton)\n",
    "\n",
    "def onSaveButtonClick(b):\n",
    "    global preprocessedData\n",
    "    global featureSelection\n",
    "    global rowSelection\n",
    "    global saveName\n",
    "    global rowSelectorColumn\n",
    "    global saveButton\n",
    "    saveButton.style.button_color = \"lightblue\"\n",
    "\n",
    "    selectedFeatures = featureSelection.value\n",
    "    selectedRows = rowSelection.value\n",
    "    rowSelector = rowSelectorColumn.value\n",
    "\n",
    "    preprocessedData = pd.read_csv(os.path.join(PATH_FEATURE_DATA, preprocessedFileDropdown.value), sep=\";\")\n",
    "\n",
    "    if len(selectedFeatures) > 0 and len(selectedRows) > 0 and len(rowSelector) > 0:\n",
    "        preprocessedData = preprocessedData[preprocessedData[rowSelector].isin(selectedRows)]\n",
    "        preprocessedData = preprocessedData.loc[:, selectedFeatures]\n",
    "\n",
    "        preprocessedData.to_csv(os.path.join(PATH_FEATURE_DATA, saveName.value + \".csv\"), sep=\";\", index=False)\n",
    "        saveButton.style.button_color = \"lightgreen\"\n",
    "\n",
    "\n",
    "saveButton.on_click(onSaveButtonClick)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b92fefe-672c-4bba-97c1-f78747ff1dc1",
   "metadata": {},
   "source": [
    "<a name=\"Maschinelles-Lernen\"></a>\n",
    "## 5. Maschinelles Lernen\n",
    "\n",
    "In diesem Bereich können, mithilfe drei verschiedener Learner, Modell anhand der generierten Trainings- und Testdaten erzeugt werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56f2b565",
   "metadata": {},
   "source": [
    "### 5.1 Trainings- und Testdaten wählen und laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da6989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT TRAINING AND TEST DATA\n",
    "clear_output(wait=True)\n",
    "featureDataDir = list(filter(lambda x: os.path.isfile(os.path.join(PATH_FEATURE_DATA, x)), os.listdir(PATH_FEATURE_DATA)))\n",
    "\n",
    "trainFileDropdown = widgets.Dropdown(description=\"training data\")\n",
    "trainFileDropdown.options = featureDataDir\n",
    "display(trainFileDropdown)\n",
    "\n",
    "testFileDropdown = widgets.Dropdown(description=\"test data\")\n",
    "testFileDropdown.options = featureDataDir\n",
    "display(testFileDropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51491a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "csvTrain = read_csv(PATH_FEATURE_DATA + trainFileDropdown.value, delimiter=\";\")\n",
    "csvTest = read_csv(PATH_FEATURE_DATA + testFileDropdown.value, delimiter=\";\")\n",
    "\n",
    "DATA_INDEX_CLASS = 1\n",
    "DATA_INDEX_TIME_BASED = 2\n",
    "DATA_INDEX_FREQ_BASED = 12\n",
    "\n",
    "trainData = csvTrain.values\n",
    "testData = csvTest.values\n",
    "\n",
    "\n",
    "featureNames = csvTrain.columns.values[DATA_INDEX_TIME_BASED:].tolist()\n",
    "trainX, trainY = trainData[:, DATA_INDEX_TIME_BASED:].astype(np.float32), trainData[:, DATA_INDEX_CLASS:DATA_INDEX_CLASS+1]\n",
    "testX, testY = testData[:, DATA_INDEX_TIME_BASED:].astype(np.float32), testData[:, DATA_INDEX_CLASS:DATA_INDEX_CLASS+1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c9baf4d",
   "metadata": {},
   "source": [
    "### 5.2 Entscheidungsbaum - sklearn (empfohlen)\n",
    "\n",
    "In diesem Abschnitt wird mit der sklearn-Bibliothek ein Entscheidungsbaummodell trainiert. Dabei können verschiedene Parameter frei gewählt werden, wie die maximale Baumtiefe, die gewünschte Mindestgenauigkeit und die Anzahl der Suchiterationen.\n",
    "Das Programm wird dann versuchen, ausgehend von einer Baumtiefe von eins, einen möglichst einfachen Entscheidungsbaum zu generieren, der die gewünschte Genauigkeit erreicht."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1db3b37",
   "metadata": {},
   "source": [
    "#### 5.2.1 Konfiguration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ddb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_MAX_DEPTH = 8                           # Maximum depth of the tree\n",
    "DT_EXPLORATION_TARGET_VAL_ACCURACY = 0.6   # Target accuracy for the decision tree\n",
    "DT_EXPLORATION_MAX_ITER = 100              # Maximum number of iterations for the random search\n",
    "DT_VERBOSE = True                         # Verbose logging for the decision tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdd07539",
   "metadata": {},
   "source": [
    "#### 5.2.2 Trainieren 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6a26fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train DecisionTreeClassifier\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m tree\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n",
      "File \u001b[1;32mc:\\Users\\phili\\Github_Philipp\\pumpen_projekt\\venv\\Lib\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[39m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\phili\\Github_Philipp\\pumpen_projekt\\venv\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_metadata_requests\u001b[39;00m \u001b[39mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[1;32mc:\\Users\\phili\\Github_Philipp\\pumpen_projekt\\venv\\Lib\\site-packages\\sklearn\\utils\\__init__.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m compress, islice\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m issparse\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning\n",
      "File \u001b[1;32mc:\\Users\\phili\\Github_Philipp\\pumpen_projekt\\venv\\Lib\\site-packages\\scipy\\sparse\\__init__.py:287\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_matrix_io\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    286\u001b[0m \u001b[39m# For backward compatibility with v0.19.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m csgraph\n\u001b[0;32m    289\u001b[0m \u001b[39m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    291\u001b[0m     base, bsr, compressed, construct, coo, csc, csr, data, dia, dok, extract,\n\u001b[0;32m    292\u001b[0m     lil, sparsetools, sputils\n\u001b[0;32m    293\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\phili\\Github_Philipp\\pumpen_projekt\\venv\\Lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:185\u001b[0m\n\u001b[0;32m    157\u001b[0m __docformat__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrestructuredtext en\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mconnected_components\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    160\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mlaplacian\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    161\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mshortest_path\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mcsgraph_to_masked\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    183\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mNegativeCycleError\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 185\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_laplacian\u001b[39;00m \u001b[39mimport\u001b[39;00m laplacian\n\u001b[0;32m    186\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_shortest_path\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    187\u001b[0m     shortest_path, floyd_warshall, dijkstra, bellman_ford, johnson,\n\u001b[0;32m    188\u001b[0m     NegativeCycleError\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    190\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_traversal\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    191\u001b[0m     breadth_first_order, depth_first_order, breadth_first_tree,\n\u001b[0;32m    192\u001b[0m     depth_first_tree, connected_components\n\u001b[0;32m    193\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\phili\\Github_Philipp\\pumpen_projekt\\venv\\Lib\\site-packages\\scipy\\sparse\\csgraph\\_laplacian.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m issparse\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearOperator\n\u001b[0;32m     10\u001b[0m \u001b[39m###############################################################################\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# Graph laplacian\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlaplacian\u001b[39m(\n\u001b[0;32m     13\u001b[0m     csgraph,\n\u001b[0;32m     14\u001b[0m     normed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     symmetrized\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\phili\\Github_Philipp\\pumpen_projekt\\venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py:123\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_dsolve\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    122\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_interface\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m--> 123\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_eigen\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_matfuncs\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    125\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_onenormest\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\phili\\Github_Philipp\\pumpen_projekt\\venv\\Lib\\site-packages\\scipy\\sparse\\linalg\\_eigen\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marpack\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlobpcg\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_svds\u001b[39;00m \u001b[39mimport\u001b[39;00m svds\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m arpack\n\u001b[0;32m     15\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mArpackError\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mArpackNoConvergence\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[39m'\u001b[39m\u001b[39meigs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39meigsh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlobpcg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msvds\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     18\u001b[0m ]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train DecisionTreeClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import clear_output\n",
    "import joblib\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder = labelEncoder.fit(np.ravel(trainY))\n",
    "labelEncodedTrainY = labelEncoder.transform(np.ravel(trainY))\n",
    "labelEncodedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "\n",
    "testAccuracy = 0.0\n",
    "trainAccuracy = 0.0\n",
    "iterations = 0\n",
    "depth = 0\n",
    "\n",
    "while (\n",
    "    testAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    "    or trainAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    ") and iterations < DT_EXPLORATION_MAX_ITER:\n",
    "    # Create decision tree classifier\n",
    "    depth = 1 + int(DT_MAX_DEPTH * (iterations / DT_EXPLORATION_MAX_ITER) // 1)\n",
    "\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, splitter=\"random\")\n",
    "    dt.fit(trainX, labelEncodedTrainY)\n",
    "\n",
    "    testPredictions = dt.predict(testX)\n",
    "    testAccuracy = accuracy_score(labelEncodedTestY, testPredictions)\n",
    "\n",
    "    trainPredictions = dt.predict(trainX)\n",
    "    trainAccuracy = accuracy_score(labelEncodedTrainY, trainPredictions)\n",
    "    iterations += 1\n",
    "\n",
    "    if DT_VERBOSE:\n",
    "        print(\"Iteration: %d\" % iterations)\n",
    "        print(\"Depth: %d\" % depth)\n",
    "        print(\"Train Accuracy: %.2f%%\" % (trainAccuracy * 100.0))\n",
    "        print(\"Test Accuracy: %.2f%%\" % (testAccuracy * 100.0))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Iteration: %d\" % iterations)\n",
    "print(\"Depth: %d\" % depth)\n",
    "print(\"Train Accuracy: %.2f%%\" % (trainAccuracy * 100.0))\n",
    "print(\"Test Accuracy: %.2f%%\" % (testAccuracy * 100.0))\n",
    "\n",
    "modelName = \"dtc.\" + str(round(time.time()))\n",
    "os.makedirs(PATH_MODEL + modelName)\n",
    "with open(PATH_MODEL + modelName + \"/encoder.pickle\", \"wb\") as f:\n",
    "    joblib.dump(labelEncoder, f)\n",
    "joblib.dump(dt, PATH_MODEL + modelName + \"/dtc.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9316a0aa",
   "metadata": {},
   "source": [
    "### 5.3 Tiefes neuronales Netz - tensorflow\n",
    "\n",
    "Im folgenden Abschnitt wird mit der tensorflow-Bibliothek ein tiefes neuronales Netzwerk trainiert. Dabei können verschiedene Parameter eingestellt werden, wie die gewünschte Mindestgenauigkeit, die Anzahl der Suchiterationen, die minimale und maximale Anzahl an Schichten und Neuronen, die Toleranz für einen vorzeitigen Abbruch einer Iteration, die Ausgabeform, die maximale Anzahl an Epochen, die Batchgröße und die Batch Normalisierung.\n",
    "\n",
    "Bitte beachte, dass die genaue Syntax und die verfügbaren Optionen von tensorflow abhängen können. Es ist empfehlenswert, die offizielle tensorflow-Dokumentation für detaillierte Informationen zu konsultieren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1731eede",
   "metadata": {},
   "source": [
    "#### 5.3.1 Konfiguration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_EXPLORATION_TARGET_VAL_ACCURACY = 0.9  # Target accuracy for the neural network\n",
    "DNN_EXPLORATION_MAX_ITER = 50              # Maximum number of iterations for the random search\n",
    "DNN_EXPLORATION_HIDDEN_LAYERS_MIN = 1       # Minimum number of hidden layers\n",
    "DNN_EXPLORATION_HIDDEN_LAYERS_MAX = 3       # Maximum number of hidden layers\n",
    "DNN_EXPLORATION_NEURONS_MIN = 8             # Minimum number of neurons per layer\n",
    "DNN_EXPLORATION_NEURONS_MAX = 32            # Maximum number of neurons per layer\n",
    "\n",
    "\n",
    "DNN_EARLY_STOPPING_PATIENCE = 50            # Patience for early stopping\n",
    "DNN_VERBOSE = 0                             # Verbosity level for the neural network\n",
    "DNN_EPOCHS = 2000                           # Maximum number of epochs for the neural network\n",
    "DNN_BATCH_SIZE = 128                        # Batch size for the neural network\n",
    "DNN_BATCH_NORMALIZATION = True              # Batch normalization for the neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bacf28f2",
   "metadata": {},
   "source": [
    "#### 5.3.2 Trainieren 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural Network\n",
    "import json\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "onehotencoder = OneHotEncoder()\n",
    "onehotencoder = onehotencoder.fit(trainY)\n",
    "onehotEncodedTrainY = onehotencoder.transform(trainY).toarray()\n",
    "onehotEncodedTestY = onehotencoder.transform(testY).toarray()\n",
    "\n",
    "explorationResults = []\n",
    "\n",
    "\n",
    "numberOfFeatures = trainX.shape[1]\n",
    "categorieCount = len(onehotEncodedTrainY[0])\n",
    "\n",
    "testAccuracy = 0.0\n",
    "trainAccuracy = 0.0\n",
    "model = Sequential()\n",
    "interation = 0\n",
    "while (\n",
    "    testAccuracy < DNN_EXPLORATION_TARGET_VAL_ACCURACY\n",
    "    or trainAccuracy < DNN_EXPLORATION_TARGET_VAL_ACCURACY\n",
    ") and interation < DNN_EXPLORATION_MAX_ITER:\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(numberOfFeatures,)))\n",
    "    if DNN_BATCH_NORMALIZATION:\n",
    "        model.add(BatchNormalization())\n",
    "    denseCount = random.randint(\n",
    "        DNN_EXPLORATION_HIDDEN_LAYERS_MIN, DNN_EXPLORATION_HIDDEN_LAYERS_MAX\n",
    "    )\n",
    "    denseNeurons = []\n",
    "    for i in range(0, denseCount):\n",
    "        neuronCount = random.randint(\n",
    "            DNN_EXPLORATION_NEURONS_MIN, DNN_EXPLORATION_NEURONS_MAX\n",
    "        )\n",
    "        denseNeurons.append(neuronCount)\n",
    "        model.add(Dense(neuronCount, activation=\"tanh\"))\n",
    "        if DNN_BATCH_NORMALIZATION:\n",
    "            model.add(BatchNormalization())\n",
    "    model.add(Dense(categorieCount, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.fit(\n",
    "        trainX,\n",
    "        onehotEncodedTrainY,\n",
    "        epochs=DNN_EPOCHS,\n",
    "        batch_size=DNN_BATCH_SIZE,\n",
    "        verbose=DNN_VERBOSE,\n",
    "        validation_data=(testX, onehotEncodedTestY),\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=DNN_EARLY_STOPPING_PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainLoss, trainAccuracy = model.evaluate(trainX, onehotEncodedTrainY, verbose=DNN_VERBOSE)\n",
    "    testLoss, testAccuracy = model.evaluate(testX, onehotEncodedTestY, verbose=DNN_VERBOSE)\n",
    "\n",
    "    explorationResults.append(\n",
    "        {\n",
    "            \"dense_count\": denseCount,\n",
    "            \"dense_neurons\": denseNeurons,\n",
    "            \"train_loss\": trainLoss,\n",
    "            \"train_acc\": trainAccuracy,\n",
    "            \"test_loss\": testLoss,\n",
    "            \"test_acc\": testAccuracy,\n",
    "        }\n",
    "    )\n",
    "    interation += 1\n",
    "\n",
    "modelName = \"dnn.\" + str(round(time.time()))\n",
    "\n",
    "model.save(PATH_MODEL + modelName)\n",
    "print(\"Iteration: %d\" % interation)\n",
    "print(\"Train Accuracy: %.2f%%\" % (trainAccuracy * 100.0))\n",
    "print(\"Test Accuracy: %.2f%%\" % (testAccuracy * 100.0))\n",
    "\n",
    "with open(PATH_MODEL + modelName + \"/encoder.pickle\", \"wb\") as f:\n",
    "    joblib.dump(onehotencoder, f)\n",
    "\n",
    "with open(PATH_EXPLORATION_DATA + modelName + \".exploration_results.json\", \"w\") as f:\n",
    "    json.dump(explorationResults, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccb7f3ca",
   "metadata": {},
   "source": [
    "### 5.4 Extremes Gradienten-Boosting - XGBoost\n",
    "\n",
    "In diesem Abschnitt wird mit der xgboost-Bibliothek ein Modell mithilfe des \"Extreme Gradient Boosting\" trainiert. Dabei können verschiedene Parameter frei gewählt werden, wie die maximale Anzahl der Bäume, die gewünschte Mindestgenauigkeit und die Anzahl der Suchiterationen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b71bb915",
   "metadata": {},
   "source": [
    "#### 5.4.1 Konfiguration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7faada",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_MAX_DEPTH = 16                           # Maximum depth of the tree\n",
    "DT_EXPLORATION_TARGET_VAL_ACCURACY = 0.9    # Target accuracy for the decision tree\n",
    "DT_EXPLORATION_MAX_ITER = 100               # Maximum number of iterations for the random search\n",
    "DT_VERBOSE = False                          # Verbose logging for the decision tree\n",
    "DT_NUM_OF_ESTIMATORS = 10                 # number of estimators (default = None -> number of estimators = number of classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ae31a94",
   "metadata": {},
   "source": [
    "#### 5.4.2 Trainieren 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder = labelEncoder.fit(np.ravel(trainY))\n",
    "labelEncodedTrainY = labelEncoder.transform(np.ravel(trainY))\n",
    "labelEncodedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "testAccuracy = 0.0\n",
    "trainAccuracy = 0.0\n",
    "iterations = 0\n",
    "while (\n",
    "    testAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    "    or trainAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    ") and iterations < DT_EXPLORATION_MAX_ITER:\n",
    "    xgb = XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        enable_categorical=True,\n",
    "        max_depth=DT_MAX_DEPTH,\n",
    "        n_estimators=labelEncoder.classes_.size if DT_NUM_OF_ESTIMATORS == None else DT_NUM_OF_ESTIMATORS,\n",
    "        random_state=np.random.randint(100000)\n",
    "    )\n",
    "    # fit model\n",
    "    labeledTrainX = pd.DataFrame(trainX, columns=featureNames)\n",
    "    labeledTestX = pd.DataFrame(testX, columns=featureNames)\n",
    "\n",
    "    xgb.fit(\n",
    "        labeledTrainX, labelEncodedTrainY, eval_set=[(labeledTestX, labelEncodedTestY)], verbose=1 if DT_VERBOSE else 0\n",
    "    )\n",
    "\n",
    "    testPredictions = xgb.predict(testX)\n",
    "    testAccuracy = accuracy_score(labelEncodedTestY, testPredictions)\n",
    "\n",
    "    trainPredictions = xgb.predict(trainX)\n",
    "    trainAccuracy = accuracy_score(labelEncodedTrainY, trainPredictions)\n",
    "    iterations += 1\n",
    "    if DT_VERBOSE:\n",
    "        print(\"Train Accuracy: %.2f%%\" % (trainAccuracy * 100.0))\n",
    "        print(\"Test Accuracy: %.2f%%\" % (testAccuracy * 100.0))\n",
    "\n",
    "modelName = \"xgb.\" + str(round(time.time()))\n",
    "os.makedirs(PATH_MODEL + modelName)\n",
    "\n",
    "with open(PATH_MODEL + modelName + \"/encoder.pickle\", \"wb\") as f:\n",
    "    joblib.dump(labelEncoder, f)\n",
    "\n",
    "\n",
    "XGBClassifier.save_model(xgb, PATH_MODEL + modelName + \"/xgb.model\")\n",
    "\n",
    "featureMap = xgb.get_booster().get_score(importance_type=\"gain\")\n",
    "\n",
    "with open(PATH_MODEL + modelName + \"/feature_map.txt\", \"w\") as file:\n",
    "    for index, feature in enumerate(xgb.get_booster().feature_names):\n",
    "        file.write(f\"{index}\\t{feature}\\tq\\n\")\n",
    "\n",
    "\n",
    "print(\"Iteration: %d\" % iterations)\n",
    "print(\"Train Accuracy: %.2f%%\" % (trainAccuracy * 100.0))\n",
    "print(\"Test Accuracy: %.2f%%\" % (testAccuracy * 100.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "615dfc90-2798-42fe-8001-7be9ea78aa35",
   "metadata": {},
   "source": [
    "## 6. Modelanalyse des Learners\n",
    "\n",
    "**In diesem Abschnitt können in Abhängigkeit von der Auswahl des zu analysierenden Modells verschiedene Analysen durchgeführt werden.**\n",
    "\n",
    "- *Visuelle Darstellung (nur Entscheidungsbaum/Extremes Gradienten-Boosting)*: Eine visuelle Darstellung des Modells kann erstellt werden, um die Entscheidungslogik und Struktur intuitiv zu erfassen.\n",
    "\n",
    "- *Konfusionsmatrix*: Eine Konfusionsmatrix wird erstellt, um die Leistung des Modells bei der Klassifikation zu bewerten. Sie zeigt, wie gut das Modell verschiedene Klassen korrekt vorhersagt und welche Fehler gemacht werden.\n",
    "\n",
    "- *Test-Genauigkeit*: Die Test-Genauigkeit wird berechnet, um zu bewerten, wie gut das Modell auf unbekannten Daten abschneidet. Dies gibt einen Indikator dafür, wie zuverlässig die Vorhersagen des Modells sind.\n",
    "\n",
    "- *Feature Importances*: Es wird eine Analyse der Feature Importances durchgeführt, um die relative Bedeutung der verschiedenen Merkmale bei der Vorhersage zu bestimmen. Dies ermöglicht Einblicke in die relevanten Merkmale und kann bei der Feature-Auswahl oder -Gewichtung helfen.\n",
    "\n",
    "Diese Analysen bieten einen umfassenden Einblick in die Leistung und Funktionsweise des ausgewählten Modells und unterstützen bei der Interpretation der Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT LEARNER AND TEST DATA\n",
    "clear_output(wait=True)\n",
    "featureDataDir = list(filter(lambda x: os.path.isfile(os.path.join(PATH_FEATURE_DATA, x)), os.listdir(PATH_FEATURE_DATA)))\n",
    "modelDir = os.listdir(PATH_MODEL)\n",
    "\n",
    "modelDropdown = widgets.Dropdown(description=\"model\")\n",
    "modelDropdown.options = modelDir\n",
    "display(modelDropdown)\n",
    "\n",
    "testFileDropdown = widgets.Dropdown(description=\"test data\")\n",
    "testFileDropdown.options = featureDataDir\n",
    "display(testFileDropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3540ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYZE MODEL\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfmath\n",
    "import tensorflow_probability as tfp\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import joblib\n",
    "from matplotlib import pyplot\n",
    "from xgboost import XGBClassifier, plot_tree\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "selectedTestFile = testFileDropdown.value\n",
    "selectedModelFile = modelDropdown.value\n",
    "\n",
    "print(\"Selected model: \" + selectedModelFile)\n",
    "\n",
    "csvTest = read_csv(PATH_FEATURE_DATA + selectedTestFile, delimiter=\";\")\n",
    "testData = csvTest.values\n",
    "\n",
    "testX, testY = testData[:,2:].astype(np.float32), testData[:, DATA_INDEX_CLASS:DATA_INDEX_CLASS+1]\n",
    "featureNames = csvTest.columns.values[2:].tolist()\n",
    "\n",
    "model = None\n",
    "predictions = None\n",
    "transformedTestY = None\n",
    "confusionMatrix = None\n",
    "classes = None\n",
    "if (selectedModelFile.startswith('dnn')):\n",
    "    onehotencoder = None\n",
    "    with open(PATH_MODEL + selectedModelFile + '/encoder.pickle', 'rb') as f:\n",
    "        onehotencoder = joblib.load(f)\n",
    "    classes = onehotencoder.categories_[0]\n",
    "    transformedTestY = onehotencoder.transform(testY).toarray()    \n",
    "\n",
    "    model = tf.keras.models.load_model(PATH_MODEL + selectedModelFile)\n",
    "    predictions = model.predict(testX)\n",
    "\n",
    "    confusionMatrix = tf.math.confusion_matrix(np.argmax(transformedTestY, axis=1), np.argmax(predictions, axis=1))\n",
    "    equality = tf.math.equal(np.argmax(predictions, axis=1), np.argmax(transformedTestY, axis=1))\n",
    "    accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "elif (selectedModelFile.startswith('xgb')):\n",
    "    labelEncoder = None\n",
    "    with open(PATH_MODEL + selectedModelFile + '/encoder.pickle', 'rb') as f:\n",
    "        labelEncoder = joblib.load(f)\n",
    "    classes = labelEncoder.classes_\n",
    "    transformedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "    model = XGBClassifier()\n",
    "    model.load_model(PATH_MODEL + selectedModelFile + '/xgb.model')\n",
    "    predictions = model.predict(testX)\n",
    "    confusionMatrix = tfmath.confusion_matrix(transformedTestY, predictions)\n",
    "\n",
    "    for i in range(model.n_estimators):\n",
    "        plot_tree(model, num_trees=i, fmap=PATH_MODEL + selectedModelFile + '/feature_map.txt')\n",
    "        pyplot.gcf().set_dpi(1200)\n",
    "        pyplot.show()\n",
    "\n",
    "\n",
    "    #pyplot.show()\n",
    "    equality = tf.math.equal(predictions, transformedTestY)\n",
    "    accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "elif (selectedModelFile.startswith('dtc')):\n",
    "    labelEncoder = None\n",
    "    with open(PATH_MODEL + selectedModelFile + '/encoder.pickle', 'rb') as f:\n",
    "        labelEncoder = joblib.load(f)\n",
    "    classes = labelEncoder.classes_\n",
    "    transformedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "    model = joblib.load(PATH_MODEL + selectedModelFile + '/dtc.model')\n",
    "\n",
    "    predictions = model.predict(testX)\n",
    "    accuracy = accuracy_score(transformedTestY, predictions)\n",
    "    confusionMatrix = tfmath.confusion_matrix(transformedTestY, predictions)\n",
    "    plt.figure(figsize=(120, 40))       \n",
    "    tree.plot_tree(model, feature_names=featureNames, class_names=labelEncoder.classes_, filled=True)\n",
    "    plt.show()\n",
    "\n",
    "mat = pyplot.matshow(confusionMatrix, 1)\n",
    "mat.axes.set_xticks(np.arange(0, len(classes), 1))\n",
    "mat.axes.set_yticks(np.arange(0, len(classes), 1))\n",
    "mat.axes.set_xticklabels(classes, rotation=90)\n",
    "mat.axes.set_yticklabels(classes)\n",
    "for (x, y), value in np.ndenumerate(confusionMatrix):\n",
    "    pyplot.text(y, x, f\"{value:.2f}\", va=\"center\", ha=\"center\")\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "print('Test Accuracy: %.3f' % accuracy)\n",
    "\n",
    "\n",
    "#correlationMatrix = tfp.stats.correlation(testX)\n",
    "#pyplot.matshow(correlationMatrix)\n",
    "#pyplot.show()\n",
    "\n",
    "perm = PermutationImportance(model, scoring=\"neg_mean_squared_error\", random_state=1).fit(testX, transformedTestY)\n",
    "print(eli5.format_as_text(eli5.explain_weights(perm, feature_names=featureNames)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "390a4a00-cdc0-4fae-bd1f-e92cd875dec1",
   "metadata": {},
   "source": [
    "## 7. Statische Interpretation des Resultats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9487c908",
   "metadata": {},
   "source": [
    "### 7.1 Deskriptive und Explorative Datenanalyse\n",
    "#### 7.1.1 Aggregation\n",
    "Die deskriptive Datenanalyse der Gutdaten ergab interessante Erkenntnisse. Eine Fehlausrichtung zeigt sich durch die Umkehrung von p1 und p2. Dieser Effekt wird besonders deutlich, wenn der maximale Förderstrom (100%) betrachtet wird. In diesem Fall wird die Fehlausrichtung sehr gut sichtbar.\n",
    "\n",
    "<img src=\"img/kidaq_visualisierung.png\" alt=\"KiDAQ Visualisierung\" width=\"40%\" height=\"20%\" title=\"KiDAQ Visualisierung\"><br>\n",
    "\n",
    "Ein Laufradschaden hingegen äußert sich durch eine vergrößerte Entfernung zwischen p1 und p2. Besonders auffällig ist, dass sich dieser Abstand auch beim maximalen Förderstrom (100%) weiter vergrößert. Es zeigt sich also ein Trend, dass sowohl p1 als auch p2 sich voneinander entfernen, welches auf einen möglichen Laufradschaden hinweisen könnte.\n",
    "\n",
    "Das Resultat der Untersuchung ist, dass Veränderungen der Daten bereits anhand der unterschiedlichen Messreihen erkannt werden können. Sowohl die Fehlausrichtung, als auch der Laufradschaden lassen sich durch die Analyse der p1- und p2-Werte identifizieren. Dies ermöglicht eine frühzeitige Erkennung von potenziellen Problemen und gibt die Möglichkeit, geeignete Maßnahmen zur Behebung einzuleiten, bevor schwerwiegendere Schäden auftreten.\n",
    "\n",
    "Die deskriptive Datenanalyse liefert somit wertvolle Informationen, um die Qualität der Daten zu beurteilen und potenzielle Abweichungen oder Schäden zu erkennen. Durch regelmäßige Überprüfung der Messreihen können frühzeitig Anomalien festgestellt und entsprechende Maßnahmen ergriffen werden, um die Funktionalität und Zuverlässigkeit der untersuchten Systeme aufrechtzuerhalten."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e50915f",
   "metadata": {},
   "source": [
    "#### 7.1.2 Frequenztransformation\n",
    "Die Auswertung der Vibrationssignale mithilfe der Fast-Fourier-Transformation (FFT) ergab interessante Unterschiede zwischen dem Kidaq-System und dem Vib-System bezüglich des Frequenzverhaltens.\n",
    "\n",
    "Im KiDAQ-System liegen die meisten Werte im niedrigen Frequenzbereich. Dies deutet darauf hin, dass die dominanten Frequenzen oder Frequenzbereiche der Vibrationssignale im Kidaq-System hauptsächlich im niedrigen Bereich zu finden sind. Dies kann auf eine bestimmte Art von Vibrationen oder Schwingungen hinweisen, die in diesem System häufig auftreten.\n",
    "\n",
    "<img src=\"img/kidaq_fft_1.png\" alt=\"KiDAQ FFT\" width=\"25%\" height=\"20%\" title=\"KiDAQ FFT\">\n",
    "<img src=\"img/kidaq_fft_2.png\" alt=\"KiDAQ FFT\" width=\"25%\" height=\"20%\" title=\"KiDAQ FFT\">\n",
    "<br>\n",
    "\n",
    "Im VIB-System hingegen sind die meisten Werte gleichmäßig im Frequenzbereich zwischen 0 und 1000 verteilt. Dies deutet darauf hin, dass im VIB-System eine breitere Palette von Frequenzen in den Vibrationssignalen vorhanden ist. Es gibt jedoch immer noch einen signifikaten Anstieg im Frequenzbereich von 800 bis 1000, was darauf hindeuten kann, dass in diesem Frequenzbereich eine erhöhte Aktivität oder eine spezifische physikalische Eigenschaft vorliegt.\n",
    "\n",
    "<img src=\"img/vib_fft_1.png\" alt=\"VIB FFT\" width=\"25%\" height=\"20%\" title=\"VIB FFT\">\n",
    "<br>\n",
    "\n",
    "Eine grobe Interpretation dieser Ergebnisse könnte darauf hinweisen, dass das KiDAQ-System möglicherweise empfindlicher oder anfälliger für niedrigfrequente Vibrationen ist, während das VIB-System eine breitere Bandbreite von Frequenzen erfasst, einschließlich höherfrequenter Vibrationen. Der Anstieg im Bereich von 800 bis 1000 im Vib-System könnte auf eine spezifische resonante Eigenschaft oder ein bestimmtes Verhalten bestimmter Komponenten hinweisen.\n",
    "\n",
    "Die FFT-Analyse liefert somit wertvolle Informationen über das Frequenzverhalten der Vibrationssignale und kann bei der Identifizierung von Mustern oder Anomalien helfen. Durch die Analyse der Frequenzspektren können bestimmte Frequenzen oder Frequenzbereiche identifiziert werden, die auf bestimmte Eigenschaften oder Verhaltensweisen hinweisen können. Dies ermöglicht es, die Vibrationssignale besser zu verstehen und die Leistung der Systeme zu verbessern. Diese Eigenschaften wurde ebenfalls für das Training des künstlich neuronalen Netzes verwendet, welche im Abschnitt 5. Maschinelles Learning erläutert wurde."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75cb8641",
   "metadata": {},
   "source": [
    "## 7.2 Interpretation des maschinellen Lernens\n",
    "\n",
    "### 7.2.1 VIB\n",
    "\n",
    "Durch die Anwendung einfacher aggregativer Mittel kann bereits eine annähernd perfekte Klassifikation der Betriebsarten anhand der VIB-Daten erreicht werden. Dabei ist sowohl die Genauigkeit hoch als auch die Modellkomplexität gering. Ein Entscheidungsbaum erzielt bereits mit einer Tiefe von 6 eine Genauigkeit von über 95%. Diese Kennzahlen sind sowohl innerhalb eines Setups reproduzierbar als auch bei der Anwendung eines Modells, das mit einem anderen Setup trainiert wurde. Die Anwendung einer Frequenzanalyse und die Übermittlung entsprechender Frequenzmerkmale an den Lernalgorithmus führen zu keiner Verbesserung.\n",
    "\n",
    "<img src=\"img/vib-dtree.png\" alt=\"VIB DTREE\" title=\"VIB DTREE\">\n",
    "\n",
    "#### Setup 1\n",
    "<div style=\"display:flex;\">\n",
    "<img src=\"img/vib-confmat-s1.png\" alt=\"VIB ConfMat S1\" title=\"VIB ConfMat S1\" />\n",
    "\n",
    "<div style=\"padding-left: 20px;\">\n",
    "Permutation Importance:\n",
    "<br><br>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Bezeichnung</th>\n",
    "        <th>Wert</th>\n",
    "        <th>Unsicherheit</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s1_std</td>\n",
    "        <td>1.0526</td>\n",
    "        <td>0.0191</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s5_range</td>\n",
    "        <td>0.7759</td>\n",
    "        <td>0.0201</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s3_mean_median</td>\n",
    "        <td>0.6220</td>\n",
    "        <td>0.0309</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s3_iqr</td>\n",
    "        <td>0.6117</td>\n",
    "        <td>0.0488</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>rpm</td>\n",
    "        <td>0.2643</td>\n",
    "        <td>0.0164</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s2_std</td>\n",
    "        <td>0.2600</td>\n",
    "        <td>0.0327</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s8_mean_median</td>\n",
    "        <td>0.2339</td>\n",
    "        <td>0.0205</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s7_iqr</td>\n",
    "        <td>0.1605</td>\n",
    "        <td>0.0122</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s4_iqr</td>\n",
    "        <td>0.1429</td>\n",
    "        <td>0.0140</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s3_std</td>\n",
    "        <td>0.1195</td>\n",
    "        <td>0.0112</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s5_iqr</td>\n",
    "        <td>0.1185</td>\n",
    "        <td>0.0015</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s2_iqr</td>\n",
    "        <td>0.0968</td>\n",
    "        <td>0.0070</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s7_std</td>\n",
    "        <td>0.0782</td>\n",
    "        <td>0.0046</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s5_mean_median</td>\n",
    "        <td>0.0726</td>\n",
    "        <td>0.0314</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "#### Setup 2\n",
    "<div style=\"display:flex;\">\n",
    "<img src=\"img/vib-confmat-s2.png\" alt=\"VIB ConfMat S2\" title=\"VIB ConfMat S2\" />\n",
    "\n",
    "<div style=\"padding-left: 20px;\">\n",
    "Permutation Importance:\n",
    "<br><br>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Bezeichnung</th>\n",
    "        <th>Wert</th>\n",
    "        <th>Unsicherheit</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s5_iqr</td>\n",
    "        <td>0.4961</td>\n",
    "        <td>0.0073</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s5_range</td>\n",
    "        <td>0.0030</td>\n",
    "        <td>0.0005</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s1_std</td>\n",
    "        <td>0.0018</td>\n",
    "        <td>0.0020</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s8_std</td>\n",
    "        <td>0.0012</td>\n",
    "        <td>0.0000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s3_iqr</td>\n",
    "        <td>0.0010</td>\n",
    "        <td>0.0002</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s7_iqr</td>\n",
    "        <td>0.0010</td>\n",
    "        <td>0.0014</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s2_range</td>\n",
    "        <td>0.0005</td>\n",
    "        <td>0.0000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s7_range</td>\n",
    "        <td>0.0000</td>\n",
    "        <td>0.0000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s3_std</td>\n",
    "        <td>0.0000</td>\n",
    "        <td>0.0000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s4_std</td>\n",
    "        <td>0.0000</td>\n",
    "        <td>0.0000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s5_std</td>\n",
    "        <td>0.0000</td>\n",
    "        <td>0.0000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s6_std</td>\n",
    "        <td>0.0000</td>\n",
    "        <td>0.0000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s7_std</td>\n",
    "        <td>0.0000</td>\n",
    "        <td>0.0000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>s3_range</td>\n",
    "        <td>0.0000</td>\n",
    "        <td>0.0000</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "637344f1",
   "metadata": {},
   "source": [
    "### 7.2.2 KiDAQ\n",
    "\n",
    "Der Versuch, ein betriebspunktunabhängiges Modell mithilfe der KiDAQ-Daten zu trainieren, gestaltet sich deutlich schwieriger. Während im ersten Setup noch eine Genauigkeit von über 70% erreicht werden kann, sieht es bei der Validierung mit Hilfe von Setup 2 leider deutlich schlechter aus. Dort scheint eine Konfusion zwischen Laufradschaden, Fehlausrichtung und Gutfällen zu bestehen. Entweder liegt hier ein Fehler in den Daten vor oder die Fälle lassen sich nicht unabhängig vom Setup differenzieren.\n",
    "\n",
    "Im Allgemeinen ist der Datenbestand quantitativ zu gering, um eine endgültige Aussage über die Klassifizierbarkeit der Fehler zu treffen.\n",
    "\n",
    "<img src=\"img/kidaq-dtree.png\" alt=\"VIB DTREE\" title=\"VIB DTREE\">\n",
    "\n",
    "#### Setup 1\n",
    "<div style=\"display:flex;\">\n",
    "<img src=\"img/kidaq-confmat-s1.png\" alt=\"KiDAQ ConfMat S1\" title=\"KiDAQ ConfMat S1\" />\n",
    "\n",
    "<div style=\"padding-left: 20px;\">\n",
    "Permutation Importance:\n",
    "<br><br>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Bezeichnung</th>\n",
    "        <th>Wert</th>\n",
    "        <th>Unsicherheit</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_band_325.0_349.0_energy_mean</td>\n",
    "        <td>0.3924</td>\n",
    "        <td>0.0029</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_275_300_std</td>\n",
    "        <td>0.3769</td>\n",
    "        <td>0.0087</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_band_400.0_424.0_energy</td>\n",
    "        <td>0.3656</td>\n",
    "        <td>0.0085</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_band_450.0_474.0_energy_mean</td>\n",
    "        <td>0.3247</td>\n",
    "        <td>0.0015</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_band_250.0_274.0_energy</td>\n",
    "        <td>0.2490</td>\n",
    "        <td>0.0063</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_75_100_range</td>\n",
    "        <td>0.2276</td>\n",
    "        <td>0.0032</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_band_275.0_299.0_energy</td>\n",
    "        <td>0.2098</td>\n",
    "        <td>0.0073</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_std</td>\n",
    "        <td>0.1274</td>\n",
    "        <td>0.0025</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_150_175_std</td>\n",
    "        <td>0.1150</td>\n",
    "        <td>0.0023</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_band_300.0_324.0_energy_mean</td>\n",
    "        <td>0.1093</td>\n",
    "        <td>0.0050</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_amplitude_19_highest_energy</td>\n",
    "        <td>0.1080</td>\n",
    "        <td>0.0046</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_band_300.0_324.0_energy</td>\n",
    "        <td>0.0933</td>\n",
    "        <td>0.0022</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>rpm</td>\n",
    "        <td>0.0778</td>\n",
    "        <td>0.0030</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_100_125_std</td>\n",
    "        <td>0.0770</td>\n",
    "        <td>0.0051</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "#### Setup 2\n",
    "\n",
    "<div style=\"display:flex;\">\n",
    "<img src=\"img/kidaq-confmat-s2.png\" alt=\"KiDAQ ConfMat S2\" title=\"KiDAQ ConfMat S2\" />\n",
    "\n",
    "<div style=\"padding-left: 20px;\">\n",
    "Permutation Importance:\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Bezeichnung</th>\n",
    "        <th>Wert</th>\n",
    "        <th>Unsicherheit</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_std</td>\n",
    "        <td>0.8039</td>\n",
    "        <td>0.0422</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_band_225.0_249.0_energy</td>\n",
    "        <td>0.3475</td>\n",
    "        <td>0.0217</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_band_275.0_299.0_energy</td>\n",
    "        <td>0.0862</td>\n",
    "        <td>0.0075</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_band_400.0_424.0_energy</td>\n",
    "        <td>0.0841</td>\n",
    "        <td>0.0171</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p2_band_325.0_349.0_energy_mean</td>\n",
    "        <td>0.0101</td>\n",
    "        <td>0.0021</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_amplitude_13_highest_energy</td>\n",
    "        <td>0.0006</td>\n",
    "        <td>0.0010</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_amplitude_14_highest_energy</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_amplitude_15_highest_energy</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_amplitude_16_highest_energy</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_amplitude_17_highest_energy</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_amplitude_1_highest_freq</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_amplitude_20_highest_energy</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_amplitude_12_highest_energy</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>p1_amplitude_2_highest_freq</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
