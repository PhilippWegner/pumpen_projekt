{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5a94bd2-a922-4b01-a9b9-7658e41f6036",
   "metadata": {},
   "source": [
    "\n",
    "<img style=\"float: right;\" src=\"img/bo_logo.png\" alt=\"Hochschule Bochum\" width=\"30%\" height=\"30%\" title=\"Hochschule Bochum\">\n",
    "<p style=\"text-align: center;\">\n",
    "<b>Hochschule Bochum<br>\n",
    "Fachbereich Informatik und Ingenieurwissenschaften<br>\n",
    "Wahlmodul: Projektbasierte Vertiefung aktueller Themen der Informatik<br>\n",
    "Betreuer: Prof. Dr. rer. nat. Henrik Blunck, Prof. Dr.-Ing. Ralph Lindken, Marc Ladwig (M.Sc.)</b><br>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea84f549",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7591dfde",
   "metadata": {},
   "source": [
    "## Aufbau des Notebooks\n",
    "- Python Import\n",
    "- Installationsanleitung\n",
    "    - Paket- und Dependency-Installation über Anaconda in der Konsole\n",
    "    - Verwendung von JupyterLab\n",
    "- 1. Aufgabe und Daten erklären/beschreiben\n",
    "    - Zusammenfassung\n",
    "    - Aufgabe\n",
    "    - Daten\n",
    "    - KiDAQ-System (Drucksignale)\n",
    "    - VIB-System (Schwingungssignale)\n",
    "    - Beschreibung der Rohdatenstruktur\n",
    "- 2. Deskriptive und Explorative Datenanalyse der KiDAQ-Daten\n",
    "- 3. Preprocessing\n",
    "    - Vorbereitung der Tainings- und Testdaten\n",
    "        - Vorauswahl der Feature Einstellungen und Auswahl der Daten\n",
    "        - Definition der Funktionen zum Preprocessing der verschiedenen Datentypen\n",
    "        - Funktionem zum Preprocessing der Daten in den Frequenbereich und Generierung der Features\n",
    "- 4. Maschinelles Lernen\n",
    "    - Trainings- und Testdaten wählen und laden\n",
    "        - Entscheidungsbaum - sklearn (empfohlen)\n",
    "        - Konfiguration\n",
    "        - Trainieren\n",
    "    - Tiefes neuronales Netz - tensorflow\n",
    "        - Konfiguration\n",
    "        - Trainieren\n",
    "    - Extremes Gradienten-Boosting - XGBoost\n",
    "        - Konfiguration\n",
    "        - Trainieren\n",
    "- 5. Modelanalyse des Learners\n",
    "- 6. Statische Interpretation des Resultats\n",
    "    - Deskriptive und Explorative Datenanalyse\n",
    "    - VIB-Fourier-Transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19235131",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "258052c1",
   "metadata": {},
   "source": [
    "### Python Imports\n",
    "Für die ordnungsgemäße Nutzung müssen einige Bibliotheken (Libraries) vorab importiert werden. Ein Import wird über das Schlüsselwort <b>import</b> durchgeführt, zusätzlich kann noch über das <b>as</b> der Name des importieren Pakets geändert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3394772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import ipywidgets as widgets\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import asammdf\n",
    "from IPython.display import display\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pathlib as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ea6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the global variables\n",
    "\n",
    "PATH_RAW_DATA = \"./data/raw/\"\n",
    "PATH_FEATURE_DATA = \"./data/\"\n",
    "PATH_EXPLORATION_DATA = \"./exploration/\"\n",
    "PATH_MODEL = \"./models/\"\n",
    "DATA_SOURCE_KIDAQ = [\"TEST_NAME\", \"TEST_TYPE\", \"RPM\", \"FLOW_RATE\", \"P1\", \"P2\"]\n",
    "RAW_DATA_TYPE = [\"KIDAQ\", \"VIB\"]\n",
    "\n",
    "DATA_SOURCE_VID = [\n",
    "    \"TEST_NAME\",\n",
    "    \"TEST_TYPE\",\n",
    "    \"RPM\",\n",
    "    \"FLOW_RATE\",\n",
    "    \"S1\",\n",
    "    \"S2\",\n",
    "    \"S3\",\n",
    "    \"S4\",\n",
    "    \"S5\",\n",
    "    \"S6\",\n",
    "    \"S7\",\n",
    "    \"S8\",\n",
    "]\n",
    "FEATURE = [\n",
    "    \"STD\",\n",
    "    \"RANGE\",\n",
    "    \"IQR\",\n",
    "    \"MEAN_MEDIAN\",\n",
    "    \"FFT\",\n",
    "]\n",
    "OPERATING_POINT_FREQ = [725, 1450, 2175, 2900]\n",
    "OPERATING_POINT_FLOW_RATE = [0, 25, 50, 75, 100]\n",
    "\n",
    "DEFAULT_RAW_DATA_TYPE = RAW_DATA_TYPE[1]\n",
    "DEFAULT_RAW_DATA = DATA_SOURCE_KIDAQ\n",
    "\n",
    "DEFAULT_CLASS_LABEL = \"TEST_TYPE\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f99d692a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad6c7352",
   "metadata": {},
   "source": [
    "## Installationsanleitung\n",
    "1. Anleitung zur Installation von Anaconda, Python und JupyterLab\n",
    "2. Paket- und Dependency-Installation über Anaconda in der Konsole\n",
    "3. Verwendung von JupyterLab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdc87225",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84e96f9e",
   "metadata": {},
   "source": [
    "## Anleitung zur Installation von Anaconda, Python und JupyterLab\n",
    "\n",
    "Anleitung zur Installation von Anaconda, Python und JupyterLab\n",
    "In dieser Anleitung werde ich Ihnen Schritt für Schritt erklären, wie Sie Anaconda, Python und JupyterLab richtig installieren können. Wir werden die Installation unter Windows durchführen.\n",
    "\n",
    "### Schritt 1: Herunterladen von Anaconda\n",
    "1. Öffnen Sie Ihren Webbrowser und gehen Sie zur offiziellen Anaconda-Website: https://www.anaconda.com/products/individual.\n",
    "2. Wählen Sie die Version von Anaconda aus, die Ihrem Betriebssystem entspricht (Windows).\n",
    "3. Klicken Sie auf den Download-Button und warten Sie, bis der Download abgeschlossen ist.\n",
    "\n",
    "### Schritt 2: Installation von Anaconda\n",
    "1. Navigieren Sie zum heruntergeladenen Anaconda-Installationsprogramm und starten Sie es.\n",
    "2. Befolgen Sie die Anweisungen des Installationsprogramms. Standardmäßig wird Anaconda in das Verzeichnis \"C:\\Anaconda\" installiert. Sie können jedoch einen anderen Installationsort auswählen, wenn Sie möchten.\n",
    "3. Stellen Sie sicher, dass die Option \"Add Anaconda to my PATH environment variable\" aktiviert ist. Dadurch wird Anaconda in Ihrem Systempfad hinzugefügt, sodass Sie es über die Befehlszeile aufrufen können.\n",
    "4. Klicken Sie auf \"Install\", um mit der Installation fortzufahren.\n",
    "5. Warten Sie, bis die Installation abgeschlossen ist. Dies kann einige Minuten dauern.\n",
    "\n",
    "### Schritt 3: Überprüfung der Installation von Anaconda\n",
    "1. Öffnen Sie das Startmenü und suchen Sie nach \"Anaconda Navigator\".\n",
    "2. Starten Sie den Anaconda Navigator aus den Suchergebnissen. Das Anaconda Navigator-Fenster wird geöffnet.\n",
    "3. Überprüfen Sie, ob der Anaconda Navigator erfolgreich gestartet wird. Wenn ja, ist die Installation von Anaconda abgeschlossen.\n",
    "\n",
    "### Schritt 4: Installation/Launch von JupyterLab\n",
    "1. Öffnen Sie den Anaconda Navigator.\n",
    "2. Im Anaconda Navigator-Fenster klicken Sie auf \"Home\" in der linken Seitenleiste.\n",
    "3. Klicken Sie auf den Button Install/Launch unter der Karte mit dem Namen JupyterLab.\n",
    "4. JupyterLab wird installiert/gestartet.\n",
    "5. JupyterLab sollte in Ihrem Standard-Webbrowser geöffnet werden.\n",
    "6. Überprüfen Sie, ob JupyterLab erfolgreich gestartet wird. Wenn ja, ist die Installation/Launch von JupyterLab abgeschlossen.\n",
    "\n",
    "### Schritt 5: Überprüfung der Python-Installation\n",
    "1. Öffnen Sie das Startmenü und suchen Sie nach \"Anaconda Prompt\".\n",
    "2. Starten Sie die Anaconda Prompt aus den Suchergebnissen. Das Anaconda Prompt-Fenster wird geöffnet.\n",
    "3. Geben Sie den folgenden Befehl ein und drücken Sie die Eingabetaste, um die Python-Version anzuzeigen: \n",
    "```bash\n",
    "python --version\n",
    "```\n",
    "4. Überprüfen Sie, ob die installierte Python-Version angezeigt wird. Wenn ja, ist die Installation von Python erfolgreich abgeschlossen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "509d2be8",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c50f72b",
   "metadata": {},
   "source": [
    "### Paket- und Dependency-Installation über Anaconda in der Konsole\n",
    "1. Öffnen Sie das Startmenü und suchen Sie nach \"Anaconda Prompt\".\n",
    "2. Starten Sie die Anaconda Prompt aus den Suchergebnissen. Das Anaconda Prompt-Fenster wird geöffnet.\n",
    "3. Installieren Sie ein Paket oder eine Dependency, indem Sie den folgenden Befehl eingeben:\n",
    "```bash\n",
    "conda install paketname\n",
    "```\n",
    "\n",
    "Ersetzen Sie `paketname` durch den Namen des Pakets oder der Dependency, die Sie installieren möchten. Sie können auch mehrere Paketnamen angeben, indem Sie sie durch Leerzeichen trennen.\n",
    "\n",
    "4. Warten Sie, bis die Installation abgeschlossen ist. Anaconda wird automatisch alle benötigten Abhängigkeiten herunterladen und installieren.\n",
    "5. Überprüfen Sie, ob die Installation erfolgreich war, indem Sie das Paket oder die Dependency verwenden oder den folgenden Befehl ausführen, um eine Liste der installierten Pakete anzuzeigen:\n",
    "```bash\n",
    "conda list\n",
    "```\n",
    "Dadurch wird eine Liste aller installierten Pakete in der aktuellen Umgebung angezeigt.\n",
    "\n",
    "8. Wiederholen Sie die Schritte 3-5 für alle weiteren Pakete oder Dependencies, die Sie installieren möchten.\n",
    "9. Wenn Sie die Installation beendet haben, können Sie die Anaconda Prompt schließen oder mit weiteren Befehlen und Aktionen fortfahren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e66a675",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "028393ae",
   "metadata": {},
   "source": [
    "### Verwendung von JupyterLab\n",
    "\n",
    "1. Öffnen Sie das Startmenü und suchen Sie nach \"Anaconda Navigator\".\n",
    "\n",
    "2. Starten Sie den Anaconda Navigator aus den Suchergebnissen. Das Anaconda Navigator-Fenster wird geöffnet.\n",
    "\n",
    "3. Klicken Sie auf das Symbol für JupyterLab. Dadurch wird ein neues Browserfenster geöffnet, das die JupyterLab-Benutzeroberfläche anzeigt.\n",
    "\n",
    "4. In der JupyterLab-Benutzeroberfläche können Sie entweder eine neue Datei erstellen oder vorhandene Dateien öffnen.\n",
    "\n",
    "   - Um eine neue Datei zu erstellen, klicken Sie auf das \"+\" -Symbol in der Symbolleiste und wählen Sie den gewünschten Dateityp, z. B. \"Python 3\". Dadurch wird eine neue Datei geöffnet.\n",
    "\n",
    "   - Um eine vorhandene Datei zu öffnen, klicken Sie auf \"File\" in der Menüleiste und wählen Sie \"Open\" aus. Navigieren Sie zum Speicherort der Datei auf Ihrem Computer und wählen Sie sie aus. Die Datei wird in einem neuen Tab geöffnet.\n",
    "\n",
    "5. Jeder geöffnete Tab enthält eine Arbeitsfläche, in der Sie Code schreiben, Dateien anzeigen und bearbeiten, sowie andere Aktionen durchführen können.\n",
    "\n",
    "6. JupyterLab bietet verschiedene Arten von Dokumenten, die Sie in der Arbeitsfläche verwenden können:\n",
    "\n",
    "   - Jupyter Notebook: Sie können Jupyter Notebook-Dateien (.ipynb) öffnen und bearbeiten. Diese Dateien bestehen aus Zellen, die Code, Text oder Markdown enthalten können.\n",
    "\n",
    "   - Texteditor: Sie können Textdateien (.txt), Python-Skripte (.py) und andere Textdateiformate anzeigen und bearbeiten.\n",
    "\n",
    "   - Terminal: Sie können eine Befehlszeilenschnittstelle öffnen und Befehle direkt in JupyterLab ausführen.\n",
    "\n",
    "7. Jeder Tab enthält eine Symbolleiste mit verschiedenen Optionen, um Aktionen wie das Ausführen von Code, das Hinzufügen neuer Zellen, das Speichern von Dateien und vieles mehr durchzuführen.\n",
    "\n",
    "8. JupyterLab bietet auch eine Vielzahl von Tastenkombinationen, die Ihnen helfen können, schneller zu arbeiten. Sie können auf \"Help\" in der Menüleiste klicken und dann \"Keyboard Shortcuts\" auswählen, um eine Liste der verfügbaren Tastenkombinationen anzuzeigen.\n",
    "\n",
    "9. Um JupyterLab zu beenden, können Sie das Browserfenster schließen oder zur JupyterLab-Benutzeroberfläche zurückkehren und auf \"File\" in der Menüleiste klicken, gefolgt von \"Quit\".\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9639b65d",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a59c8f79",
   "metadata": {},
   "source": [
    "## 1. Aufgabe und Daten erklären/beschreiben [Philipp]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d08d88a7-0fc1-4c7a-91f7-df5aa0a4f9a2",
   "metadata": {},
   "source": [
    "### Zusammenfassung\n",
    "Die Zustandsüberwachung hydraulischer Anlagen und insbesondere Pumpen spielt eine wichtige Rolle bei der Sicherung eines korrekten Betriebs. Durch die Integration von künstlicher Intelligenz in vorrausschauende Systeme gewinnt die Predictive Maintenance gestützt durch künstliche Intelligenz an Bedeutung. Kreiselpumpen sind aufgrund ihrer Verbreitung in Industrie, Gewerbe und Haushalten besonders relevant für eine effiziente Betriebsweise. Schwingungsbasierte Systeme sind derzeit der Stand der Technik zur Überwachung von Pumpen, können jedoch nicht immer eindeutige Schlüsse auf die Ursachen von Schwingungen ziehen. In diesem Beitrag erfolgt ein Vergleich von druck- und schwingungsbasierten Zustandsüberwachungssystemen an einer typischen Kreiselpumpe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9c57ea6",
   "metadata": {},
   "source": [
    "### Aufgabe\n",
    "Die Aufgabe besteht darin, die übergebenen Daten aus dem Pumpenprüfstand im Labor der Hochschule Bochum auszuwerten und in ein geeignetes KI-Modell zu überführen. Das KI-Modell soll in der Lage sein, den Zustand der Pumpe zu erkennen und zu klassifizieren. Diese Erkennung soll anhand der Prognosefähigkeit des Modells die Wartungsintervalle und die Nutzungsdauer der Pumpe optimieren.\n",
    "Die Daten enthalten dabei sowohl Druck- als auch Schwingungssignale, die an unterschiedlichen Positionen an der Pumpe angebracht sind.<br>\n",
    "\n",
    "<img src=\"img/positionierung_sensoren.png\" alt=\"Positionierung Sensoren\" width=\"40%\" height=\"40%\" title=\"Positionierung der Sensoren\"><br>\n",
    "\n",
    "Jede Messung wird dabei nach Erreichen eines stationären Förderstroms für 12 Minuten konstant gehalten. Während dieser Zeit werden die Messungen in hoher Frequenz aufgezeichnet und in den Formaten .mf4 und .csv abgespeichert. Beide Signalarten werden getrennt von einander durch zwei unterschiedliche System aufgezeichnet.\n",
    "Das KiDAQ-System ist für die Aufzeichnung der Drucksignale zuständig und das VIB-System ist für die Aufnahme der Schwingungssignale zuständig. Beide Signalarten sollen innerhalb des Projektes zur Erarbeitung eines KI-Modells beitragen. Ebenfalls soll bewertet werden, ob es vielleicht auch ausreichend ist nur eine Signalart für die Erkennung des Pumpenzustandes zu verwenden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36fc13a4",
   "metadata": {},
   "source": [
    "### Daten\n",
    "Wie bereits in der Vorstellung der Projektaufgabe kurz erläuert wurde, werden die Daten in zwei unterschiedlichen Formaten abgespeichert. Die Daten des KiDAQ-Systems werden im Format .mf4 abgespeichert und die Daten des VIB-Systems werden im Format .csv abgespeichert. Beide Datenarten werden im folgenden Abschnitt kurz erläutert.\n",
    "\n",
    "#### KiDAQ-System (Drucksignale):\n",
    "Das KiDAQ-System zeichnet die Messwerte der Drucksignale auf. Hierfür werden piezoelektrische Sensoren verwendet, die an den Positionen P1 und P2 an der Pumpe angebracht sind. Die Sensoren sind in der Lage Druckänderungen zu messen und in elektrische Signale umzuwandeln. Um die Messungen zu homogenisieren existiert für jede Messsequenz ein Formblatt, welches genau dokumentiert welcher Förderstrom (l/min) vorliegt, an welchem Datum und zu welcher Zeit die Messung durchgeführt wurde und mit welcher Drehzahl die Pumpe angetrieben wurde. Ebenfalls wurde festgehalten, ob es sich bei dem Versuch, um einen Gut-, Kavitations-, Fehlausrichtungs- oder Laufradschadenversuch gehandelt hat. Da während der Messung eine große Datenanzahl generiert wird, wird das mf4-Format zum Abspeichern verwendet. Das mf4-Format ist ein standardisiertes Format, das in der Messtechnik und in der Datenanalyse eingesetzt wird. \n",
    "\n",
    "#### VIB-System (Schwingungssignale):\n",
    "Das VIB-System ist die Aufzeichnung der Schwingungssignals zuständig. VIB steht hierbei für Vibration. Es werden auch hier spezielle Sensoren an der Pumpe angebracht. Insgesamt sind XX Sensoren verbaut, die alle ausgelesen werden. Die Schwingungssignale werden nach dem selben Prinzip wie die Drucksignale aufgezeichnet. Sie wandeln eltromagnetische Schwingungen in elektrische Signale um. Die Messungen werden ebenfalls in einem Formblatt dokumentiert. Die Formblätter sind identisch zu den Formblättern des KiDAQ-Systems. Die Messungen werden in einem .csv-Format abgespeichert. Das .csv-Format lässt sich hierbei ohne Umformungsschritt sofort auslesen und vereinfacht somit die Arbeit, allerdings zu den kosten der Dateigröße.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89062378",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf14d673",
   "metadata": {},
   "source": [
    "### Beschreibung der Rohdatenstruktur [Philipp]\n",
    "Die Rohdaten des Pumpen Versuchs wurden ins in folgender Form übergeben:\n",
    "* Pumpensetup\n",
    "  * Versuchsart\n",
    "    * Formblatt\n",
    "    * Aufzeichnungssystem\n",
    "      * VIB-System\n",
    "      * KiDAQ-System\n",
    "      * Umrichterdaten\n",
    "  \n",
    "\n",
    "Das Pumpensetup gibt hierbei an um welche Pumpe es sich handelt und an welcher Stelle sich welche Sensoren befinden. Für unsere Auswertung wurden uns zwei unterschiedliche Setups übergeben. Das erste Setup nennt sich innerhalb der Ordnerstruktur Setup-I und das zweite Setup nennt sich Setup-II.\n",
    "In jedem Setup-Ordner befinden sich die Versuchsarten.\n",
    "Das Versuchsart-Verzeichnis gibt an, um welche Art von Versuch es sich handelt. Hierbei gibt es vier unterschiedliche Versuchsarten, welche in unterschiedlicher Anzahl vorliegen:\n",
    "- Gutversuch \n",
    "- Kavitationsversuch\n",
    "- Fehlausrichtungsversuch\n",
    "- Laufradschadenversuch\n",
    "\n",
    "Alle Versuchsarten sind in den beiden Setups enthalten und werden im späteren Verlauf auch für die Erzeugung des KI-Modells und/oder für die Validation eingesetzt.\n",
    "\n",
    "Ebenfalls interessant ist das Formblatt. Es definiert in welcher Position sich die Pumpe zur Versuchsreihe befunden hat und zeigt ebenfalls die Ausrichtung der Messensensoren. Ebenfalls wird hier dokumentiert, welche Drehzahl die Pumpe angetrieben hat und welcher Förderstrom vorlag. Das Format der Formblätter ist für jede Versuchsart identisch.\n",
    "\n",
    "Die beiden Aufzeichnungssysteme, die bereits im vorherigen Abschnitt erläutert wurden, enthalten die Messdaten: VIB für Schwingungssignale und KiDAQ für Drucksignale. Beide System verwenden unterschiedliche Dateiformate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f764a965",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f428e0b",
   "metadata": {},
   "source": [
    "### 2. Deskriptive und Explorative Datenanalyse der KiDAQ-Daten\n",
    "Um die Daten deskriptiv und explorativ auszuwerten, müssen die Daten visualisert werden. Um diesen Prozess zu automatisieren und dadurch zu vereinfachen können innerhalb dieses interaktiven Abschnitts verschiedene Einstellungen vorgenommen werden, um sich Plots zu generieren.\n",
    "Diese Plots werden innerhalb des Notebooks angezeigt und können Einzeln oder als Serie abgespeichert werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7679728d",
   "metadata": {},
   "source": [
    "Um den kidaq-folder Pfad ordnungsgemäß zu setzen kann dieser einfach aus dem Explorer kopiert und wie folgt eingefügt werden: r'<FOLDER_PATH>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4a58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E:\\Messdaten\\Setup-I\\2021-04-28 - Gut 1 as pathlib path\n",
    "kidaq_folder = r'E:\\Messdaten\\Setup-I\\2021-04-28 - Gut 1\\KiDAQ'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ac8d82a",
   "metadata": {},
   "source": [
    "Nun kann der Ordner ausgewählt werden aus dessen Dateien später die Plots generiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a path object\n",
    "path = pl.Path(kidaq_folder)\n",
    "# list all folders in the path\n",
    "folders = [str(x) for x in path.iterdir() if x.is_dir()]\n",
    "# create a widget to select the folders\n",
    "select = widgets.Select(\n",
    "    options=folders,\n",
    "    value=folders[0],\n",
    "    description='Folders:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='90%')\n",
    ")\n",
    "display(select)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51bc6559",
   "metadata": {},
   "source": [
    "Über die Window-Size kann die Größer der Fenster eingestellt werden. Standardmäßig werden 100.000 Punkte zu einem Fenster zusammengefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8adc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set window size\n",
    "window_size_int = widgets.BoundedIntText(\n",
    "    value=100000,\n",
    "    min=0,\n",
    "    max=1000000,\n",
    "    step=1,\n",
    "    description='Window Size:',\n",
    "    disabled=False\n",
    ")\n",
    "display(window_size_int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f85c4508",
   "metadata": {},
   "source": [
    "Über die Step-Size kann die Verschiebung der einzelnen Fenster eingestellt werden. Stellen wir diesen ebenfalls auf 100.000 kommt es zu keiner Überlappung, da jedes Fenster immer um 100.000 Punkte weitergeschoben wird. Stellen wir allerdings die Step-Size kleiner als die Window-Size, so kommt es zu einer Überlappung der Fenster, da die selben Punkte nun in mehreren Fenstern liegen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set window size\n",
    "step_size_int = widgets.BoundedIntText(\n",
    "    value=100000,\n",
    "    min=0,\n",
    "    max=1000000,\n",
    "    step=1,\n",
    "    description='Step Size:',\n",
    "    disabled=False\n",
    ")\n",
    "display(step_size_int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79465f17",
   "metadata": {},
   "source": [
    "Die letzte Einstellung beinhaltet die Aggregation der Werte. Es kann hierbei zwischen 'median', 'std', 'min' und 'max' gewählt werden. Die Aggregation bezieht sich hierbei immer auf die einzelnen Fenster. Alle Punkte innerhalb des Fenstern werden mittels der Aggregation zusammen gerechnet und es entsteht ein neuer einzelner Punkte pro Fenster. Aus diesem Grund ist es manchmal Ratsam eine Fensterüberlappung zu zulassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ba63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_type_dropdown = widgets.Dropdown(\n",
    "    options=['median', 'std', 'min', 'max'],\n",
    "    value='median',\n",
    "    description='Aggregation:',\n",
    "    disabled=False,\n",
    ")\n",
    "display(agg_type_dropdown)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ab6d722",
   "metadata": {},
   "source": [
    "Der letzten und längste Schritt ist nun die Generierung der einzelnen Plots untereinander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mdf file in loop and aggregate data\n",
    "# create a path object\n",
    "path = pl.Path(select.value)\n",
    "# list all mdf files in the path\n",
    "mdf_files = [x for x in path.iterdir() if x.is_file() and x.suffix == '.mf4']\n",
    "# loop over all mdf files\n",
    "for mdf_file in mdf_files:\n",
    "    # create a mdf object\n",
    "    mdf_obj = asammdf.MDF(mdf_file)\n",
    "    # transform to dataframe\n",
    "    df = mdf_obj.to_dataframe()\n",
    "    # aggregate data\n",
    "    df_agg = df[['p1', 'p2']].rolling(window=window_size_int.value, step=step_size_int.value).agg(agg_type_dropdown.value)\n",
    "    # give title to plot example: median of 2021-04-28 - Gut 1_725rpm@0% \n",
    "    title = f'{agg_type_dropdown.value} of {mdf_file.stem}'\n",
    "    # plot data\n",
    "    df_agg.plot(title=title, grid=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24e03416",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6efc6220-a14b-4ff0-b0d8-66ba17569050",
   "metadata": {},
   "source": [
    "## 3. Preprocessing [Valerij]\n",
    "\n",
    "Aufgeteilt nach KIDAQ und VIB (separat um es einfach zu halten)\n",
    "\n",
    "- Auswahl der Fenstergröße in Millisekunden\n",
    "- Auswahl der Abtastrate\n",
    "- Multi-Selektion der Aggregation (avg, mean, std, ...)\n",
    "- Frequenzanalyse\n",
    "- Fourier-Transformation\n",
    "- Fenstergröße nach Frequenzbereichen\n",
    "\n",
    "### 3.1 Vorbereitung der Tainings- und Testdaten\n",
    "\n",
    "Multi-Selektion für:\n",
    "\n",
    "- Features\n",
    "- Betriebspunkte (RPM/FLOWRATE)\n",
    "- Klassifikationsarten (Szenario / Testdurchlauf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b1531ac",
   "metadata": {},
   "source": [
    "### Vorauswahl der Feature Einstellungen und Auswahl der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f5b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6482394aef5246aeb93b5ed4dc51b418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Window size in ms: ', options=('100', '200', '300', '400', '500', '600', '700', '800', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fec0f8aafd9407987719951f99a03a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Raw data type: ', options=('KIDAQ', 'VIB'), value='KIDAQ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321504028d7741c0b8ca88b4e28e7efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Raw data folder: ', options=('Setup-I',), value='Setup-I')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WINDOW_SIZE_MS = [\"100\", \"200\", \"300\", \"400\", \"500\", \"600\", \"700\", \"800\",\"900\",\"1000\"]\n",
    "AGGREGATIONS = [\"std\", \"range\", \"iqr\", \"median\"]\n",
    "\n",
    "win_sizes = widgets.Dropdown(\n",
    "    placeholder= \"Choose the window size in ms\",\n",
    "    options = WINDOW_SIZE_MS,\n",
    "    description = \"Window size in ms: \",\n",
    "    ensure_option=True,\n",
    "    disabled = False\n",
    ")\n",
    "display(win_sizes)\n",
    "\n",
    "raw_data_type = widgets.Dropdown(\n",
    "    placeholder= \"Choose the raw data type\",\n",
    "    options = RAW_DATA_TYPE,\n",
    "    description = \"Raw data type: \",\n",
    "    ensure_option=True,\n",
    "    disabled = False\n",
    ")\n",
    "display(raw_data_type)\n",
    "\n",
    "# list all available directories in data/raw with max depth of 1\n",
    "raw_data_folders = [f.name for f in os.scandir(PATH_RAW_DATA) if f.is_dir()]\n",
    "raw_data_folder = widgets.Dropdown(\n",
    "    placeholder= \"Choose the raw data folder\",\n",
    "    options = raw_data_folders,\n",
    "    description = \"Raw data folder: \",\n",
    "    ensure_option=True,\n",
    "    disabled = False\n",
    ")\n",
    "display(raw_data_folder)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc4bf780",
   "metadata": {},
   "source": [
    "### Definition der Funktionen zum Preprocessing der verschiedenen Datentypen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da864f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "COLUMNS_KiDAQ = [\n",
    "    \"p1\",\n",
    "    \"p2\",\n",
    "    \"a2\",\n",
    "    \"T2\",\n",
    "    \"T1\"\n",
    "]\n",
    "\n",
    "\n",
    "CWD = pl.Path.cwd()\n",
    "\n",
    "PATH_RAW_DATA = CWD / \"data\" / \"raw\"\n",
    "PATH_TO_SETUP = PATH_RAW_DATA / raw_data_folder.value \n",
    "\n",
    "\n",
    "if raw_data_type.value == \"KIDAQ\":\n",
    "    raw_files = [file for file in PATH_TO_SETUP.glob(\"**/*.mf4\") if \"KiDAQ\" in file.parts]    \n",
    "elif raw_data_type.value == \"VIB\":\n",
    "    raw_files = [file for file in PATH_TO_SETUP.glob(\"**/*.csv\") if \"Rohdaten CSV\" in file.parts]\n",
    "def process_vib(file):\n",
    "    # error_type is the first folder name after the setup folder\n",
    "    error_type_with_number = file.parts[8]\n",
    "    error_type = error_type_with_number.split(\" \")[0]\n",
    "    rpm = file.parts[10].split(\"r\")[0]\n",
    "    rpm_percent = file.parts[11].split(\"%\")[0]\n",
    "    sensor = file.parts[-1].split(\" \")[0]\n",
    "    version = file.parts[12] if file.parts[12] != None else '0'\n",
    "\n",
    "    df = read_csv(file, skiprows=2, encoding=\"ISO-8859-1\", sep=\";\")\n",
    "\n",
    "    df[\"Timestamp [ns]\"] = pd.to_datetime(df[\"Timestamp [ns]\"], unit=\"ns\")\n",
    "    df = df.set_index(\"Timestamp [ns]\")\n",
    "    resampled = df.resample(\"1s\")\n",
    "\n",
    "    df_mean = resampled.mean()\n",
    "    df_mean = df_mean.rename(columns={\"Value\": f\"{sensor}_mean\"})\n",
    "    df_range = resampled.max() - resampled.min()\n",
    "    df_range = df_range.rename(columns={\"Value\": f\"{sensor}_range\"})\n",
    "    df_std = resampled.std()\n",
    "    df_std = df_std.rename(columns={\"Value\": f\"{sensor}_std\"})\n",
    "    df_iqr = resampled.quantile(0.75) - resampled.quantile(0.25)\n",
    "    df_iqr = df_iqr.rename(columns={\"Value\": f\"{sensor}_iqr\"})\n",
    "    df_mean_median = resampled.mean() - resampled.median()\n",
    "    df_mean_median = df_mean_median.rename(columns={\"Value\": f\"{sensor}_mean_median\"})\n",
    "\n",
    "    df = pd.concat([df_mean, df_range, df_std, df_iqr, df_mean_median], axis=1)\n",
    "    df[\"Fehlertyp\"] = error_type_with_number\n",
    "    df[\"Fehlertyp_allgemein\"] = error_type\n",
    "    df[\"rpm\"] = rpm\n",
    "    df[\"rpm%\"] = rpm_percent\n",
    "    df[\"version\"] = version\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=[\"Timestamp [ns]\"])\n",
    "    df[\"ID\"] = df.index\n",
    "    df = df.melt(\n",
    "        id_vars=[\"ID\", \"Fehlertyp\", \"Fehlertyp_allgemein\", \"rpm\", \"rpm%\", \"version\"],\n",
    "        var_name=\"Aggregation\",\n",
    "        value_name=\"Value\",\n",
    "    )\n",
    "\n",
    "    return df \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def process_kidaq(file):\n",
    "    \n",
    "    error_type_with_number = file.parts[8]\n",
    "    error_type = error_type_with_number.split(\" \")[0]\n",
    "    rpm = file.parts[10].split(\"r\")[0]\n",
    "    rpm_percent = file.parts[11].split(\"m\")[1].split(\"%\")[0]\n",
    "    \n",
    "    mdf = asammdf.MDF(file)\n",
    "    df = mdf.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df = df[COLUMNS_KiDAQ]\n",
    "\n",
    "    df_features = None\n",
    "\n",
    "    windows = df.groupby(df.index // (int(win_sizes.value) * 20))\n",
    "\n",
    "    for _, window in windows:\n",
    "\n",
    "        features = {\n",
    "            \"Fehlertyp\": error_type_with_number, \n",
    "            \"Fehlertyp_allgemein\": error_type,\n",
    "            \"rpm\": rpm, \n",
    "            \"rpm%\": rpm_percent\n",
    "            }\n",
    "\n",
    "        features[\"p1_std\"] = window[\"p1\"].std()\n",
    "        features[\"p2_std\"] = window[\"p2\"].std()\n",
    "        features[\"a2_std\"] = window[\"a2\"].std()\n",
    "        features[\"T2_std\"] = window[\"T2\"].std()\n",
    "        features[\"T1_std\"] = window[\"T1\"].std()\n",
    "        \n",
    "        features[\"p1_range\"] =  window[\"p1\"].max() - window[\"p1\"].min()\n",
    "        features[\"p2_range\"] =  window[\"p2\"].max() - window[\"p2\"].min()\n",
    "        features[\"a2_range\"] =  window[\"a2\"].max() - window[\"a2\"].min()\n",
    "        features[\"T2_range\"] =  window[\"T2\"].max() - window[\"T2\"].min()\n",
    "        features[\"T1_range\"] =  window[\"T1\"].max() - window[\"T1\"].min()\n",
    "        \n",
    "        features[\"p1_iqr\"] = window[\"p1\"].quantile(0.75) - window[\"p1\"].quantile(0.25)\n",
    "        features[\"p2_iqr\"] = window[\"p2\"].quantile(0.75) - window[\"p2\"].quantile(0.25)\n",
    "        features[\"a2_iqr\"] = window[\"a2\"].quantile(0.75) - window[\"a2\"].quantile(0.25)\n",
    "        features[\"T2_iqr\"] = window[\"T2\"].quantile(0.75) - window[\"T2\"].quantile(0.25)\n",
    "        features[\"T1_iqr\"] = window[\"T1\"].quantile(0.75) - window[\"T1\"].quantile(0.25)\n",
    "        \n",
    "        features[\"p1_mean_median\"] = window[\"p1\"].mean() - window[\"p1\"].median()\n",
    "        features[\"p2_mean_median\"] = window[\"p2\"].mean() - window[\"p2\"].median()\n",
    "        features[\"a2_mean_median\"] = window[\"a2\"].mean() - window[\"a2\"].median()\n",
    "        features[\"T2_mean_median\"] = window[\"T2\"].mean() - window[\"T2\"].median()\n",
    "        features[\"T1_mean_median\"] = window[\"T1\"].mean() - window[\"T1\"].median()\n",
    "\n",
    "        if df_features is None:\n",
    "            df_features = pd.DataFrame(features, index=[0])    \n",
    "        else:\n",
    "            df_features = pd.concat([df_features, pd.DataFrame(features, index=[0])])\n",
    "\n",
    "    return df_features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b132deb",
   "metadata": {},
   "source": [
    "### Funktionem zum Preprocessing der Daten in den Frequenbereich und Generierung der Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_freq_kidaq(file):\n",
    "\n",
    "    error_type_with_number = file.parts[8]\n",
    "    error_type = error_type_with_number.split(\" \")[0]\n",
    "    rpm = file.parts[10].split(\"r\")[0]\n",
    "    rpm_percent = file.parts[11].split(\"m\")[1].split(\"%\")[0]\n",
    "    \n",
    "    mdf = asammdf.MDF(file)\n",
    "    df = mdf.to_dataframe()\n",
    "    df.index = pd.to_timedelta(df.index, unit=\"s\")\n",
    "\n",
    "    resample_time = df.resample(\"1s\")\n",
    "\n",
    "    r = resample_time.aggregate(lambda sample: np.fft.fft)\n",
    "\n",
    "    \n",
    "    df_fft = pd.DataFrame(np.fft.fft(df))\n",
    "\n",
    "    df_fft.columns = df.columns\n",
    "    df_fft.index = df.index\n",
    "    df_fft.index = pd.to_timedelta(df_fft.index, unit=\"s\")\n",
    "    df_fft = df_fft.apply(np.abs)\n",
    "    resampled = df_fft.resample(\"1s\")\n",
    "\n",
    "    df_mean = resampled.mean().to_numpy()\n",
    "    df_range = resampled.max() - resampled.min()\n",
    "    df_range = df_range.to_numpy()\n",
    "    df_std = resampled.std().to_numpy()\n",
    "    df_iqr = resampled.quantile(0.75) - resampled.quantile(0.25)\n",
    "    df_iqr = df_iqr.to_numpy()\n",
    "    df_mean_median = resampled.mean() - resampled.median()\n",
    "    df_mean_median = df_mean_median.to_numpy()\n",
    "\n",
    "    data = np.concatenate((np.repeat([[error_type_with_number, error_type, rpm, rpm_percent]], df_mean.shape[0], axis=0), df_mean, df_range, df_std, df_iqr, df_mean_median), axis=1)\n",
    "\n",
    "    with open(\"fft_test.csv\", \"a\") as f:\n",
    "        for feature in data:\n",
    "            f.write(\";\".join(feature) + \"\\n\")    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for file in tqdm(raw_files):\n",
    "    df = process_freq_kidaq(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585ac03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d321ae3c36c34db4bfebd31f3c051fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "FREQ_WINDOWS = 16\n",
    "\n",
    "def process_freq_kidaq(file):\n",
    "\n",
    "    error_type_with_number = file.parts[8]\n",
    "    error_type = error_type_with_number.split(\" \")[0]\n",
    "    rpm = file.parts[10].split(\"r\")[0]\n",
    "    rpm_percent = file.parts[11].split(\"m\")[1].split(\"%\")[0]\n",
    "    \n",
    "    mdf = asammdf.MDF(file)\n",
    "    df = mdf.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df = df[COLUMNS_KiDAQ]\n",
    "\n",
    "    df_features = None\n",
    "\n",
    "    windows = df.groupby(df.index // (int(win_sizes.value) * 20))\n",
    "\n",
    "    for _, window in windows:\n",
    "\n",
    "        for col in window.columns:\n",
    "            X = np.fft.fft(window[col])\n",
    "            amps = np.abs(X)\n",
    "\n",
    "            xf = np.fft.fftfreq(len(window[col]), 1 / 20000)\n",
    "\n",
    "            idxMax = np.argmax(amps)\n",
    "\n",
    "            if df_features is None:\n",
    "                df_features = np.concatenate(([xf[idxMax]], [amps[idxMax]]), axis=0)\n",
    "            else:\n",
    "                df_features = np.concatenate((df_features, [xf[idxMax]], [amps[idxMax]]), axis=0)\n",
    "\n",
    "            freqs_window_size = len(amps) / FREQ_WINDOWS\n",
    "\n",
    "            for x in range(FREQ_WINDOWS):\n",
    "                freq_window = amps[int(x*freqs_window_size):int((x+1)*freqs_window_size)]\n",
    "                df_features = np.concatenate((df_features, [np.max(freq_window, axis=0)], [np.average(freq_window, axis=0)], [np.mean(freq_window, axis=0)] ), axis=0)\n",
    "\n",
    "    df_features = np.concatenate(([error_type_with_number], [error_type], [rpm], [rpm_percent], df_features), axis=0) \n",
    "\n",
    "    with open(f\"data/freq_{raw_data_type}_features.csv\", \"w\") as f:\n",
    "        f.write(\";\".join(df_features))\n",
    "\n",
    "    # for _, window in windows:\n",
    "\n",
    "    #         for col in window.columns:\n",
    "    #             X = np.fft.fft(window[col])\n",
    "    #             amps = np.abs(X)\n",
    "\n",
    "    #             xf = np.fft.fftfreq(len(window[col]), 1 / 20000)\n",
    "\n",
    "    #             idxMax = np.argmax(amps)\n",
    "\n",
    "    #             # create a dataframe with two columns amps and xf\n",
    "    #             df_fft = pd.DataFrame({'amps': amps, 'xf': xf})\n",
    "\n",
    "    #             freqs_window_size = len(amps) / FREQ_WINDOWS\n",
    "\n",
    "    #             for x in range(FREQ_WINDOWS):\n",
    "    #                 freq_window = amps[int(x*freqs_window_size):int((x+1)*freqs_window_size)]\n",
    "    #                 window_features = {\n",
    "    #                     \"Fehlertyp\": error_type_with_number,\n",
    "    #                     \"Fehlertyp_allgemein\": error_type,\n",
    "    #                     \"rpm\": rpm,\n",
    "    #                     \"rpm%\": rpm_percent,\n",
    "    #                     \"xf_max\": xf[idxMax],\n",
    "    #                     \"amps_max\": amps[idxMax],\n",
    "    #                     \"amps_max_window\": np.max(freq_window, axis=0),\n",
    "    #                     \"amps_avg_window\": np.average(freq_window, axis=0),\n",
    "    #                     \"amps_mean_window\": np.mean(freq_window, axis=0)\n",
    "    #                 }\n",
    "\n",
    "    #                 if df_features is None:\n",
    "    #                     df_features = pd.DataFrame(window_features, index=[0])\n",
    "    #                 else:\n",
    "    #                     df_features = pd.concat([df_features, pd.DataFrame(window_features, index=[0])])\n",
    "\n",
    "\n",
    "    # if not pl.Path(f\"freq_{raw_data_type}_1_features.csv\").exists():\n",
    "    #     df_features.to_csv(f\"data/freq_{raw_data_type}_features.csv\", header=True, index=False, sep=\";\")\n",
    "    # else:\n",
    "         \n",
    "    #     df_features.to_csv(f\"freq_{raw_data_type}_1_features.csv\", mode='a', header=False, index=False, sep=\";\")\n",
    "\n",
    "\n",
    "        \n",
    "for file in tqdm(raw_files):\n",
    "    process_freq_kidaq(file)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "281acf71",
   "metadata": {},
   "source": [
    "#### Durch ausführen der folgenden Zelle, werden die Daten mit den ausgewählten Parametern vorbereitet und abgespeichert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fa42ccb",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b92fefe-672c-4bba-97c1-f78747ff1dc1",
   "metadata": {},
   "source": [
    "## 4. Maschinelles Lernen [Kevin]\n",
    "\n",
    "In diesem Bereich können, mithilfe drei verschiedener Learner, Modell anhand der generierten Trainings- und Testdaten erzeugt werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56f2b565",
   "metadata": {},
   "source": [
    "### 4.1 Trainings- und Testdaten wählen und laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da6989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT TRAINING AND TEST DATA\n",
    "featureDataDir = list(filter(lambda x: os.path.isfile(os.path.join(PATH_FEATURE_DATA, x)) and DEFAULT_RAW_DATA_TYPE in x.upper(), os.listdir(PATH_FEATURE_DATA)))\n",
    "\n",
    "trainFileDropdown = widgets.Dropdown(description=\"training data\")\n",
    "trainFileDropdown.options = featureDataDir\n",
    "selectedTrainFile = None\n",
    "def onTrainigFileChange(change):\n",
    "    global selectedTrainFile\n",
    "    selectedTrainFile = change['new']\n",
    "trainFileDropdown.observe(onTrainigFileChange, names='value')\n",
    "display(trainFileDropdown)\n",
    "\n",
    "testFileDropdown = widgets.Dropdown(description=\"test data\")\n",
    "testFileDropdown.options = featureDataDir\n",
    "selectedTestFile = None\n",
    "def onTestFileChange(change):\n",
    "    global selectedTestFile\n",
    "    selectedTestFile = change['new']\n",
    "testFileDropdown.observe(onTestFileChange, names='value')\n",
    "display(testFileDropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51491a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "csvTrain = read_csv(PATH_FEATURE_DATA + selectedTrainFile, delimiter=\";\")\n",
    "csvTest = read_csv(PATH_FEATURE_DATA + selectedTestFile, delimiter=\";\")\n",
    "\n",
    "\n",
    "\n",
    "trainData = csvTrain.values\n",
    "testData = csvTest.values\n",
    "\n",
    "\n",
    "featureNames = csvTrain.columns.values[2:].tolist()\n",
    "trainX, trainY = trainData[:, 2:].astype('float32'), trainData[:, 1:2]\n",
    "testX, testY = testData[:, 2:].astype('float32'), testData[:, 1:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c9baf4d",
   "metadata": {},
   "source": [
    "### 4.2 Entscheidungsbaum - sklearn (empfohlen)\n",
    "\n",
    "\n",
    "In diesem Abschnitt wird mit der sklearn-Bibliothek ein Entscheidungsbaummodell trainiert. Dabei können verschiedene Parameter frei gewählt werden, wie die maximale Baumtiefe, die gewünschte Mindestgenauigkeit und die Anzahl der Suchiterationen.\n",
    "Das Programm wird dann versuchen, ausgehend von einer Baumtiefe von eins, einen möglichst einfachen Entscheidungsbaum zu generieren, der die gewünschte Genauigkeit erreicht."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1db3b37",
   "metadata": {},
   "source": [
    "#### 4.2.1 Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ddb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_MAX_DEPTH = 10                           # Maximum depth of the tree\n",
    "DT_EXPLORATION_TARGET_VAL_ACCURACY = 0.95   # Target accuracy for the decision tree\n",
    "DT_EXPLORATION_MAX_ITER = 100000            # Maximum number of iterations for the random search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdd07539",
   "metadata": {},
   "source": [
    "#### 4.2.2 Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DecisionTreeClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import clear_output\n",
    "import joblib\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder = labelEncoder.fit(np.ravel(trainY))\n",
    "labelEncodedTrainY = labelEncoder.transform(np.ravel(trainY))\n",
    "labelEncodedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "\n",
    "testAccuracy = 0.0\n",
    "trainAccuracy = 0.0\n",
    "iterations = 0\n",
    "depth = 1\n",
    "while (\n",
    "    testAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    "    or trainAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    ") and iterations < DT_EXPLORATION_MAX_ITER:\n",
    "    # Create decision tree classifier\n",
    "    \n",
    "\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, splitter=\"random\")\n",
    "    dt.fit(trainX, labelEncodedTrainY)\n",
    "\n",
    "    testPredictions = dt.predict(testX)\n",
    "    testAccuracy = accuracy_score(labelEncodedTestY, testPredictions)\n",
    "\n",
    "    trainPredictions = dt.predict(trainX)\n",
    "    trainAccuracy = accuracy_score(labelEncodedTrainY, trainPredictions)\n",
    "    iterations += 1\n",
    "    \n",
    "    print(\"Iteration: %d\" % iterations)\n",
    "    print(\"Depth: %d\" % depth)\n",
    "    print(\"Train Accuracy: %.2f%%\" % (trainAccuracy * 100.0))\n",
    "    print(\"Test Accuracy: %.2f%%\" % (testAccuracy * 100.0))\n",
    "\n",
    "    depth = 1 + iterations // (DT_EXPLORATION_MAX_ITER // DT_MAX_DEPTH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelName = \"dtc.\" + str(round(time.time()))\n",
    "os.makedirs(PATH_MODEL + modelName)\n",
    "with open(PATH_MODEL + modelName + \"/encoder.pickle\", \"wb\") as f:\n",
    "    joblib.dump(labelEncoder, f)\n",
    "joblib.dump(dt, PATH_MODEL + modelName + \"/dtc.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9316a0aa",
   "metadata": {},
   "source": [
    "### 4.3 Tiefes neuronales Netz - tensorflow\n",
    "\n",
    "Im folgenden Abschnitt wird mit der tensorflow-Bibliothek ein tiefes neuronales Netzwerk trainiert. Dabei können verschiedene Parameter eingestellt werden, wie die gewünschte Mindestgenauigkeit, die Anzahl der Suchiterationen, die minimale und maximale Anzahl an Schichten und Neuronen, die Toleranz für einen vorzeitigen Abbruch einer Iteration, die Ausgabeform, die maximale Anzahl an Epochen, die Batchgröße und die Batch Normalisierung.\n",
    "\n",
    "Bitte beachte, dass die genaue Syntax und die verfügbaren Optionen von tensorflow abhängen können. Es ist empfehlenswert, die offizielle tensorflow-Dokumentation für detaillierte Informationen zu konsultieren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1731eede",
   "metadata": {},
   "source": [
    "#### 4.3.1 Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_EXPLORATION_TARGET_VAL_ACCURACY = 0.98  # Target accuracy for the neural network\n",
    "DNN_EXPLORATION_MAX_ITER = 100              # Maximum number of iterations for the random search\n",
    "DNN_EXPLORATION_HIDDEN_LAYERS_MIN = 1       # Minimum number of hidden layers\n",
    "DNN_EXPLORATION_HIDDEN_LAYERS_MAX = 3       # Maximum number of hidden layers\n",
    "DNN_EXPLORATION_NEURONS_MIN = 4             # Minimum number of neurons per layer\n",
    "DNN_EXPLORATION_NEURONS_MAX = 16            # Maximum number of neurons per layer\n",
    "\n",
    "\n",
    "DNN_EARLY_STOPPING_PATIENCE = 50            # Patience for early stopping\n",
    "DNN_VERBOSE = 1                             # Verbosity level for the neural network\n",
    "DNN_EPOCHS = 2000                           # Maximum number of epochs for the neural network\n",
    "DNN_BATCH_SIZE = 128                        # Batch size for the neural network\n",
    "DNN_BATCH_NORMALIZATION = True              # Batch normalization for the neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bacf28f2",
   "metadata": {},
   "source": [
    "#### 4.3.2 Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural Network\n",
    "import json\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "onehotencoder = OneHotEncoder()\n",
    "onehotencoder = onehotencoder.fit(trainY)\n",
    "onehotEncodedTrainY = onehotencoder.transform(trainY).toarray()\n",
    "onehotEncodedTestY = onehotencoder.transform(testY).toarray()\n",
    "\n",
    "explorationResults = []\n",
    "\n",
    "\n",
    "n_features = trainX.shape[1]\n",
    "categorieCount = len(onehotEncodedTrainY[0])\n",
    "\n",
    "testAccuracy = 0.0\n",
    "trainAccuracy = 0.0\n",
    "model = Sequential()\n",
    "interation = 0\n",
    "while (\n",
    "    testAccuracy < DNN_EXPLORATION_TARGET_VAL_ACCURACY\n",
    "    or trainAccuracy < DNN_EXPLORATION_TARGET_VAL_ACCURACY\n",
    ") and interation < DNN_EXPLORATION_MAX_ITER:\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(n_features,)))\n",
    "    if DNN_BATCH_NORMALIZATION:\n",
    "        model.add(BatchNormalization())\n",
    "    denseCount = random.randint(\n",
    "        DNN_EXPLORATION_HIDDEN_LAYERS_MIN, DNN_EXPLORATION_HIDDEN_LAYERS_MAX\n",
    "    )\n",
    "    denseNeurons = []\n",
    "    for i in range(0, denseCount):\n",
    "        neuronCount = random.randint(\n",
    "            DNN_EXPLORATION_NEURONS_MIN, DNN_EXPLORATION_NEURONS_MAX\n",
    "        )\n",
    "        denseNeurons.append(neuronCount)\n",
    "        model.add(Dense(neuronCount, activation=\"tanh\"))\n",
    "        if DNN_BATCH_NORMALIZATION:\n",
    "            model.add(BatchNormalization())\n",
    "    model.add(Dense(categorieCount, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.fit(\n",
    "        trainX,\n",
    "        onehotEncodedTrainY,\n",
    "        epochs=DNN_EPOCHS,\n",
    "        batch_size=DNN_BATCH_SIZE,\n",
    "        verbose=DNN_VERBOSE,\n",
    "        validation_data=(testX, onehotEncodedTestY),\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=DNN_EARLY_STOPPING_PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainLoss, trainAccuracy = model.evaluate(trainX, onehotEncodedTrainY)\n",
    "    testLoss, testAccuracy = model.evaluate(testX, onehotEncodedTestY)\n",
    "\n",
    "    explorationResults.append(\n",
    "        {\n",
    "            \"dense_count\": denseCount,\n",
    "            \"dense_neurons\": denseNeurons,\n",
    "            \"train_loss\": trainLoss,\n",
    "            \"train_acc\": trainAccuracy,\n",
    "            \"test_loss\": testLoss,\n",
    "            \"test_acc\": testAccuracy,\n",
    "        }\n",
    "    )\n",
    "    interation += 1\n",
    "\n",
    "modelName = \"dnn.\" + str(round(time.time()))\n",
    "\n",
    "model.save(PATH_MODEL + modelName)\n",
    "\n",
    "with open(PATH_MODEL + modelName + \"/encoder.pickle\", \"wb\") as f:\n",
    "    joblib.dump(onehotencoder, f)\n",
    "\n",
    "with open(PATH_EXPLORATION_DATA + modelName + \".exploration_results.json\", \"w\") as f:\n",
    "    json.dump(explorationResults, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccb7f3ca",
   "metadata": {},
   "source": [
    "### 4.4 Extremes Gradienten-Boosting - XGBoost\n",
    "\n",
    "In diesem Abschnitt wird mit der xgboost-Bibliothek ein Modell mithilfe des \"Extreme Gradient Boosting\" trainiert. Dabei können verschiedene Parameter frei gewählt werden, wie die maximale Anzahl der Bäume, die gewünschte Mindestgenauigkeit und die Anzahl der Suchiterationen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b71bb915",
   "metadata": {},
   "source": [
    "#### 4.4.1 Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7faada",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_MAX_DEPTH = 10                           # Maximum depth of the tree\n",
    "DT_EXPLORATION_TARGET_VAL_ACCURACY = 0.95   # Target accuracy for the decision tree\n",
    "DT_EXPLORATION_MAX_ITER = 100000            # Maximum number of iterations for the random search\n",
    "\n",
    "DT_NUM_OF_ESTIMATORS = None                 # number of estimators (default = None -> number of estimators = number of classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ae31a94",
   "metadata": {},
   "source": [
    "#### 4.4.2 Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder = labelEncoder.fit(np.ravel(trainY))\n",
    "labelEncodedTrainY = labelEncoder.transform(np.ravel(trainY))\n",
    "labelEncodedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "testAccuracy = 0.0\n",
    "trainAccuracy = 0.0\n",
    "iterations = 0\n",
    "while (\n",
    "    testAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    "    or trainAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    ") and iterations < DT_EXPLORATION_MAX_ITER:\n",
    "    xgb = XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        enable_categorical=True,\n",
    "        max_depth=DT_MAX_DEPTH,\n",
    "        n_estimators=labelEncoder.classes_.size if DT_NUM_OF_ESTIMATORS == None else DT_NUM_OF_ESTIMATORS,\n",
    "        random_state=np.random.randint(),\n",
    "    )\n",
    "    # fit model\n",
    "    labeledTrainX = pd.DataFrame(trainX, columns=featureNames)\n",
    "    labeledTestX = pd.DataFrame(testX, columns=featureNames)\n",
    "\n",
    "    xgb.fit(\n",
    "        labeledTrainX, labelEncodedTrainY, eval_set=[(labeledTestX, labelEncodedTestY)]\n",
    "    )\n",
    "\n",
    "    testPredictions = xgb.predict(testX)\n",
    "    testAccuracy = accuracy_score(labelEncodedTestY, testPredictions)\n",
    "\n",
    "    trainPredictions = xgb.predict(trainX)\n",
    "    trainAccuracy = accuracy_score(labelEncodedTrainY, trainPredictions)\n",
    "    iterations += 1\n",
    "\n",
    "modelName = \"xgb.\" + str(round(time.time()))\n",
    "os.makedirs(PATH_MODEL + modelName)\n",
    "\n",
    "with open(PATH_MODEL + modelName + \"/encoder.pickle\", \"wb\") as f:\n",
    "    joblib.dump(labelEncoder, f)\n",
    "\n",
    "\n",
    "XGBClassifier.save_model(xgb, PATH_MODEL + modelName + \"/xgb.model\")\n",
    "\n",
    "featureMap = xgb.get_booster().get_score(importance_type=\"gain\")\n",
    "\n",
    "with open(PATH_MODEL + modelName + \"/feature_map.txt\", \"w\") as file:\n",
    "    for index, feature in enumerate(xgb.get_booster().feature_names):\n",
    "        file.write(f\"{index}\\t{feature}\\tq\\n\")\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (testAccuracy * 100.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87ab65a9",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "615dfc90-2798-42fe-8001-7be9ea78aa35",
   "metadata": {},
   "source": [
    "## 5. Modelanalyse des Learners [Kevin]\n",
    "\n",
    "**In diesem Abschnitt können in Abhängigkeit von der Auswahl des zu analysierenden Modells verschiedene Analysen durchgeführt werden.**\n",
    "\n",
    "- *Visuelle Darstellung (nur Entscheidungsbaum/Extremes Gradienten-Boosting)*: Eine visuelle Darstellung des Modells kann erstellt werden, um die Entscheidungslogik und Struktur intuitiv zu erfassen.\n",
    "\n",
    "- *Konfusionsmatrix*: Eine Konfusionsmatrix wird erstellt, um die Leistung des Modells bei der Klassifikation zu bewerten. Sie zeigt, wie gut das Modell verschiedene Klassen korrekt vorhersagt und welche Fehler gemacht werden.\n",
    "\n",
    "- *Test-Genauigkeit*: Die Test-Genauigkeit wird berechnet, um zu bewerten, wie gut das Modell auf unbekannten Daten abschneidet. Dies gibt einen Indikator dafür, wie zuverlässig die Vorhersagen des Modells sind.\n",
    "\n",
    "- *Feature Importances*: Es wird eine Analyse der Feature Importances durchgeführt, um die relative Bedeutung der verschiedenen Merkmale bei der Vorhersage zu bestimmen. Dies ermöglicht Einblicke in die relevanten Merkmale und kann bei der Feature-Auswahl oder -Gewichtung helfen.\n",
    "\n",
    "Diese Analysen bieten einen umfassenden Einblick in die Leistung und Funktionsweise des ausgewählten Modells und unterstützen bei der Interpretation der Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT LEARNER AND TEST DATA\n",
    "featureDataDir = list(filter(lambda x: os.path.isfile(os.path.join(PATH_FEATURE_DATA, x)) and DEFAULT_RAW_DATA_TYPE in x.upper(), os.listdir(PATH_FEATURE_DATA)))\n",
    "modelDir = os.listdir(PATH_MODEL)\n",
    "\n",
    "modelDropdown = widgets.Dropdown(description=\"model\")\n",
    "modelDropdown.options = modelDir\n",
    "selectedModelFile = None\n",
    "def onTrainigFileChange(change):\n",
    "    global selectedModelFile\n",
    "    selectedModelFile = change['new']\n",
    "modelDropdown.observe(onTrainigFileChange, names='value')\n",
    "display(modelDropdown)\n",
    "\n",
    "testFileDropdown = widgets.Dropdown(description=\"test data\")\n",
    "testFileDropdown.options = featureDataDir\n",
    "selectedTestFile = None\n",
    "def onTestFileChange(change):\n",
    "    global selectedTestFile\n",
    "    selectedTestFile = change['new']\n",
    "testFileDropdown.observe(onTestFileChange, names='value')\n",
    "display(testFileDropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3540ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYZE MODEL\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfmath\n",
    "import tensorflow_probability as tfp\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import joblib\n",
    "from matplotlib import pyplot\n",
    "from xgboost import XGBClassifier, plot_tree\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "csvTest = read_csv(PATH_FEATURE_DATA + selectedTestFile, delimiter=\";\")\n",
    "testData = csvTest.values\n",
    "testX, testY = testData[:, 2:].astype('float32'), testData[:, 1:2]\n",
    "featureNames = csvTest.columns.values[2:].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = None\n",
    "predictions = None\n",
    "transformedTestY = None\n",
    "confusionMatrix = None\n",
    "classes = None\n",
    "if (selectedModelFile.startswith('dnn')):\n",
    "    onehotencoder = None\n",
    "    with open(PATH_MODEL + selectedModelFile + '/encoder.pickle', 'rb') as f:\n",
    "        onehotencoder = joblib.load(f)\n",
    "    classes = onehotencoder.categories_[0]\n",
    "    transformedTestY = onehotencoder.transform(testY).toarray()    \n",
    "\n",
    "    model = tf.keras.models.load_model(PATH_MODEL + selectedModelFile)\n",
    "    predictions = model.predict(testX)\n",
    "\n",
    "    confusionMatrix = tf.math.confusion_matrix(np.argmax(transformedTestY, axis=1), np.argmax(predictions, axis=1))\n",
    "    equality = tf.math.equal(np.argmax(predictions, axis=1), np.argmax(transformedTestY, axis=1))\n",
    "    accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "elif (selectedModelFile.startswith('xgb')):\n",
    "    labelEncoder = None\n",
    "    with open(PATH_MODEL + selectedModelFile + '/encoder.pickle', 'rb') as f:\n",
    "        labelEncoder = joblib.load(f)\n",
    "    classes = labelEncoder.classes_\n",
    "    transformedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "    model = XGBClassifier()\n",
    "    model.load_model(PATH_MODEL + selectedModelFile + '/xgb.model')\n",
    "    predictions = model.predict(testX)\n",
    "    confusionMatrix = tfmath.confusion_matrix(transformedTestY, predictions)\n",
    "\n",
    "    for i in range(model.n_estimators):\n",
    "        plot_tree(model, num_trees=i, fmap=PATH_MODEL + selectedModelFile + '/feature_map.txt')\n",
    "        pyplot.gcf().set_dpi(1200)\n",
    "        pyplot.show()\n",
    "\n",
    "\n",
    "    #pyplot.show()\n",
    "    equality = tf.math.equal(predictions, transformedTestY)\n",
    "    accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "elif (selectedModelFile.startswith('dtc')):\n",
    "    labelEncoder = None\n",
    "    with open(PATH_MODEL + selectedModelFile + '/encoder.pickle', 'rb') as f:\n",
    "        labelEncoder = joblib.load(f)\n",
    "    classes = labelEncoder.classes_\n",
    "    transformedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "    model = joblib.load(PATH_MODEL + selectedModelFile + '/dtc.model')\n",
    "\n",
    "    predictions = model.predict(testX)\n",
    "    accuracy = accuracy_score(transformedTestY, predictions)\n",
    "    confusionMatrix = tfmath.confusion_matrix(transformedTestY, predictions)\n",
    "    plt.figure(figsize=(120, 40))       \n",
    "    tree.plot_tree(model, feature_names=featureNames, class_names=labelEncoder.classes_, filled=True)\n",
    "    plt.show()\n",
    "\n",
    "mat = pyplot.matshow(confusionMatrix, 1)\n",
    "mat.axes.set_xticks(np.arange(0, len(classes), 1))\n",
    "mat.axes.set_yticks(np.arange(0, len(classes), 1))\n",
    "mat.axes.set_xticklabels(classes, rotation=90)\n",
    "mat.axes.set_yticklabels(classes)\n",
    "for (x, y), value in np.ndenumerate(confusionMatrix):\n",
    "    pyplot.text(y, x, f\"{value:.2f}\", va=\"center\", ha=\"center\")\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "print('Test Accuracy: %.3f' % accuracy)\n",
    "\n",
    "\n",
    "#correlationMatrix = tfp.stats.correlation(testX)\n",
    "#pyplot.matshow(correlationMatrix)\n",
    "#pyplot.show()\n",
    "\n",
    "perm = PermutationImportance(model, scoring=\"neg_mean_squared_error\", random_state=1).fit(testX, transformedTestY)\n",
    "print(eli5.format_as_text(eli5.explain_weights(perm, feature_names=featureNames)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d508152e",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "390a4a00-cdc0-4fae-bd1f-e92cd875dec1",
   "metadata": {},
   "source": [
    "## 6. Statische Interpretation des Resultats\n",
    "\n",
    "### 6.1 Deskriptive und Explorative Datenanalyse\n",
    "Die deskriptive Datenanalyse der Gutdaten ergab interessante Erkenntnisse. Eine Fehlausrichtung zeigt sich durch die Umkehrung von p1 und p2. Dieser Effekt wird besonders deutlich, wenn der maximale Förderstrom (100%) betrachtet wird. In diesem Fall wird die Fehlausrichtung sehr gut sichtbar.\n",
    "\n",
    "<img src=\"img/kidaq_visualisierung.png\" alt=\"KiDAQ Visualisierung\" width=\"40%\" height=\"20%\" title=\"KiDAQ Visualisierung\"><br>\n",
    "\n",
    "Ein Laufradschaden hingegen äußert sich durch eine vergrößerte Entfernung zwischen p1 und p2. Besonders auffällig ist, dass sich dieser Abstand auch beim maximalen Förderstrom (100%) weiter vergrößert. Es zeigt sich also ein Trend, dass sowohl p1 als auch p2 sich voneinander entfernen, welches auf einen möglichen Laufradschaden hinweisen könnte.\n",
    "\n",
    "Das Resultat der Untersuchung ist, dass Veränderungen der Daten bereits anhand der unterschiedlichen Messreihen erkannt werden können. Sowohl die Fehlausrichtung, als auch der Laufradschaden lassen sich durch die Analyse der p1- und p2-Werte identifizieren. Dies ermöglicht eine frühzeitige Erkennung von potenziellen Problemen und gibt die Möglichkeit, geeignete Maßnahmen zur Behebung einzuleiten, bevor schwerwiegendere Schäden auftreten.\n",
    "\n",
    "Die deskriptive Datenanalyse liefert somit wertvolle Informationen, um die Qualität der Daten zu beurteilen und potenzielle Abweichungen oder Schäden zu erkennen. Durch regelmäßige Überprüfung der Messreihen können frühzeitig Anomalien festgestellt und entsprechende Maßnahmen ergriffen werden, um die Funktionalität und Zuverlässigkeit der untersuchten Systeme aufrechtzuerhalten.\n",
    "\n",
    "### 6.2 VIB-Fourier-Transformation\n",
    "Bei der Transformation der VIB-Daten ergab sich kein eindeutiges Bild. Die Überführung der Vibrationen in einen absoluten Zustandsraum zeigte jedoch interessante Ergebnisse. Es wurde ein signifikanter Peak bei 11 kHz sowie generell Frequenzen im hochfrequenten Bereich festgestellt.\n",
    "\n",
    "<img src=\"img/vib_fourier_transformation.png\" alt=\"VIB Fourier Transformation\" width=\"20%\" height=\"40%\" title=\"VIB Fourier Transformation\"><br>\n",
    "\n",
    "Der Peak bei 11 kHz deutet auf eine starke Aktivität oder Resonanz in diesem Frequenzbereich hin. Dies könnte auf eine bestimmte Komponente oder ein Phänomen hinweisen, das bei dieser Frequenz besonders präsent ist. Es ist wichtig, diese Beobachtung genauer zu untersuchen, um die genaue Ursache für den Peak zu ermitteln und mögliche Auswirkungen auf das System zu verstehen.\n",
    "\n",
    "Darüber hinaus ist es interessant zu bemerken, dass auch im allgemeinen hochfrequenten Bereich Frequenzen auffällig sind. Dies könnte auf eine Vielzahl von Effekten oder Aktivitäten hindeuten, die im Hochfrequenzspektrum auftreten. Es ist ratsam, weitere Analysen durchzuführen, um die Natur dieser Frequenzen zu bestimmen und festzustellen, ob sie mit bekannten Mustern oder Problemen in Verbindung stehen.\n",
    "\n",
    "Insgesamt deutet die Transformation der VBS-Daten auf eine komplexe und vielfältige Verteilung von Frequenzen hin, die auf verschiedene Phänomene im untersuchten System hinweisen könnten. Es ist wichtig, diese Ergebnisse weiter zu untersuchen, um ein besseres Verständnis für die zugrunde liegenden Mechanismen zu erlangen und mögliche Auswirkungen auf die Leistung oder den Zustand des Systems zu bewerten.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
