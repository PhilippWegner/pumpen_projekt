{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5a94bd2-a922-4b01-a9b9-7658e41f6036",
   "metadata": {},
   "source": [
    "# Aufbau der Notebooks [Philipp]\n",
    "\n",
    "- Multivalue bei Notebooks angucken\n",
    "- Wie installiere ich den \"scheiß\"\n",
    "- Widgets: https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3394772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import ipywidgets as widgets\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import asammdf\n",
    "from IPython.display import display\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pathlib as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ea6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the global variables\n",
    "\n",
    "PATH_RAW_DATA = \"./data/raw/\"\n",
    "PATH_FEATURE_DATA = \"./data/\"\n",
    "PATH_EXPLORATION_DATA = \"./exploration/\"\n",
    "PATH_MODEL = \"./models/\"\n",
    "DATA_SOURCE_KIDAQ = [\"TEST_NAME\", \"TEST_TYPE\", \"RPM\", \"FLOW_RATE\", \"P1\", \"P2\"]\n",
    "RAW_DATA_TYPE = [\"KIDAQ\", \"VIB\"]\n",
    "\n",
    "DATA_SOURCE_VID = [\n",
    "    \"TEST_NAME\",\n",
    "    \"TEST_TYPE\",\n",
    "    \"RPM\",\n",
    "    \"FLOW_RATE\",\n",
    "    \"S1\",\n",
    "    \"S2\",\n",
    "    \"S3\",\n",
    "    \"S4\",\n",
    "    \"S5\",\n",
    "    \"S6\",\n",
    "    \"S7\",\n",
    "    \"S8\",\n",
    "]\n",
    "FEATURE = [\n",
    "    \"STD\",\n",
    "    \"RANGE\",\n",
    "    \"IQR\",\n",
    "    \"MEAN_MEDIAN\",\n",
    "    \"FFT\",\n",
    "]\n",
    "OPERATING_POINT_FREQ = [725, 1450, 2175, 2900]\n",
    "OPERATING_POINT_FLOW_RATE = [0, 25, 50, 75, 100]\n",
    "\n",
    "DEFAULT_RAW_DATA_TYPE = RAW_DATA_TYPE[1]\n",
    "DEFAULT_RAW_DATA = DATA_SOURCE_KIDAQ\n",
    "\n",
    "DEFAULT_CLASS_LABEL = \"TEST_TYPE\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d08d88a7-0fc1-4c7a-91f7-df5aa0a4f9a2",
   "metadata": {},
   "source": [
    "## 1. Aufgabe und Daten erklären/beschreiben [Philipp]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6efc6220-a14b-4ff0-b0d8-66ba17569050",
   "metadata": {},
   "source": [
    "## 3. Preprocessing [Valerij]\n",
    "\n",
    "Aufgeteilt nach KIDAQ und VIB (separat um es einfach zu halten)\n",
    "\n",
    "- Auswahl der Fenstergröße in Millisekunden\n",
    "- Auswahl der Abtastrate\n",
    "- Multi-Selektion der Aggregation (avg, mean, std, ...)\n",
    "- Frequenzanalyse\n",
    "- Fourier-Transformation\n",
    "- Fenstergröße nach Frequenzbereichen\n",
    "\n",
    "### 3.1 Vorbereitung der Tainings- und Testdaten\n",
    "\n",
    "Multi-Selektion für:\n",
    "\n",
    "- Features\n",
    "- Betriebspunkte (RPM/FLOWRATE)\n",
    "- Klassifikationsarten (Szenario / Testdurchlauf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b1531ac",
   "metadata": {},
   "source": [
    "### Vorauswahl der Feature Einstellungen und Auswahl der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f5b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6482394aef5246aeb93b5ed4dc51b418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Window size in ms: ', options=('100', '200', '300', '400', '500', '600', '700', '800', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fec0f8aafd9407987719951f99a03a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Raw data type: ', options=('KIDAQ', 'VIB'), value='KIDAQ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321504028d7741c0b8ca88b4e28e7efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Raw data folder: ', options=('Setup-I',), value='Setup-I')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WINDOW_SIZE_MS = [\"100\", \"200\", \"300\", \"400\", \"500\", \"600\", \"700\", \"800\",\"900\",\"1000\"]\n",
    "AGGREGATIONS = [\"std\", \"range\", \"iqr\", \"median\"]\n",
    "\n",
    "win_sizes = widgets.Dropdown(\n",
    "    placeholder= \"Choose the window size in ms\",\n",
    "    options = WINDOW_SIZE_MS,\n",
    "    description = \"Window size in ms: \",\n",
    "    ensure_option=True,\n",
    "    disabled = False\n",
    ")\n",
    "display(win_sizes)\n",
    "\n",
    "raw_data_type = widgets.Dropdown(\n",
    "    placeholder= \"Choose the raw data type\",\n",
    "    options = RAW_DATA_TYPE,\n",
    "    description = \"Raw data type: \",\n",
    "    ensure_option=True,\n",
    "    disabled = False\n",
    ")\n",
    "display(raw_data_type)\n",
    "\n",
    "# list all available directories in data/raw with max depth of 1\n",
    "raw_data_folders = [f.name for f in os.scandir(PATH_RAW_DATA) if f.is_dir()]\n",
    "raw_data_folder = widgets.Dropdown(\n",
    "    placeholder= \"Choose the raw data folder\",\n",
    "    options = raw_data_folders,\n",
    "    description = \"Raw data folder: \",\n",
    "    ensure_option=True,\n",
    "    disabled = False\n",
    ")\n",
    "display(raw_data_folder)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc4bf780",
   "metadata": {},
   "source": [
    "### Definition der Funktionen zum Preprocessing der verschiedenen Datentypen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da864f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "COLUMNS_KiDAQ = [\n",
    "    \"p1\",\n",
    "    \"p2\",\n",
    "    \"a2\",\n",
    "    \"T2\",\n",
    "    \"T1\"\n",
    "]\n",
    "\n",
    "\n",
    "CWD = pl.Path.cwd()\n",
    "\n",
    "PATH_RAW_DATA = CWD / \"data\" / \"raw\"\n",
    "PATH_TO_SETUP = PATH_RAW_DATA / raw_data_folder.value \n",
    "\n",
    "\n",
    "if raw_data_type.value == \"KIDAQ\":\n",
    "    raw_files = [file for file in PATH_TO_SETUP.glob(\"**/*.mf4\") if \"KiDAQ\" in file.parts]    \n",
    "elif raw_data_type.value == \"VIB\":\n",
    "    raw_files = [file for file in PATH_TO_SETUP.glob(\"**/*.csv\") if \"Rohdaten CSV\" in file.parts]\n",
    "def process_vib(file):\n",
    "    # error_type is the first folder name after the setup folder\n",
    "    error_type_with_number = file.parts[8]\n",
    "    error_type = error_type_with_number.split(\" \")[0]\n",
    "    rpm = file.parts[10].split(\"r\")[0]\n",
    "    rpm_percent = file.parts[11].split(\"%\")[0]\n",
    "    sensor = file.parts[-1].split(\" \")[0]\n",
    "    version = file.parts[12] if file.parts[12] != None else '0'\n",
    "\n",
    "    df = read_csv(file, skiprows=2, encoding=\"ISO-8859-1\", sep=\";\")\n",
    "\n",
    "    df[\"Timestamp [ns]\"] = pd.to_datetime(df[\"Timestamp [ns]\"], unit=\"ns\")\n",
    "    df = df.set_index(\"Timestamp [ns]\")\n",
    "    resampled = df.resample(\"1s\")\n",
    "\n",
    "    df_mean = resampled.mean()\n",
    "    df_mean = df_mean.rename(columns={\"Value\": f\"{sensor}_mean\"})\n",
    "    df_range = resampled.max() - resampled.min()\n",
    "    df_range = df_range.rename(columns={\"Value\": f\"{sensor}_range\"})\n",
    "    df_std = resampled.std()\n",
    "    df_std = df_std.rename(columns={\"Value\": f\"{sensor}_std\"})\n",
    "    df_iqr = resampled.quantile(0.75) - resampled.quantile(0.25)\n",
    "    df_iqr = df_iqr.rename(columns={\"Value\": f\"{sensor}_iqr\"})\n",
    "    df_mean_median = resampled.mean() - resampled.median()\n",
    "    df_mean_median = df_mean_median.rename(columns={\"Value\": f\"{sensor}_mean_median\"})\n",
    "\n",
    "    df = pd.concat([df_mean, df_range, df_std, df_iqr, df_mean_median], axis=1)\n",
    "    df[\"Fehlertyp\"] = error_type_with_number\n",
    "    df[\"Fehlertyp_allgemein\"] = error_type\n",
    "    df[\"rpm\"] = rpm\n",
    "    df[\"rpm%\"] = rpm_percent\n",
    "    df[\"version\"] = version\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=[\"Timestamp [ns]\"])\n",
    "    df[\"ID\"] = df.index\n",
    "    df = df.melt(\n",
    "        id_vars=[\"ID\", \"Fehlertyp\", \"Fehlertyp_allgemein\", \"rpm\", \"rpm%\", \"version\"],\n",
    "        var_name=\"Aggregation\",\n",
    "        value_name=\"Value\",\n",
    "    )\n",
    "\n",
    "    return df \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def process_kidaq(file):\n",
    "    \n",
    "    error_type_with_number = file.parts[8]\n",
    "    error_type = error_type_with_number.split(\" \")[0]\n",
    "    rpm = file.parts[10].split(\"r\")[0]\n",
    "    rpm_percent = file.parts[11].split(\"m\")[1].split(\"%\")[0]\n",
    "    \n",
    "    mdf = asammdf.MDF(file)\n",
    "    df = mdf.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df = df[COLUMNS_KiDAQ]\n",
    "\n",
    "    df_features = None\n",
    "\n",
    "    windows = df.groupby(df.index // (int(win_sizes.value) * 20))\n",
    "\n",
    "    for _, window in windows:\n",
    "\n",
    "        features = {\n",
    "            \"Fehlertyp\": error_type_with_number, \n",
    "            \"Fehlertyp_allgemein\": error_type,\n",
    "            \"rpm\": rpm, \n",
    "            \"rpm%\": rpm_percent\n",
    "            }\n",
    "\n",
    "        features[\"p1_std\"] = window[\"p1\"].std()\n",
    "        features[\"p2_std\"] = window[\"p2\"].std()\n",
    "        features[\"a2_std\"] = window[\"a2\"].std()\n",
    "        features[\"T2_std\"] = window[\"T2\"].std()\n",
    "        features[\"T1_std\"] = window[\"T1\"].std()\n",
    "        \n",
    "        features[\"p1_range\"] =  window[\"p1\"].max() - window[\"p1\"].min()\n",
    "        features[\"p2_range\"] =  window[\"p2\"].max() - window[\"p2\"].min()\n",
    "        features[\"a2_range\"] =  window[\"a2\"].max() - window[\"a2\"].min()\n",
    "        features[\"T2_range\"] =  window[\"T2\"].max() - window[\"T2\"].min()\n",
    "        features[\"T1_range\"] =  window[\"T1\"].max() - window[\"T1\"].min()\n",
    "        \n",
    "        features[\"p1_iqr\"] = window[\"p1\"].quantile(0.75) - window[\"p1\"].quantile(0.25)\n",
    "        features[\"p2_iqr\"] = window[\"p2\"].quantile(0.75) - window[\"p2\"].quantile(0.25)\n",
    "        features[\"a2_iqr\"] = window[\"a2\"].quantile(0.75) - window[\"a2\"].quantile(0.25)\n",
    "        features[\"T2_iqr\"] = window[\"T2\"].quantile(0.75) - window[\"T2\"].quantile(0.25)\n",
    "        features[\"T1_iqr\"] = window[\"T1\"].quantile(0.75) - window[\"T1\"].quantile(0.25)\n",
    "        \n",
    "        features[\"p1_mean_median\"] = window[\"p1\"].mean() - window[\"p1\"].median()\n",
    "        features[\"p2_mean_median\"] = window[\"p2\"].mean() - window[\"p2\"].median()\n",
    "        features[\"a2_mean_median\"] = window[\"a2\"].mean() - window[\"a2\"].median()\n",
    "        features[\"T2_mean_median\"] = window[\"T2\"].mean() - window[\"T2\"].median()\n",
    "        features[\"T1_mean_median\"] = window[\"T1\"].mean() - window[\"T1\"].median()\n",
    "\n",
    "        if df_features is None:\n",
    "            df_features = pd.DataFrame(features, index=[0])    \n",
    "        else:\n",
    "            df_features = pd.concat([df_features, pd.DataFrame(features, index=[0])])\n",
    "\n",
    "    return df_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b132deb",
   "metadata": {},
   "source": [
    "### Funktionem zum Preprocessing der Daten in den Frequenbereich und Generierung der Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_freq_kidaq(file):\n",
    "\n",
    "    error_type_with_number = file.parts[8]\n",
    "    error_type = error_type_with_number.split(\" \")[0]\n",
    "    rpm = file.parts[10].split(\"r\")[0]\n",
    "    rpm_percent = file.parts[11].split(\"m\")[1].split(\"%\")[0]\n",
    "    \n",
    "    mdf = asammdf.MDF(file)\n",
    "    df = mdf.to_dataframe()\n",
    "    df.index = pd.to_timedelta(df.index, unit=\"s\")\n",
    "\n",
    "    resample_time = df.resample(\"1s\")\n",
    "\n",
    "    r = resample_time.aggregate(lambda sample: np.fft.fft)\n",
    "\n",
    "    \n",
    "    df_fft = pd.DataFrame(np.fft.fft(df))\n",
    "\n",
    "    df_fft.columns = df.columns\n",
    "    df_fft.index = df.index\n",
    "    df_fft.index = pd.to_timedelta(df_fft.index, unit=\"s\")\n",
    "    df_fft = df_fft.apply(np.abs)\n",
    "    resampled = df_fft.resample(\"1s\")\n",
    "\n",
    "    df_mean = resampled.mean().to_numpy()\n",
    "    df_range = resampled.max() - resampled.min()\n",
    "    df_range = df_range.to_numpy()\n",
    "    df_std = resampled.std().to_numpy()\n",
    "    df_iqr = resampled.quantile(0.75) - resampled.quantile(0.25)\n",
    "    df_iqr = df_iqr.to_numpy()\n",
    "    df_mean_median = resampled.mean() - resampled.median()\n",
    "    df_mean_median = df_mean_median.to_numpy()\n",
    "\n",
    "    data = np.concatenate((np.repeat([[error_type_with_number, error_type, rpm, rpm_percent]], df_mean.shape[0], axis=0), df_mean, df_range, df_std, df_iqr, df_mean_median), axis=1)\n",
    "\n",
    "    with open(\"fft_test.csv\", \"a\") as f:\n",
    "        for feature in data:\n",
    "            f.write(\";\".join(feature) + \"\\n\")    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for file in tqdm(raw_files):\n",
    "    df = process_freq_kidaq(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585ac03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d321ae3c36c34db4bfebd31f3c051fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "FREQ_WINDOWS = 16\n",
    "\n",
    "def process_freq_kidaq(file):\n",
    "\n",
    "    error_type_with_number = file.parts[8]\n",
    "    error_type = error_type_with_number.split(\" \")[0]\n",
    "    rpm = file.parts[10].split(\"r\")[0]\n",
    "    rpm_percent = file.parts[11].split(\"m\")[1].split(\"%\")[0]\n",
    "    \n",
    "    mdf = asammdf.MDF(file)\n",
    "    df = mdf.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df = df[COLUMNS_KiDAQ]\n",
    "\n",
    "    df_features = None\n",
    "\n",
    "    windows = df.groupby(df.index // (int(win_sizes.value) * 20))\n",
    "\n",
    "    for _, window in windows:\n",
    "\n",
    "        for col in window.columns:\n",
    "            X = np.fft.fft(window[col])\n",
    "            amps = np.abs(X)\n",
    "\n",
    "            xf = np.fft.fftfreq(len(window[col]), 1 / 20000)\n",
    "\n",
    "            idxMax = np.argmax(amps)\n",
    "\n",
    "            if df_features is None:\n",
    "                df_features = np.concatenate(([xf[idxMax]], [amps[idxMax]]), axis=0)\n",
    "            else:\n",
    "                df_features = np.concatenate((df_features, [xf[idxMax]], [amps[idxMax]]), axis=0)\n",
    "\n",
    "            freqs_window_size = len(amps) / FREQ_WINDOWS\n",
    "\n",
    "            for x in range(FREQ_WINDOWS):\n",
    "                freq_window = amps[int(x*freqs_window_size):int((x+1)*freqs_window_size)]\n",
    "                df_features = np.concatenate((df_features, [np.max(freq_window, axis=0)], [np.average(freq_window, axis=0)], [np.mean(freq_window, axis=0)] ), axis=0)\n",
    "\n",
    "    df_features = np.concatenate(([error_type_with_number], [error_type], [rpm], [rpm_percent], df_features), axis=0) \n",
    "\n",
    "    with open(f\"data/freq_{raw_data_type}_features.csv\", \"w\") as f:\n",
    "        f.write(\";\".join(df_features))\n",
    "\n",
    "    # for _, window in windows:\n",
    "\n",
    "    #         for col in window.columns:\n",
    "    #             X = np.fft.fft(window[col])\n",
    "    #             amps = np.abs(X)\n",
    "\n",
    "    #             xf = np.fft.fftfreq(len(window[col]), 1 / 20000)\n",
    "\n",
    "    #             idxMax = np.argmax(amps)\n",
    "\n",
    "    #             # create a dataframe with two columns amps and xf\n",
    "    #             df_fft = pd.DataFrame({'amps': amps, 'xf': xf})\n",
    "\n",
    "    #             freqs_window_size = len(amps) / FREQ_WINDOWS\n",
    "\n",
    "    #             for x in range(FREQ_WINDOWS):\n",
    "    #                 freq_window = amps[int(x*freqs_window_size):int((x+1)*freqs_window_size)]\n",
    "    #                 window_features = {\n",
    "    #                     \"Fehlertyp\": error_type_with_number,\n",
    "    #                     \"Fehlertyp_allgemein\": error_type,\n",
    "    #                     \"rpm\": rpm,\n",
    "    #                     \"rpm%\": rpm_percent,\n",
    "    #                     \"xf_max\": xf[idxMax],\n",
    "    #                     \"amps_max\": amps[idxMax],\n",
    "    #                     \"amps_max_window\": np.max(freq_window, axis=0),\n",
    "    #                     \"amps_avg_window\": np.average(freq_window, axis=0),\n",
    "    #                     \"amps_mean_window\": np.mean(freq_window, axis=0)\n",
    "    #                 }\n",
    "\n",
    "    #                 if df_features is None:\n",
    "    #                     df_features = pd.DataFrame(window_features, index=[0])\n",
    "    #                 else:\n",
    "    #                     df_features = pd.concat([df_features, pd.DataFrame(window_features, index=[0])])\n",
    "\n",
    "\n",
    "    # if not pl.Path(f\"freq_{raw_data_type}_1_features.csv\").exists():\n",
    "    #     df_features.to_csv(f\"data/freq_{raw_data_type}_features.csv\", header=True, index=False, sep=\";\")\n",
    "    # else:\n",
    "         \n",
    "    #     df_features.to_csv(f\"freq_{raw_data_type}_1_features.csv\", mode='a', header=False, index=False, sep=\";\")\n",
    "\n",
    "\n",
    "        \n",
    "for file in tqdm(raw_files):\n",
    "    process_freq_kidaq(file)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "281acf71",
   "metadata": {},
   "source": [
    "### Durch ausführen der folgenden Zelle, werden die Daten mit den ausgewählten Parametern vorbereitet und abgespeichert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8ed0170-f3d5-4287-8dd8-7439080d27af",
   "metadata": {},
   "source": [
    "## 4. Deskriptive/Explorative Datenanalyse [Philipp]\n",
    "\n",
    "- Plots\n",
    "- Beschreibung der Plots\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b92fefe-672c-4bba-97c1-f78747ff1dc1",
   "metadata": {},
   "source": [
    "## 5. Maschinelles Lernen [Kevin]\n",
    "\n",
    "In diesem Bereich können, mithilfe drei verschiedener Learner, Modell anhand der generierten Trainings- und Testdaten erzeugt werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56f2b565",
   "metadata": {},
   "source": [
    "### 5.1 Trainings- und Testdaten wählen und laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da6989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT TRAINING AND TEST DATA\n",
    "featureDataDir = list(filter(lambda x: os.path.isfile(os.path.join(PATH_FEATURE_DATA, x)) and DEFAULT_RAW_DATA_TYPE in x.upper(), os.listdir(PATH_FEATURE_DATA)))\n",
    "\n",
    "trainFileDropdown = widgets.Dropdown(description=\"training data\")\n",
    "trainFileDropdown.options = featureDataDir\n",
    "selectedTrainFile = None\n",
    "def onTrainigFileChange(change):\n",
    "    global selectedTrainFile\n",
    "    selectedTrainFile = change['new']\n",
    "trainFileDropdown.observe(onTrainigFileChange, names='value')\n",
    "display(trainFileDropdown)\n",
    "\n",
    "testFileDropdown = widgets.Dropdown(description=\"test data\")\n",
    "testFileDropdown.options = featureDataDir\n",
    "selectedTestFile = None\n",
    "def onTestFileChange(change):\n",
    "    global selectedTestFile\n",
    "    selectedTestFile = change['new']\n",
    "testFileDropdown.observe(onTestFileChange, names='value')\n",
    "display(testFileDropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51491a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "csvTrain = read_csv(PATH_FEATURE_DATA + selectedTrainFile, delimiter=\";\")\n",
    "csvTest = read_csv(PATH_FEATURE_DATA + selectedTestFile, delimiter=\";\")\n",
    "\n",
    "\n",
    "\n",
    "trainData = csvTrain.values\n",
    "testData = csvTest.values\n",
    "\n",
    "\n",
    "featureNames = csvTrain.columns.values[2:].tolist()\n",
    "trainX, trainY = trainData[:, 2:].astype('float32'), trainData[:, 1:2]\n",
    "testX, testY = testData[:, 2:].astype('float32'), testData[:, 1:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c9baf4d",
   "metadata": {},
   "source": [
    "### 5.2 Entscheidungsbaum - sklearn (empfohlen)\n",
    "\n",
    "\n",
    "In diesem Abschnitt wird mit der sklearn-Bibliothek ein Entscheidungsbaummodell trainiert. Dabei können verschiedene Parameter frei gewählt werden, wie die maximale Baumtiefe, die gewünschte Mindestgenauigkeit und die Anzahl der Suchiterationen.\n",
    "Das Programm wird dann versuchen, ausgehend von einer Baumtiefe von eins, einen möglichst einfachen Entscheidungsbaum zu generieren, der die gewünschte Genauigkeit erreicht."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1db3b37",
   "metadata": {},
   "source": [
    "#### 5.2.1 Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ddb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_MAX_DEPTH = 10                           # Maximum depth of the tree\n",
    "DT_EXPLORATION_TARGET_VAL_ACCURACY = 0.95   # Target accuracy for the decision tree\n",
    "DT_EXPLORATION_MAX_ITER = 100000            # Maximum number of iterations for the random search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdd07539",
   "metadata": {},
   "source": [
    "#### 5.2.2 Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DecisionTreeClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import clear_output\n",
    "import joblib\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder = labelEncoder.fit(np.ravel(trainY))\n",
    "labelEncodedTrainY = labelEncoder.transform(np.ravel(trainY))\n",
    "labelEncodedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "\n",
    "testAccuracy = 0.0\n",
    "trainAccuracy = 0.0\n",
    "iterations = 0\n",
    "depth = 1\n",
    "while (\n",
    "    testAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    "    or trainAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    ") and iterations < DT_EXPLORATION_MAX_ITER:\n",
    "    # Create decision tree classifier\n",
    "    \n",
    "\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, splitter=\"random\")\n",
    "    dt.fit(trainX, labelEncodedTrainY)\n",
    "\n",
    "    testPredictions = dt.predict(testX)\n",
    "    testAccuracy = accuracy_score(labelEncodedTestY, testPredictions)\n",
    "\n",
    "    trainPredictions = dt.predict(trainX)\n",
    "    trainAccuracy = accuracy_score(labelEncodedTrainY, trainPredictions)\n",
    "    iterations += 1\n",
    "    \n",
    "    print(\"Iteration: %d\" % iterations)\n",
    "    print(\"Depth: %d\" % depth)\n",
    "    print(\"Train Accuracy: %.2f%%\" % (trainAccuracy * 100.0))\n",
    "    print(\"Test Accuracy: %.2f%%\" % (testAccuracy * 100.0))\n",
    "\n",
    "    depth = 1 + iterations // (DT_EXPLORATION_MAX_ITER // DT_MAX_DEPTH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelName = \"dtc.\" + str(round(time.time()))\n",
    "os.makedirs(PATH_MODEL + modelName)\n",
    "with open(PATH_MODEL + modelName + \"/encoder.pickle\", \"wb\") as f:\n",
    "    joblib.dump(labelEncoder, f)\n",
    "joblib.dump(dt, PATH_MODEL + modelName + \"/dtc.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9316a0aa",
   "metadata": {},
   "source": [
    "### 5.3 Tiefes neuronales Netz - tensorflow\n",
    "\n",
    "Im folgenden Abschnitt wird mit der tensorflow-Bibliothek ein tiefes neuronales Netzwerk trainiert. Dabei können verschiedene Parameter eingestellt werden, wie die gewünschte Mindestgenauigkeit, die Anzahl der Suchiterationen, die minimale und maximale Anzahl an Schichten und Neuronen, die Toleranz für einen vorzeitigen Abbruch einer Iteration, die Ausgabeform, die maximale Anzahl an Epochen, die Batchgröße und die Batch Normalisierung.\n",
    "\n",
    "Bitte beachte, dass die genaue Syntax und die verfügbaren Optionen von tensorflow abhängen können. Es ist empfehlenswert, die offizielle tensorflow-Dokumentation für detaillierte Informationen zu konsultieren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1731eede",
   "metadata": {},
   "source": [
    "#### 5.3.1 Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_EXPLORATION_TARGET_VAL_ACCURACY = 0.98  # Target accuracy for the neural network\n",
    "DNN_EXPLORATION_MAX_ITER = 100              # Maximum number of iterations for the random search\n",
    "DNN_EXPLORATION_HIDDEN_LAYERS_MIN = 1       # Minimum number of hidden layers\n",
    "DNN_EXPLORATION_HIDDEN_LAYERS_MAX = 3       # Maximum number of hidden layers\n",
    "DNN_EXPLORATION_NEURONS_MIN = 4             # Minimum number of neurons per layer\n",
    "DNN_EXPLORATION_NEURONS_MAX = 16            # Maximum number of neurons per layer\n",
    "\n",
    "\n",
    "DNN_EARLY_STOPPING_PATIENCE = 50            # Patience for early stopping\n",
    "DNN_VERBOSE = 1                             # Verbosity level for the neural network\n",
    "DNN_EPOCHS = 2000                           # Maximum number of epochs for the neural network\n",
    "DNN_BATCH_SIZE = 128                        # Batch size for the neural network\n",
    "DNN_BATCH_NORMALIZATION = True              # Batch normalization for the neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bacf28f2",
   "metadata": {},
   "source": [
    "#### 5.3.2 Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural Network\n",
    "import json\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "onehotencoder = OneHotEncoder()\n",
    "onehotencoder = onehotencoder.fit(trainY)\n",
    "onehotEncodedTrainY = onehotencoder.transform(trainY).toarray()\n",
    "onehotEncodedTestY = onehotencoder.transform(testY).toarray()\n",
    "\n",
    "explorationResults = []\n",
    "\n",
    "\n",
    "n_features = trainX.shape[1]\n",
    "categorieCount = len(onehotEncodedTrainY[0])\n",
    "\n",
    "testAccuracy = 0.0\n",
    "trainAccuracy = 0.0\n",
    "model = Sequential()\n",
    "interation = 0\n",
    "while (\n",
    "    testAccuracy < DNN_EXPLORATION_TARGET_VAL_ACCURACY\n",
    "    or trainAccuracy < DNN_EXPLORATION_TARGET_VAL_ACCURACY\n",
    ") and interation < DNN_EXPLORATION_MAX_ITER:\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(n_features,)))\n",
    "    if DNN_BATCH_NORMALIZATION:\n",
    "        model.add(BatchNormalization())\n",
    "    denseCount = random.randint(\n",
    "        DNN_EXPLORATION_HIDDEN_LAYERS_MIN, DNN_EXPLORATION_HIDDEN_LAYERS_MAX\n",
    "    )\n",
    "    denseNeurons = []\n",
    "    for i in range(0, denseCount):\n",
    "        neuronCount = random.randint(\n",
    "            DNN_EXPLORATION_NEURONS_MIN, DNN_EXPLORATION_NEURONS_MAX\n",
    "        )\n",
    "        denseNeurons.append(neuronCount)\n",
    "        model.add(Dense(neuronCount, activation=\"tanh\"))\n",
    "        if DNN_BATCH_NORMALIZATION:\n",
    "            model.add(BatchNormalization())\n",
    "    model.add(Dense(categorieCount, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.fit(\n",
    "        trainX,\n",
    "        onehotEncodedTrainY,\n",
    "        epochs=DNN_EPOCHS,\n",
    "        batch_size=DNN_BATCH_SIZE,\n",
    "        verbose=DNN_VERBOSE,\n",
    "        validation_data=(testX, onehotEncodedTestY),\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=DNN_EARLY_STOPPING_PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainLoss, trainAccuracy = model.evaluate(trainX, onehotEncodedTrainY)\n",
    "    testLoss, testAccuracy = model.evaluate(testX, onehotEncodedTestY)\n",
    "\n",
    "    explorationResults.append(\n",
    "        {\n",
    "            \"dense_count\": denseCount,\n",
    "            \"dense_neurons\": denseNeurons,\n",
    "            \"train_loss\": trainLoss,\n",
    "            \"train_acc\": trainAccuracy,\n",
    "            \"test_loss\": testLoss,\n",
    "            \"test_acc\": testAccuracy,\n",
    "        }\n",
    "    )\n",
    "    interation += 1\n",
    "\n",
    "modelName = \"dnn.\" + str(round(time.time()))\n",
    "\n",
    "model.save(PATH_MODEL + modelName)\n",
    "\n",
    "with open(PATH_MODEL + modelName + \"/encoder.pickle\", \"wb\") as f:\n",
    "    joblib.dump(onehotencoder, f)\n",
    "\n",
    "with open(PATH_EXPLORATION_DATA + modelName + \".exploration_results.json\", \"w\") as f:\n",
    "    json.dump(explorationResults, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccb7f3ca",
   "metadata": {},
   "source": [
    "### 5.4 Extremes Gradienten-Boosting - XGBoost\n",
    "\n",
    "In diesem Abschnitt wird mit der xgboost-Bibliothek ein Modell mithilfe des \"Extreme Gradient Boosting\" trainiert. Dabei können verschiedene Parameter frei gewählt werden, wie die maximale Anzahl der Bäume, die gewünschte Mindestgenauigkeit und die Anzahl der Suchiterationen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b71bb915",
   "metadata": {},
   "source": [
    "#### 5.4.1 Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7faada",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_MAX_DEPTH = 10                           # Maximum depth of the tree\n",
    "DT_EXPLORATION_TARGET_VAL_ACCURACY = 0.95   # Target accuracy for the decision tree\n",
    "DT_EXPLORATION_MAX_ITER = 100000            # Maximum number of iterations for the random search\n",
    "\n",
    "DT_NUM_OF_ESTIMATORS = None                 # number of estimators (default = None -> number of estimators = number of classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ae31a94",
   "metadata": {},
   "source": [
    "#### 5.4.2 Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder = labelEncoder.fit(np.ravel(trainY))\n",
    "labelEncodedTrainY = labelEncoder.transform(np.ravel(trainY))\n",
    "labelEncodedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "testAccuracy = 0.0\n",
    "trainAccuracy = 0.0\n",
    "iterations = 0\n",
    "while (\n",
    "    testAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    "    or trainAccuracy < DT_EXPLORATION_TARGET_VAL_ACCURACY\n",
    ") and iterations < DT_EXPLORATION_MAX_ITER:\n",
    "    xgb = XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        enable_categorical=True,\n",
    "        max_depth=DT_MAX_DEPTH,\n",
    "        n_estimators=labelEncoder.classes_.size if DT_NUM_OF_ESTIMATORS == None else DT_NUM_OF_ESTIMATORS,\n",
    "        random_state=np.random.randint(),\n",
    "    )\n",
    "    # fit model\n",
    "    labeledTrainX = pd.DataFrame(trainX, columns=featureNames)\n",
    "    labeledTestX = pd.DataFrame(testX, columns=featureNames)\n",
    "\n",
    "    xgb.fit(\n",
    "        labeledTrainX, labelEncodedTrainY, eval_set=[(labeledTestX, labelEncodedTestY)]\n",
    "    )\n",
    "\n",
    "    testPredictions = xgb.predict(testX)\n",
    "    testAccuracy = accuracy_score(labelEncodedTestY, testPredictions)\n",
    "\n",
    "    trainPredictions = xgb.predict(trainX)\n",
    "    trainAccuracy = accuracy_score(labelEncodedTrainY, trainPredictions)\n",
    "    iterations += 1\n",
    "\n",
    "modelName = \"xgb.\" + str(round(time.time()))\n",
    "os.makedirs(PATH_MODEL + modelName)\n",
    "\n",
    "with open(PATH_MODEL + modelName + \"/encoder.pickle\", \"wb\") as f:\n",
    "    joblib.dump(labelEncoder, f)\n",
    "\n",
    "\n",
    "XGBClassifier.save_model(xgb, PATH_MODEL + modelName + \"/xgb.model\")\n",
    "\n",
    "featureMap = xgb.get_booster().get_score(importance_type=\"gain\")\n",
    "\n",
    "with open(PATH_MODEL + modelName + \"/feature_map.txt\", \"w\") as file:\n",
    "    for index, feature in enumerate(xgb.get_booster().feature_names):\n",
    "        file.write(f\"{index}\\t{feature}\\tq\\n\")\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (testAccuracy * 100.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "615dfc90-2798-42fe-8001-7be9ea78aa35",
   "metadata": {},
   "source": [
    "## 6. Modelanalyse des Learners [Kevin]\n",
    "\n",
    "**In diesem Abschnitt können in Abhängigkeit von der Auswahl des zu analysierenden Modells verschiedene Analysen durchgeführt werden.**\n",
    "\n",
    "- *Visuelle Darstellung (nur Entscheidungsbaum/Extremes Gradienten-Boosting)*: Eine visuelle Darstellung des Modells kann erstellt werden, um die Entscheidungslogik und Struktur intuitiv zu erfassen.\n",
    "\n",
    "- *Konfusionsmatrix*: Eine Konfusionsmatrix wird erstellt, um die Leistung des Modells bei der Klassifikation zu bewerten. Sie zeigt, wie gut das Modell verschiedene Klassen korrekt vorhersagt und welche Fehler gemacht werden.\n",
    "\n",
    "- *Test-Genauigkeit*: Die Test-Genauigkeit wird berechnet, um zu bewerten, wie gut das Modell auf unbekannten Daten abschneidet. Dies gibt einen Indikator dafür, wie zuverlässig die Vorhersagen des Modells sind.\n",
    "\n",
    "- *Feature Importances*: Es wird eine Analyse der Feature Importances durchgeführt, um die relative Bedeutung der verschiedenen Merkmale bei der Vorhersage zu bestimmen. Dies ermöglicht Einblicke in die relevanten Merkmale und kann bei der Feature-Auswahl oder -Gewichtung helfen.\n",
    "\n",
    "Diese Analysen bieten einen umfassenden Einblick in die Leistung und Funktionsweise des ausgewählten Modells und unterstützen bei der Interpretation der Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT LEARNER AND TEST DATA\n",
    "featureDataDir = list(filter(lambda x: os.path.isfile(os.path.join(PATH_FEATURE_DATA, x)) and DEFAULT_RAW_DATA_TYPE in x.upper(), os.listdir(PATH_FEATURE_DATA)))\n",
    "modelDir = os.listdir(PATH_MODEL)\n",
    "\n",
    "modelDropdown = widgets.Dropdown(description=\"model\")\n",
    "modelDropdown.options = modelDir\n",
    "selectedModelFile = None\n",
    "def onTrainigFileChange(change):\n",
    "    global selectedModelFile\n",
    "    selectedModelFile = change['new']\n",
    "modelDropdown.observe(onTrainigFileChange, names='value')\n",
    "display(modelDropdown)\n",
    "\n",
    "testFileDropdown = widgets.Dropdown(description=\"test data\")\n",
    "testFileDropdown.options = featureDataDir\n",
    "selectedTestFile = None\n",
    "def onTestFileChange(change):\n",
    "    global selectedTestFile\n",
    "    selectedTestFile = change['new']\n",
    "testFileDropdown.observe(onTestFileChange, names='value')\n",
    "display(testFileDropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3540ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYZE MODEL\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfmath\n",
    "import tensorflow_probability as tfp\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import joblib\n",
    "from matplotlib import pyplot\n",
    "from xgboost import XGBClassifier, plot_tree\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "csvTest = read_csv(PATH_FEATURE_DATA + selectedTestFile, delimiter=\";\")\n",
    "testData = csvTest.values\n",
    "testX, testY = testData[:, 2:].astype('float32'), testData[:, 1:2]\n",
    "featureNames = csvTest.columns.values[2:].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = None\n",
    "predictions = None\n",
    "transformedTestY = None\n",
    "confusionMatrix = None\n",
    "classes = None\n",
    "if (selectedModelFile.startswith('dnn')):\n",
    "    onehotencoder = None\n",
    "    with open(PATH_MODEL + selectedModelFile + '/encoder.pickle', 'rb') as f:\n",
    "        onehotencoder = joblib.load(f)\n",
    "    classes = onehotencoder.categories_[0]\n",
    "    transformedTestY = onehotencoder.transform(testY).toarray()    \n",
    "\n",
    "    model = tf.keras.models.load_model(PATH_MODEL + selectedModelFile)\n",
    "    predictions = model.predict(testX)\n",
    "\n",
    "    confusionMatrix = tf.math.confusion_matrix(np.argmax(transformedTestY, axis=1), np.argmax(predictions, axis=1))\n",
    "    equality = tf.math.equal(np.argmax(predictions, axis=1), np.argmax(transformedTestY, axis=1))\n",
    "    accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "elif (selectedModelFile.startswith('xgb')):\n",
    "    labelEncoder = None\n",
    "    with open(PATH_MODEL + selectedModelFile + '/encoder.pickle', 'rb') as f:\n",
    "        labelEncoder = joblib.load(f)\n",
    "    classes = labelEncoder.classes_\n",
    "    transformedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "    model = XGBClassifier()\n",
    "    model.load_model(PATH_MODEL + selectedModelFile + '/xgb.model')\n",
    "    predictions = model.predict(testX)\n",
    "    confusionMatrix = tfmath.confusion_matrix(transformedTestY, predictions)\n",
    "\n",
    "    for i in range(model.n_estimators):\n",
    "        plot_tree(model, num_trees=i, fmap=PATH_MODEL + selectedModelFile + '/feature_map.txt')\n",
    "        pyplot.gcf().set_dpi(1200)\n",
    "        pyplot.show()\n",
    "\n",
    "\n",
    "    #pyplot.show()\n",
    "    equality = tf.math.equal(predictions, transformedTestY)\n",
    "    accuracy = tf.math.reduce_mean(tf.cast(equality, tf.float32))\n",
    "elif (selectedModelFile.startswith('dtc')):\n",
    "    labelEncoder = None\n",
    "    with open(PATH_MODEL + selectedModelFile + '/encoder.pickle', 'rb') as f:\n",
    "        labelEncoder = joblib.load(f)\n",
    "    classes = labelEncoder.classes_\n",
    "    transformedTestY = labelEncoder.transform(np.ravel(testY))\n",
    "\n",
    "    model = joblib.load(PATH_MODEL + selectedModelFile + '/dtc.model')\n",
    "\n",
    "    predictions = model.predict(testX)\n",
    "    accuracy = accuracy_score(transformedTestY, predictions)\n",
    "    confusionMatrix = tfmath.confusion_matrix(transformedTestY, predictions)\n",
    "    plt.figure(figsize=(120, 40))       \n",
    "    tree.plot_tree(model, feature_names=featureNames, class_names=labelEncoder.classes_, filled=True)\n",
    "    plt.show()\n",
    "\n",
    "mat = pyplot.matshow(confusionMatrix, 1)\n",
    "mat.axes.set_xticks(np.arange(0, len(classes), 1))\n",
    "mat.axes.set_yticks(np.arange(0, len(classes), 1))\n",
    "mat.axes.set_xticklabels(classes, rotation=90)\n",
    "mat.axes.set_yticklabels(classes)\n",
    "for (x, y), value in np.ndenumerate(confusionMatrix):\n",
    "    pyplot.text(y, x, f\"{value:.2f}\", va=\"center\", ha=\"center\")\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "print('Test Accuracy: %.3f' % accuracy)\n",
    "\n",
    "\n",
    "#correlationMatrix = tfp.stats.correlation(testX)\n",
    "#pyplot.matshow(correlationMatrix)\n",
    "#pyplot.show()\n",
    "\n",
    "perm = PermutationImportance(model, scoring=\"neg_mean_squared_error\", random_state=1).fit(testX, transformedTestY)\n",
    "print(eli5.format_as_text(eli5.explain_weights(perm, feature_names=featureNames)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "390a4a00-cdc0-4fae-bd1f-e92cd875dec1",
   "metadata": {},
   "source": [
    "## 7. Statische Interpretation des Resultats\n",
    "\n",
    "- Welches Ergebnis haben wir erzielt und wie kann man es anwenden?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
