{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "d5a94bd2-a922-4b01-a9b9-7658e41f6036",
            "metadata": {},
            "source": [
                "# Aufbau der Notebooks [Philipp]\n",
                "\n",
                "- Multivalue bei Notebooks angucken\n",
                "- Wie installiere ich den \"scheiß\"\n",
                "- Widgets: https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "d3394772",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "ldf is not supported\n",
                        "xls is not supported\n",
                        "xlsx is not supported\n",
                        "yaml is not supported\n"
                    ]
                }
            ],
            "source": [
                "from enum import Enum\n",
                "import ipywidgets as widgets\n",
                "from pandas import read_csv\n",
                "import pandas as pd\n",
                "import asammdf\n",
                "from IPython.display import display\n",
                "import os\n",
                "import numpy as np\n",
                "import time\n",
                "import pathlib as pl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "268ea6b8",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "033bac617dff432294aae7ae89b6414f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "TagsInput(value=['TEST_NAME', 'TEST_TYPE', 'RPM', 'FLOW_RATE', 'P1', 'P2'], allow_duplicates=False, allowed_ta…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Globals\n",
                "\n",
                "PATH_RAW_DATA = \"./data/raw/\"\n",
                "PATH_FEATURE_DATA = \"./data/\"\n",
                "PATH_EXPLORATION_DATA = \"./exploration/\"\n",
                "PATH_MODEL = \"./models/\"\n",
                "DATA_SOURCE_KIDAQ = [\"TEST_NAME\", \"TEST_TYPE\", \"RPM\", \"FLOW_RATE\", \"P1\", \"P2\"]\n",
                "RAW_DATA_TYPE = [\"KIDAQ\", \"VIB\"]\n",
                "\n",
                "DATA_SOURCE_VID = [\n",
                "    \"TEST_NAME\",\n",
                "    \"TEST_TYPE\",\n",
                "    \"RPM\",\n",
                "    \"FLOW_RATE\",\n",
                "    \"S1\",\n",
                "    \"S2\",\n",
                "    \"S3\",\n",
                "    \"S4\",\n",
                "    \"S5\",\n",
                "    \"S6\",\n",
                "    \"S7\",\n",
                "    \"S8\",\n",
                "]\n",
                "FEATURE = [\n",
                "    \"STD\",\n",
                "    \"RANGE\",\n",
                "    \"IQR\",\n",
                "    \"MEAN_MEDIAN\",\n",
                "    \"FFT\",\n",
                "]\n",
                "OPERATING_POINT_FREQ = [725, 1450, 2175, 2900]\n",
                "OPERATING_POINT_FLOW_RATE = [0, 25, 50, 75, 100]\n",
                "\n",
                "LEARNER = Enum(\"LEARNER\", [\"DNN\", \"DT\"])\n",
                "\n",
                "DEFAULT_LEARNER = LEARNER.DNN\n",
                "DEFAULT_RAW_DATA_TYPE = RAW_DATA_TYPE[0]\n",
                "DEFAULT_RAW_DATA = DATA_SOURCE_KIDAQ\n",
                "\n",
                "DEFAULT_CLASS_LABEL = \"TEST_TYPE\"\n",
                "\n",
                "tags = widgets.TagsInput(\n",
                "    value=DEFAULT_RAW_DATA, allowed_tags=DEFAULT_RAW_DATA, allow_duplicates=False\n",
                ")\n",
                "display(tags)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6efc6220-a14b-4ff0-b0d8-66ba17569050",
            "metadata": {},
            "source": [
                "## 3. Preprocessing [Valerij]\n",
                "\n",
                "Aufgeteilt nach KIDAQ und VIB (separat um es einfach zu halten)\n",
                "\n",
                "- Auswahl der Fenstergröße in Millisekunden\n",
                "- Auswahl der Abtastrate\n",
                "- Multi-Selektion der Aggregation (avg, mean, std, ...)\n",
                "- Frequenzanalyse\n",
                "- Fourier-Transformation\n",
                "- Fenstergröße nach Frequenzbereichen\n",
                "\n",
                "### 3.1 Vorbereitung der Tainings- und Testdaten\n",
                "\n",
                "Multi-Selektion für:\n",
                "\n",
                "- Features\n",
                "- Betriebspunkte (RPM/FLOWRATE)\n",
                "- Klassifikationsarten (Szenario / Testdurchlauf)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1eaf4db6",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "d08d88a7-0fc1-4c7a-91f7-df5aa0a4f9a2",
            "metadata": {},
            "source": [
                "## 1. Aufgabe und Daten erklären/beschreiben [Philipp]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "103ea4fa",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "df8aab3b8bfc464a93ff7654436c0d98",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Dropdown(description='Window size in ms: ', options=('100', '200', '300', '400', '500', '600', '700', '800', '…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d16812093da14340ae9695c0f3d38ef5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Dropdown(description='Raw data type: ', options=('KIDAQ', 'VIB'), value='KIDAQ')"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2e71bd082d3943ae9ac0344d8b51d645",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Dropdown(description='Raw data folder: ', options=('Setup-I',), value='Setup-I')"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "WINDOW_SIZE_MS = [\"100\", \"200\", \"300\", \"400\", \"500\", \"600\", \"700\", \"800\",\"900\",\"1000\"]\n",
                "AGGREGATIONS = [\"std\", \"range\", \"iqr\", \"median\"]\n",
                "\n",
                "win_sizes = widgets.Dropdown(\n",
                "    placeholder= \"Choose the window size in ms\",\n",
                "    options = WINDOW_SIZE_MS,\n",
                "    description = \"Window size in ms: \",\n",
                "    ensure_option=True,\n",
                "    disabled = False\n",
                ")\n",
                "display(win_sizes)\n",
                "\n",
                "raw_data_type = widgets.Dropdown(\n",
                "    placeholder= \"Choose the raw data type\",\n",
                "    options = RAW_DATA_TYPE,\n",
                "    description = \"Raw data type: \",\n",
                "    ensure_option=True,\n",
                "    disabled = False\n",
                ")\n",
                "display(raw_data_type)\n",
                "\n",
                "# list all available directories in data/raw with max depth of 1\n",
                "raw_data_folders = [f.name for f in os.scandir(PATH_RAW_DATA) if f.is_dir()]\n",
                "raw_data_folder = widgets.Dropdown(\n",
                "    placeholder= \"Choose the raw data folder\",\n",
                "    options = raw_data_folders,\n",
                "    description = \"Raw data folder: \",\n",
                "    ensure_option=True,\n",
                "    disabled = False\n",
                ")\n",
                "display(raw_data_folder)\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "38d30db2",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "62ca695a2bcc40aa9f56114da8a51479",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/540 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "\n",
                "from tqdm.notebook import tqdm\n",
                "COLUMNS_KiDAQ = [\n",
                "    \"p1\",\n",
                "    \"p2\",\n",
                "    \"a2\",\n",
                "    \"T2\",\n",
                "    \"T1\"\n",
                "]\n",
                "\n",
                "\n",
                "CWD = pl.Path.cwd()\n",
                "\n",
                "PATH_RAW_DATA = CWD / \"data\" / \"raw\"\n",
                "PATH_TO_SETUP = PATH_RAW_DATA / raw_data_folder.value \n",
                "\n",
                "\n",
                "if raw_data_type.value == \"KIDAQ\":\n",
                "    raw_files = [file for file in PATH_TO_SETUP.glob(\"**/*.mf4\") if \"KiDAQ\" in file.parts]    \n",
                "elif raw_data_type.value == \"VIB\":\n",
                "    raw_files = [file for file in PATH_TO_SETUP.glob(\"**/*.csv\") if \"Rohdaten CSV\" in file.parts]\n",
                "def process_vib(file):\n",
                "    # error_type is the first folder name after the setup folder\n",
                "    error_type_with_number = file.parts[8]\n",
                "    error_type = error_type_with_number.split(\" \")[0]\n",
                "    rpm = file.parts[10].split(\"r\")[0]\n",
                "    rpm_percent = file.parts[11].split(\"%\")[0]\n",
                "    sensor = file.parts[-1].split(\" \")[0]\n",
                "    version = file.parts[12] if file.parts[12] != None else '0'\n",
                "\n",
                "    df = read_csv(file, skiprows=2, encoding=\"ISO-8859-1\", sep=\";\")\n",
                "\n",
                "    # the first column is the timestamp in format 1623686400000000000\n",
                "    # convert it to a datetime object and create windows of selected window size\n",
                "    df[\"Timestamp [ns]\"] = pd.to_datetime(df[\"Timestamp [ns]\"], unit=\"ns\")\n",
                "    df = df.set_index(\"Timestamp [ns]\")\n",
                "    # reduce the number of rows by grouping them by the selected window size\n",
                "    # for mean, range, std, iqr, mean-median\n",
                "\n",
                "    df_mean = df.resample(\"1s\").mean()\n",
                "    # rename value to sensor_mean\n",
                "    df_mean = df_mean.rename(columns={\"Value\": f\"{sensor}_mean\"})\n",
                "    df_range = df.resample(\"1s\").max() - df.resample(\"1s\").min()\n",
                "    df_range = df_range.rename(columns={\"Value\": f\"{sensor}_range\"})\n",
                "    df_std = df.resample(\"1s\").std()\n",
                "    df_std = df_std.rename(columns={\"Value\": f\"{sensor}_std\"})\n",
                "    df_iqr = df.resample(\"1s\").quantile(0.75) - df.resample(\"1s\").quantile(0.25)\n",
                "    df_iqr = df_iqr.rename(columns={\"Value\": f\"{sensor}_iqr\"})\n",
                "    df_mean_median = df.resample(\"1s\").mean() - df.resample(\"1s\").median()\n",
                "    df_mean_median = df_mean_median.rename(columns={\"Value\": f\"{sensor}_mean_median\"})\n",
                "\n",
                "    # merge all dataframes together so we have one timestamp column and all aggreagtion columns\n",
                "    # \n",
                "    df = pd.concat([df_mean, df_range, df_std, df_iqr, df_mean_median], axis=1)\n",
                "    df[\"Fehlertyp\"] = error_type_with_number\n",
                "    df[\"Fehlertyp_allgemein\"] = error_type\n",
                "    df[\"rpm\"] = rpm\n",
                "    df[\"rpm%\"] = rpm_percent\n",
                "    df[\"version\"] = version\n",
                "    # drop the Timestamp column\n",
                "    df = df.reset_index()\n",
                "    df = df.drop(columns=[\"Timestamp [ns]\"])\n",
                "    # reduce the aggregation columns to two columns. one with the aggregation type and one with the value\n",
                "    # create an index so each column is later easily identifiable before melting the dataframe and later pivoting it\n",
                "    # create an unique index for each row\n",
                "    df[\"ID\"] = df.index\n",
                "    df = df.melt(\n",
                "        id_vars=[\"ID\", \"Fehlertyp\", \"Fehlertyp_allgemein\", \"rpm\", \"rpm%\", \"version\"],\n",
                "        var_name=\"Aggregation\",\n",
                "        value_name=\"Value\",\n",
                "    )\n",
                "\n",
                "    return df \n",
                "\n",
                "    \n",
                "\n",
                "\n",
                "def process_kidaq(file):\n",
                "    \n",
                "    error_type_with_number = file.parts[8]\n",
                "    error_type = error_type_with_number.split(\" \")[0]\n",
                "    rpm = file.parts[10].split(\"r\")[0]\n",
                "    rpm_percent = file.parts[11].split(\"m\")[1].split(\"%\")[0]\n",
                "    \n",
                "    mdf = asammdf.MDF(file)\n",
                "    df = mdf.to_dataframe()\n",
                "    df = df.reset_index()\n",
                "\n",
                "    df = df[COLUMNS_KiDAQ]\n",
                "\n",
                "    df_features = None\n",
                "\n",
                "    windows = df.groupby(df.index // (int(win_sizes.value) * 20))\n",
                "\n",
                "    for _, window in windows:\n",
                "\n",
                "        features = {\n",
                "            \"Fehlertyp\": error_type_with_number, \n",
                "            \"Fehlertyp_allgemein\": error_type,\n",
                "            \"rpm\": rpm, \n",
                "            \"rpm%\": rpm_percent\n",
                "            }\n",
                "\n",
                "        features[\"p1_std\"] = window[\"p1\"].std()\n",
                "        features[\"p2_std\"] = window[\"p2\"].std()\n",
                "        features[\"a2_std\"] = window[\"a2\"].std()\n",
                "        features[\"T2_std\"] = window[\"T2\"].std()\n",
                "        features[\"T1_std\"] = window[\"T1\"].std()\n",
                "        \n",
                "        features[\"p1_range\"] =  window[\"p1\"].max() - window[\"p1\"].min()\n",
                "        features[\"p2_range\"] =  window[\"p2\"].max() - window[\"p2\"].min()\n",
                "        features[\"a2_range\"] =  window[\"a2\"].max() - window[\"a2\"].min()\n",
                "        features[\"T2_range\"] =  window[\"T2\"].max() - window[\"T2\"].min()\n",
                "        features[\"T1_range\"] =  window[\"T1\"].max() - window[\"T1\"].min()\n",
                "        \n",
                "        features[\"p1_iqr\"] = window[\"p1\"].quantile(0.75) - window[\"p1\"].quantile(0.25)\n",
                "        features[\"p2_iqr\"] = window[\"p2\"].quantile(0.75) - window[\"p2\"].quantile(0.25)\n",
                "        features[\"a2_iqr\"] = window[\"a2\"].quantile(0.75) - window[\"a2\"].quantile(0.25)\n",
                "        features[\"T2_iqr\"] = window[\"T2\"].quantile(0.75) - window[\"T2\"].quantile(0.25)\n",
                "        features[\"T1_iqr\"] = window[\"T1\"].quantile(0.75) - window[\"T1\"].quantile(0.25)\n",
                "        \n",
                "        features[\"p1_mean_median\"] = window[\"p1\"].mean() - window[\"p1\"].median()\n",
                "        features[\"p2_mean_median\"] = window[\"p2\"].mean() - window[\"p2\"].median()\n",
                "        features[\"a2_mean_median\"] = window[\"a2\"].mean() - window[\"a2\"].median()\n",
                "        features[\"T2_mean_median\"] = window[\"T2\"].mean() - window[\"T2\"].median()\n",
                "        features[\"T1_mean_median\"] = window[\"T1\"].mean() - window[\"T1\"].median()\n",
                "\n",
                "        if df_features is None:\n",
                "            df_features = pd.DataFrame(features, index=[0])    \n",
                "        else:\n",
                "            df_features = pd.concat([df_features, pd.DataFrame(features, index=[0])])\n",
                "\n",
                "    return df_features\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "877111cd",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "02f80592728f4e859314895281c25fd6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/540 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from tqdm.notebook import tqdm\n",
                "\n",
                "RESULT_FILE = CWD / \"data\" / f\"{raw_data_type.value}_features.csv\"\n",
                "\n",
                "if raw_data_type.value == \"VIB\":\n",
                "    result = None\n",
                "    for file in tqdm(raw_files):\n",
                "        if \"VIB\" in file.name:\n",
                "            if result is None:\n",
                "                result = process_vib(file)\n",
                "            else:\n",
                "                result = pd.concat([result, process_vib(file)])\n",
                "\n",
                "\n",
                "    #create an ascending index to be able to pivot the dataframe\n",
                "    result = result.pivot(index=[\"ID\",\"Fehlertyp\", \"Fehlertyp_allgemein\", \"rpm\", \"rpm%\", \"version\"], columns=\"Aggregation\", values=\"Value\")\n",
                "    result = result.drop(columns=[\"ID\"])\n",
                "    # save the result to a csv file\n",
                "    result.to_csv(RESULT_FILE, sep=\";\")\n",
                "\n",
                "\n",
                "elif raw_data_type.value == \"KIDAQ\":\n",
                "    for file in tqdm(raw_files):\n",
                "        result = process_kidaq(file)\n",
                "\n",
                "\n",
                "    if result is not None:\n",
                "        if not pl.Path(RESULT_FILE).exists():\n",
                "            result.to_csv(RESULT_FILE, index=False, sep=\";\")\n",
                "        else:\n",
                "            result.to_csv(RESULT_FILE, mode=\"a\", header=False, index=False, sep=\";\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "9da8dbc8",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1a82807fdebb45dba7f7a6994c0433bd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Dropdown(description='Feature Type:', options=('KIDAQ', 'VIB'), value='KIDAQ')"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "feature_types = [\"KIDAQ\", \"VIB\"]\n",
                "\n",
                "f_type = widgets.Dropdown(\n",
                "    options=feature_types,\n",
                "    value=feature_types[0],\n",
                "    description=\"Feature Type:\",\n",
                "    disabled=False,\n",
                ")\n",
                "display(f_type)\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "28b85819",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "11b43ce57eea472191256fca80d767e5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "SelectMultiple(description='Error Type:', index=(0,), options=('Fehlausrichtung 1',), value=('Fehlausrichtung …"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d34e288c550042bc94a3e6f6c83eec11",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "SelectMultiple(description='RPM:', index=(0, 1, 2, 3), options=(725, 1450, 2175, 2900), value=(725, 1450, 2175…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "23d81b2442c9414d916ef6fe780be6d9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "SelectMultiple(description='RPM%:', index=(0, 1, 2, 3, 4), options=(0, 25, 50, 75, 100), value=(0, 25, 50, 75,…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "\n",
                "df_features = pd.read_csv(CWD / \"data\" / f\"{f_type.value}_features.csv\", sep=\",\")\n",
                "\n",
                "error_types = df_features[\"Fehlertyp\"].unique().tolist()\n",
                "rpm_values = [725, 1450, 2175, 2900]\n",
                "rpm_percent_values = [0, 25, 50, 75, 100]\n",
                "\n",
                "error_type = widgets.SelectMultiple(\n",
                "    options=error_types,\n",
                "    value=error_types,\n",
                "    description=\"Error Type:\",\n",
                "    disabled=False,\n",
                ")\n",
                "\n",
                "rpm = widgets.SelectMultiple(\n",
                "    options=rpm_values,\n",
                "    value=rpm_values,\n",
                "    description=\"RPM:\",\n",
                "    disabled=False,\n",
                ")\n",
                "\n",
                "rpm_percent = widgets.SelectMultiple(\n",
                "    options=rpm_percent_values,\n",
                "    value=rpm_percent_values,\n",
                "    description=\"RPM%:\",\n",
                "    disabled=False,\n",
                ")\n",
                "\n",
                "display(error_type)\n",
                "display(rpm)\n",
                "display(rpm_percent)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "e9bcd401",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# filter the dataframe based on the selected rpm and rpm%\n",
                "df_features = df_features[df_features[\"Fehlertyp\"].isin(error_type.value)]\n",
                "df_features = df_features[df_features[\"rpm\"].isin(rpm.value)]\n",
                "df_features = df_features[df_features[\"rpm%\"].isin(rpm_percent.value)]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b8ed0170-f3d5-4287-8dd8-7439080d27af",
            "metadata": {},
            "source": [
                "## 4. Deskriptive/Explorative Datenanalyse [Philipp]\n",
                "\n",
                "- Plots\n",
                "- Beschreibung der Plots\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4b92fefe-672c-4bba-97c1-f78747ff1dc1",
            "metadata": {},
            "source": [
                "## 5. Machine Learning [Kevin]\n",
                "\n",
                "Multi-Selektion für:\n",
                "\n",
                "- Auswahl der Featuredateien (Train/Testdaten)\n",
                "- Auswahl des Learners\n",
                "- Konfiguration der Hyperparameter\n",
                "- Live-Validation des Models mit vorausgewählten Testdaten (Random-Search, ...)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d8da6989",
            "metadata": {},
            "outputs": [],
            "source": [
                "# SELECT TRAINING AND TEST DATA\n",
                "featureDataDir = list(filter(lambda x: os.path.isfile(os.path.join(PATH_FEATURE_DATA, x)) and DEFAULT_RAW_DATA_TYPE in x.upper(), os.listdir(PATH_FEATURE_DATA)))\n",
                "\n",
                "modelDropdown = widgets.Dropdown(description=\"training features\")\n",
                "modelDropdown.options = featureDataDir\n",
                "selectedModelFile = None\n",
                "def onTrainigFileChange(change):\n",
                "    global selectedModelFile\n",
                "    selectedModelFile = change['new']\n",
                "modelDropdown.observe(onTrainigFileChange, names='value')\n",
                "display(modelDropdown)\n",
                "\n",
                "testFileDropdown = widgets.Dropdown(description=\"test features\")\n",
                "testFileDropdown.options = featureDataDir\n",
                "selectedTestFile = None\n",
                "def onTestFileChange(change):\n",
                "    global selectedTestFile\n",
                "    selectedTestFile = change['new']\n",
                "testFileDropdown.observe(onTestFileChange, names='value')\n",
                "display(testFileDropdown)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dbbd27e2",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5e8f18d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# SET ML CONFIG\n",
                "FEATURE_TRAIN_FILE = PATH_FEATURE_DATA + selectedModelFile\n",
                "FEATURE_TEST_FILE = PATH_FEATURE_DATA + selectedTestFile\n",
                "\n",
                "DEFAULT_OPERATING_POINTS = OPERATING_POINT_FREQ + OPERATING_POINT_FLOW_RATE\n",
                "\n",
                "\n",
                "# DNN\n",
                "DNN_EXPLORATION_TARGET_VAL_ACCURACY = 0.9\n",
                "DNN_EXPLORATION_MAX_ITER = 1\n",
                "DNN_EXPLORATION_HIDDEN_LAYERS_MIN = 2\n",
                "DNN_EXPLORATION_HIDDEN_LAYERS_MAX = 4\n",
                "DNN_EXPLORATION_NEURONS_MIN = 8\n",
                "DNN_EXPLORATION_NEURONS_MAX = 64\n",
                "\n",
                "\n",
                "DNN_EARLY_STOPPING_PATIENCE = 50\n",
                "DNN_VERBOSE = 0\n",
                "DNN_EPOCHS = 100\n",
                "DNN_BATCH_SIZE = 32\n",
                "DNN_BATCH_NORMALIZATION = True\n",
                "\n",
                "\n",
                "# DT\n",
                "DT_MAX_DEPTH = 6\n",
                "DT_NUM_ESTIMATORS = 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "51491a91",
            "metadata": {},
            "outputs": [],
            "source": [
                "# LOAD DATA\n",
                "train_data = read_csv(FEATURE_TRAIN_FILE, header=None, delimiter=\",\", decimal='.', low_memory=False).values\n",
                "test_data = read_csv(FEATURE_TEST_FILE, header=None, delimiter=\",\", decimal='.', low_memory=False).values\n",
                "print(train_data[1:, :3])\n",
                "\n",
                "train_x, train_y = train_data[1:, 3:].astype('float32'), train_data[1:, :1]\n",
                "test_x, test_y = test_data[1:, 3:].astype('float32'), test_data[1:, :1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d6d7b343",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Decision Tree\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.metrics import accuracy_score\n",
                "from xgboost import XGBClassifier\n",
                "\n",
                "\n",
                "labelEncoder = LabelEncoder()\n",
                "labelEncoder = labelEncoder.fit(np.ravel(train_y))\n",
                "label_encoded_train_y = labelEncoder.transform(np.ravel(train_y))\n",
                "label_encoded_test_y = labelEncoder.transform(np.ravel(test_y))\n",
                "\n",
                "\n",
                "xgb = XGBClassifier(\n",
                "    tree_method=\"hist\",\n",
                "    enable_categorical=True,\n",
                "    max_depth=DT_MAX_DEPTH,\n",
                "    n_estimators=DT_NUM_ESTIMATORS,\n",
                ")\n",
                "# fit model\n",
                "xgb.fit(train_x, label_encoded_train_y, eval_set=[(test_x, label_encoded_test_y)])\n",
                "\n",
                "\n",
                "preds = xgb.predict(test_x)\n",
                "accuracy = accuracy_score(label_encoded_test_y, preds)\n",
                "\n",
                "XGBClassifier.save_model(xgb, PATH_MODEL+\"dt.\"+str(round(time.time())))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f987c57a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Neural Network\n",
                "import json\n",
                "import random\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "from tensorflow.keras import Sequential\n",
                "from tensorflow.keras.layers import Dense, InputLayer, BatchNormalization\n",
                "from tensorflow.keras.callbacks import EarlyStopping\n",
                "\n",
                "onehotencoder = OneHotEncoder()\n",
                "onehotencoder = onehotencoder.fit(train_y)\n",
                "onehot_encoded_train_y = onehotencoder.transform(train_y).toarray()\n",
                "onehot_encoded_test_y = onehotencoder.transform(test_y).toarray()\n",
                "\n",
                "exploration_results = []\n",
                "\n",
                "\n",
                "n_features = train_x.shape[1]\n",
                "categories = len(onehot_encoded_train_y[0])\n",
                "\n",
                "test_acc = 0.0\n",
                "model = Sequential()\n",
                "interation = 0\n",
                "while (\n",
                "    test_acc < DNN_EXPLORATION_TARGET_VAL_ACCURACY\n",
                "    and interation < DNN_EXPLORATION_MAX_ITER\n",
                "):\n",
                "    model = Sequential()\n",
                "    model.add(InputLayer(input_shape=(n_features,)))\n",
                "    model.add(BatchNormalization())\n",
                "    dense_count = random.randint(\n",
                "        DNN_EXPLORATION_HIDDEN_LAYERS_MIN, DNN_EXPLORATION_HIDDEN_LAYERS_MAX\n",
                "    )\n",
                "    dense_neurons = []\n",
                "    for i in range(0, dense_count):\n",
                "        neurons = random.randint(\n",
                "            DNN_EXPLORATION_NEURONS_MIN, DNN_EXPLORATION_NEURONS_MAX\n",
                "        )\n",
                "        dense_neurons.append(neurons)\n",
                "        model.add(Dense(neurons, activation=\"tanh\"))\n",
                "        model.add(BatchNormalization())\n",
                "    model.add(Dense(categories, activation=\"sigmoid\"))\n",
                "    model.compile(\n",
                "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
                "    )\n",
                "    model.fit(\n",
                "        train_x,\n",
                "        onehot_encoded_train_y,\n",
                "        epochs=DNN_EPOCHS,\n",
                "        batch_size=DNN_BATCH_SIZE,\n",
                "        verbose=1,\n",
                "        validation_data=(test_x, onehot_encoded_test_y),\n",
                "        callbacks=[\n",
                "            EarlyStopping(\n",
                "                monitor=\"val_loss\",\n",
                "                patience=DNN_EARLY_STOPPING_PATIENCE,\n",
                "                restore_best_weights=True,\n",
                "            )\n",
                "        ],\n",
                "    )\n",
                "\n",
                "    train_loss, train_acc = model.evaluate(train_x, onehot_encoded_train_y)\n",
                "    test_loss, test_acc = model.evaluate(test_x, onehot_encoded_test_y)\n",
                "\n",
                "    exploration_results.append(\n",
                "        {\n",
                "            \"dense_count\": dense_count,\n",
                "            \"dense_neurons\": dense_neurons,\n",
                "            \"train_loss\": train_loss,\n",
                "            \"train_acc\": train_acc,\n",
                "            \"test_loss\": test_loss,\n",
                "            \"test_acc\": test_acc,\n",
                "        }\n",
                "    )\n",
                "    interation += 1\n",
                "\n",
                "modelName = \"dnn.\" + str(round(time.time()))\n",
                "model.save(PATH_MODEL + modelName)\n",
                "\n",
                "with open(PATH_EXPLORATION_DATA + modelName + \".exploration_results.json\", \"w\") as f:\n",
                "    json.dump(exploration_results, f, indent=4)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "615dfc90-2798-42fe-8001-7be9ea78aa35",
            "metadata": {},
            "source": [
                "## 6. Modelanalyse des Learners [Kevin]\n",
                "\n",
                "- Vorherige Auswahl eines Learners\n",
                "- Feature Importance\n",
                "- Korrelationsmatrix\n",
                "- Konfusionsmatrix\n",
                "- Post-Validation des Models mit auswählbaren Daten\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "72ea0d36",
            "metadata": {},
            "outputs": [],
            "source": [
                "# SELECT LEARNER AND TEST DATA\n",
                "featureDataDir = list(filter(lambda x: os.path.isfile(os.path.join(PATH_FEATURE_DATA, x)) and DEFAULT_RAW_DATA_TYPE in x.upper(), os.listdir(PATH_FEATURE_DATA)))\n",
                "modelDir = os.listdir(PATH_MODEL)\n",
                "\n",
                "modelDropdown = widgets.Dropdown(description=\"model\")\n",
                "modelDropdown.options = modelDir\n",
                "selectedModelFile = None\n",
                "def onTrainigFileChange(change):\n",
                "    global selectedModelFile\n",
                "    selectedModelFile = change['new']\n",
                "modelDropdown.observe(onTrainigFileChange, names='value')\n",
                "display(modelDropdown)\n",
                "\n",
                "testFileDropdown = widgets.Dropdown(description=\"test data\")\n",
                "testFileDropdown.options = featureDataDir\n",
                "selectedTestFile = None\n",
                "def onTestFileChange(change):\n",
                "    global selectedTestFile\n",
                "    selectedTestFile = change['new']\n",
                "testFileDropdown.observe(onTestFileChange, names='value')\n",
                "display(testFileDropdown)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3e3540ef",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ANALYZE MODEL\n",
                "from matplotlib import pyplot\n",
                "from tensorflow import math as tfmath\n",
                "import tensorflow_probability as tfp\n",
                "import eli5\n",
                "from eli5.sklearn import PermutationImportance\n",
                "\n",
                "conf_matrix = tfmath.confusion_matrix(np.argmax(y_test, axis=1), np.argmax(result, axis=1))\n",
                "pyplot.matshow(conf_matrix, 1)\n",
                "for (x, y), value in np.ndenumerate(conf_matrix):\n",
                "    pyplot.text(y, x, f\"{value:.2f}\", va=\"center\", ha=\"center\")\n",
                "pyplot.show()\n",
                "\n",
                "\n",
                "print('Test Accuracy: %.3f' % test_acc)\n",
                "\n",
                "\n",
                "corr_matrix = tfp.stats.correlation(X_test)\n",
                "pyplot.matshow(corr_matrix)\n",
                "pyplot.show()\n",
                "\n",
                "perm = PermutationImportance(model, scoring=\"neg_mean_squared_error\", random_state=1).fit(X_test, y_test)\n",
                "print(eli5.format_as_text(eli5.explain_weights(perm, feature_names=feature_names)))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "390a4a00-cdc0-4fae-bd1f-e92cd875dec1",
            "metadata": {},
            "source": [
                "## 7. Statische Interpretation des Resultats\n",
                "\n",
                "- Welches Ergebnis haben wir erzielt und wie kann man es anwenden?\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
